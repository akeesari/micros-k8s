{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"contact/","title":"Contact","text":"<p>If you have any question, suggestion or want to request a topic, feel free to drop me a message and we\u2019ll get in touch when I can with my busy schedule.</p> <ul> <li>Email: anjkeesari@gmail.com</li> <li>Website: https://anjikeesari.com</li> </ul>"},{"location":"argocd/1-argocd-intro/","title":"Introduction to Argo CD","text":"<p>Chapter-5: ArgoCD: Continuous Deployment for Kubernetes</p>"},{"location":"argocd/1-argocd-intro/#introduction-to-argo-cd","title":"Introduction to Argo CD","text":""},{"location":"argocd/1-argocd-intro/#introduction","title":"Introduction","text":"<p>One of my goals is to simplify the development, deployment, and scaling of complex applications and to bring the full power of Kubernetes to all projects. <code>ArgoCD</code> is one of the tools introduced to reach the goals.</p> <p>With ArgoCD, developers can create application manifests and store them in a Git repository. They can then use ArgoCD to deploy those manifests to a Kubernetes cluster, and it will continuously monitor and reconcile the live environment with the desired state defined in Git.</p> <p>In this section, we'll take a simple approach to understand Argo CD. We'll explore what it is, how it works, and why it's so useful. Whether you're a tech pro or just starting out, this article aims to explain Argo CD in a way that makes sense. Let's look into it and discover how Argo CD can simplify the way you deploy and manage your applications, especially if you're using Kubernetes.</p>"},{"location":"argocd/1-argocd-intro/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn following:</p> <ul> <li>What is GitOps?</li> <li>Core components of GitOps</li> <li>What is ArgoCD?</li> <li>How ArgoCD works?</li> <li>Why do we need to use ArgoCD?</li> <li>Key features of ArgoCD</li> <li>ArgoCD architecture components</li> <li>ArgoCD supports tools</li> </ul> <p>Before we explore into hands-on labs, let's lay the groundwork by exploring the fundamental concepts of ArgoCD. Our journey begins with a close look at GitOps, providing a solid understanding before we explore into the specifics of ArgoCD. This foundational knowledge will help for upcoming exercises in practical labs. Let's start by understanding the key principles of GitOps before looking ourselves in to ArgoCD.</p>"},{"location":"argocd/1-argocd-intro/#what-is-gitops","title":"What is GitOps?","text":"<p>GitOps is a modern application development and deployment methodology that uses Git as the single source of truth for the desired state of the system. In GitOps, all configuration changes and updates to the system are made through Git commits and pull requests. This makes it easy to track changes, collaborate with other developers, and manage deployments at scale.</p> <p>GitOps principles</p> <ul> <li>Infrastructure as code</li> <li>Declarative configuration</li> <li>Git as the single source of truth</li> <li>Continuous deployment</li> <li>Rollback Capabilities:</li> <li>Observability</li> <li>Security</li> </ul>"},{"location":"argocd/1-argocd-intro/#core-components-of-gitops","title":"Core components of GitOps","text":"<p>The core components of GitOps include:</p> <ul> <li>Git repository: Git is the central source of truth for the desired state of the system. </li> <li>CI/CD pipeline: A CI/CD pipeline is used to build, test, and deploy changes to the cluster.</li> <li>Kubernetes cluster: The Kubernetes cluster is the target environment where the changes are deployed. </li> <li>Deployment tool: A deployment tool such as ArgoCD is used to continuously deploy and synchronize the cluster with the Git repository. The tool monitors the Git repository for changes and deploys them to the cluster.</li> <li>Infrastructure as code: GitOps relies on Infrastructure as Code (IaC) to define the desired state of the system. This means that all infrastructure changes are made through code, which can be stored in a Git repository and version controlled.</li> <li>Observability: Observability tools are used to monitor and debug the system. By using Git to track changes and version control, it becomes easier to identify issues and roll back changes if necessary.</li> <li>Security: Security best practices such as RBAC and secrets management are used to secure the system. </li> </ul> <p>GitOps architecture is designed to promote automation, repeatability, and reliability in the software development and deployment process. By using a GitOps approach, organizations can improve their ability to deliver high-quality software at scale.</p> <p>now let's start talking about ArgoCD here.</p>"},{"location":"argocd/1-argocd-intro/#what-is-argocd","title":"What is ArgoCD?","text":"<p>Argo CD is an open-source declarative GitOps continuous delivery tool designed for Kubernetes. It helps to manage and synchronize application deployments with the desired state specified in the Git repository. ArgoCD allows developers to version control their application configuration and use Git as a single source of truth for deployments, ensuring that the desired state of the applications in the cluster always matches the desired state defined in Git.</p> <p>Argo CD has gained popularity in the Kubernetes ecosystem due to its simplicity, flexibility, and strong adherence to GitOps practices. It provides a robust solution for managing and automating the continuous delivery of applications in Kubernetes clusters.</p>"},{"location":"argocd/1-argocd-intro/#how-argocd-works","title":"How ArgoCD works?","text":"<p>ArgoCD follows the GitOps pattern of using Git repositories as the source of truth for defining the desired application state. Kubernetes manifests can be specified in several ways:</p> <ul> <li>kustomize applications</li> <li>helm charts</li> <li>Plain directory of YAML/json manifests</li> </ul> <p></p> <p>ArgoCD works by continuously monitoring the desired state of applications defined in Git, and comparing that to the actual state of the applications in a target Kubernetes cluster. It uses this comparison to perform automated sync operations that bring the live cluster into the desired state.</p> <p>Here is a high-level overview of the steps involved in using ArgoCD:</p> <ol> <li>Define the desired state of your applications in a Git repository.</li> <li>Connect ArgoCD to the Git repository and target Kubernetes cluster.</li> <li>Create applications in ArgoCD, which are a mapping between the desired state in Git and the target cluster.</li> <li>ArgoCD continuously monitors the state of the applications and compares it to the desired state in Git.</li> <li>When a difference is detected, ArgoCD performs a sync operation, which updates the target cluster to match the desired state in Git.</li> <li>ArgoCD provides a user interface that allows you to view the current state of your applications, perform manual sync operations, and track changes over time.</li> </ol> <p>This GitOps-based approach provides a reliable and consistent way to manage and deploy applications to a Kubernetes cluster, helping to eliminate inconsistencies and minimize errors.</p>"},{"location":"argocd/1-argocd-intro/#why-do-we-need-to-use-argocd","title":"Why do we need to use ArgoCD?","text":"<p>Here are several reasons why one might choose to use ArgoCD:</p> <ol> <li>Consistency and reliability: ArgoCD provides a single source of truth for your application deployments, ensuring that the desired state of your applications in the cluster always matches the desired state defined in Git.</li> <li>Automation: ArgoCD automates the process of deploying and updating applications, reducing the chance of human error and freeing up time for developers to focus on other tasks.</li> <li>Version control: ArgoCD integrates with Git, allowing you to version control your application configurations and track changes over time.</li> <li>Collaboration: ArgoCD makes it easier for teams to collaborate on application deployments by providing a common, shared repository for defining the desired state of applications.</li> <li>Rollback: ArgoCD provides a way to quickly revert to previous versions of an application if something goes wrong during an update.</li> <li>Scalability: ArgoCD can scale to manage multiple applications and multiple clusters, making it easier to manage large-scale deployments.</li> </ol> <p>By using ArgoCD, organizations can ensure consistent, reliable, and automated deployments of their applications to a Kubernetes cluster.</p>"},{"location":"argocd/1-argocd-intro/#key-features-of-argocd","title":"Key features of ArgoCD","text":"<p>ArgoCD comes with a rich set of features that make it a powerful tool for continuous delivery and GitOps workflows in Kubernetes. Here are some key features of ArgoCD:</p> <ol> <li>GitOps: ArgoCD uses Git as the single source of truth for your application deployments, ensuring that the desired state of your applications in the cluster always matches the desired state defined in Git.</li> <li>Continuous monitoring and synchronization: ArgoCD continuously monitors the state of your applications and compares it to the desired state in Git. When a difference is detected, ArgoCD performs a sync operation to bring the live cluster into the desired state.</li> <li>User interface: ArgoCD provides a user interface that allows you to view the current state of your applications, perform manual sync operations, and track changes over time.</li> <li>Role-based access control: ArgoCD allows you to control who can access and make changes to your applications, making it easier to manage deployments in large organizations.</li> <li> <p>Rollback: ArgoCD provides a way to quickly revert to previous versions of an application if something goes wrong during an update. Custom resource validation: ArgoCD can validate the custom resource definitions used in your applications, ensuring that your applications are deployed according to your desired state.</p> </li> <li> <p>Multi-cluster support: ArgoCD can manage applications across multiple Kubernetes clusters, making it easier to manage large-scale deployments.</p> </li> </ol> <p>These features make ArgoCD a powerful tool for managing and deploying applications in a Kubernetes cluster in a GitOps-based manner.</p>"},{"location":"argocd/1-argocd-intro/#argocd-architecture-components","title":"ArgoCD architecture components","text":"<p>ArgoCD's architecture consists several components that work together to facilitate continuous delivery and GitOps workflows. Here are the key components of ArgoCD:</p> <ul> <li>API server: The API server is the central component of ArgoCD architecture. It provides a REST API that allows users to manage the lifecycle of applications deployed in Kubernetes clusters.</li> <li>Application controller: The application controller is responsible for monitoring the Git repository for changes to the desired state and reconciling them with the current state of the cluster. It deploys, updates, and deletes resources in the cluster to match the desired state.</li> <li>Repository server: The repository server is responsible for authenticating and accessing the Git repository that contains the desired state of the Kubernetes environment. It provides support for multiple Git repositories, including private and public repositories.</li> <li>Redis cache: Redis is used as an in-memory cache to store application and cluster state information. This helps to improve the performance of the application controller by reducing the number of API requests to the Kubernetes cluster.</li> <li>Prometheus metrics server: Prometheus is used to collect and store metrics related to the performance and health of the ArgoCD application. These metrics can be used to monitor the application and identify potential issues.</li> <li>Web UI: ArgoCD provides a web-based UI that allows users to view the state of the Kubernetes environment, manage applications, and perform other administrative tasks.</li> <li>Dex authentication server: Dex is used to provide single sign-on (SSO) authentication for ArgoCD. It supports multiple identity providers, including azure active directory, GitHub, GitLab, and Google.</li> <li>RBAC: ArgoCD supports role-based access control (RBAC) to control access to resources and operations within the Kubernetes cluster.</li> </ul> <p>The components of ArgoCD architecture work together to automate the deployment and management of applications in Kubernetes environments. By leveraging GitOps principles, ArgoCD helps to ensure that the desired state of the environment is always in sync with the actual state of the cluster, reducing the likelihood of errors and improving the overall efficiency of the software development lifecycle.</p> <pre><code>graph TD\n  subgraph clusterArgoCD\n    API_Server --&gt;|Manages state| Application_Controller\n    Application_Controller --&gt;|Triggers synchronization| Repository_Server\n    Repository_Server --&gt;|Fetches configurations| Git_Repository\n    Application_Controller --&gt;|Optional: Manages sets| Application_Set_Controller\n    API_Server --&gt;|Optional: Authentication| Dex\n    API_Server --&gt;|Optional: Caching| Redis\n    API_Server --&gt;|Optional: Metrics| Metrics_Server\n    API_Server --&gt;|Optional: Logging| Log_Collector\n  end</code></pre>"},{"location":"argocd/1-argocd-intro/#argocd-supports-tools","title":"ArgoCD supports tools","text":"<p>Here is the ArgoCD supports tools in which Kubernetes manifests can be defined:</p> <ul> <li>YAML manifests: YAML manifests are the most common format for defining Kubernetes resources, and ArgoCD supports them out of the box.</li> <li>Kustomize: Kustomize is a tool for customizing Kubernetes YAML configurations. It allows users to define a base set of resources and then apply overlays to modify them. ArgoCD supports Kustomize out of the box.</li> <li>Helm: Helm is a package manager for Kubernetes that allows users to define and manage sets of related Kubernetes resources. ArgoCD supports Helm charts and can deploy and manage Helm releases.</li> <li>Jsonnet: Jsonnet is a data templating language that can be used to generate Kubernetes manifests. ArgoCD supports Jsonnet through the use of the Jsonnet Bundler.</li> <li>Jenkins: is a popular continuous integration and continuous delivery (CI/CD) tool that can be used to automate the build, test, and deployment of applications. Jenkins has a plugin called the Kubernetes plugin that allows users to define Kubernetes manifests using the Jenkins pipeline syntax. ArgoCD can then be used to deploy and manage these manifests in a GitOps workflow.</li> <li>Ksonnet: is a command-line tool for defining Kubernetes applications using a higher-level abstraction than raw YAML files. Ksonnet provides a way to define applications using a library of reusable components, making it easier to manage complex applications. ArgoCD supports Ksonnet and can deploy and manage applications defined using this tool.</li> </ul> <p>By supporting multiple formats for defining Kubernetes manifests, ArgoCD allows users to choose the tool that best fits their workflow and preferences. This flexibility is one of the key benefits of using ArgoCD for continuous delivery in Kubernetes environments.</p>"},{"location":"argocd/1-argocd-intro/#references","title":"References","text":"<ul> <li>Argo CD - Overview</li> </ul>"},{"location":"argocd/2-install-argocd/","title":"Install Argo CD in AKS with helm chart using terraform","text":""},{"location":"argocd/2-install-argocd/#introduction","title":"Introduction","text":"<p>In this article, we will explore the process of installing Argo CD in Azure Kubernetes Service (AKS) using Helm charts and terraform. </p>"},{"location":"argocd/2-install-argocd/#installation-types","title":"Installation types","text":"<p>Argo CD offers two types of installations: multi-tenant and core. Before proceeding with the installation on AKS, let's delve into the differences between multi-tenant and core installations of Argo CD.</p>"},{"location":"argocd/2-install-argocd/#multi-tenant-installation","title":"Multi-Tenant Installation","text":"<p>A multi-tenant installation of Argo CD is designed to accommodate larger organizations with multiple teams or users, enabling them to share a common Argo CD instance while maintaining isolation and security. In this setup, each tenant is allocated its dedicated set of resources, including applications, projects, and roles, ensuring separation from resources belonging to other tenants. The use of Kubernetes namespaces and RBAC (Role-Based Access Control) facilitates this segregation, allowing each team to operate independently within their designated environment.</p> <p>Key Features:</p> <ul> <li> <p>Resource Isolation: Tenants enjoy dedicated spaces for applications, projects, and roles, ensuring independence and avoiding interference from other teams.</p> </li> <li> <p>RBAC Implementation: Role-Based Access Control is employed to manage permissions within each tenant's namespace, guaranteeing secure and controlled access.</p> </li> <li> <p>Suitable for Larger Organizations: Ideal for organizations with diverse teams that necessitate separate environments and distinct permission structures.</p> </li> </ul>"},{"location":"argocd/2-install-argocd/#core-installation","title":"Core Installation","text":"<p>A core installation of Argo CD offers a simpler deployment designed for smaller organizations or individual users, emphasizing ease of use and straightforward management. In this single-tenant setup, there is a unified set of resources, including applications, projects, and roles, all managed by the same user or team. This streamlined approach is well-suited for scenarios where the additional complexity of a multi-tenant environment is unnecessary.</p> <p>Key Features:</p> <ul> <li> <p>Single-Tenant Model: All resources are managed within a single namespace, providing a straightforward and consolidated environment.</p> </li> <li> <p>Simplicity and Quick Deployment: Core installations are characterized by their simplicity, making them easy to set up and manage. They are particularly suitable for smaller teams or individual users.</p> </li> <li> <p>Ideal for Smaller Organizations: Suited for scenarios where a single team or individual user oversees the entire continuous delivery process without the need for extensive isolation.</p> </li> </ul> <p>Choosing the right installation</p> <p>The decision between a multi-tenant and core installation hinges on the organizational structure, collaboration requirements, and scale of the continuous delivery environment. Multi-tenant installations are optimal for larger organizations with diverse teams, whereas core installations offer a streamlined approach for smaller teams or individual users who prioritize simplicity and efficiency. Both options retain the robust GitOps capabilities that Argo CD provides.</p> <p>Note</p> <p>Here we are going to use the <code>Multi-tenant installation</code></p>"},{"location":"argocd/2-install-argocd/#non-high-availability-vs-high-availability","title":"Non-High Availability vs High Availability","text":"<p>Argo CD can be installed in two different ways in terms of High Availability (HA): <code>Non-High Availability</code> and <code>High Availability</code>.</p>"},{"location":"argocd/2-install-argocd/#non-high-availability-installation","title":"Non-High Availability Installation:","text":"<p>In a Non-High Availability (HA) installation, Argo CD is deployed on a single instance or node. This configuration implies that the entire system relies on a single point of deployment. If the node encounters failure or downtime, the entire system becomes unavailable. Non-High Availability installations are well-suited for development or testing environments where high availability is not a critical requirement.</p> <p>Key Characteristics:</p> <ul> <li> <p>Single Node Deployment: Argo CD is installed on a single instance, making the system dependent on the availability of that specific node.</p> </li> <li> <p>Limited Fault Tolerance: The system lacks redundancy, and if the single node fails, it results in a downtime until the node is restored.</p> </li> <li> <p>Suitable for Development/Testing: Non-High Availability installations are appropriate for environments where system uptime is less critical, such as development or testing scenarios.</p> </li> </ul>"},{"location":"argocd/2-install-argocd/#high-availability-installation","title":"High Availability Installation:","text":"<p>In a High Availability installation, Argo CD is deployed across multiple instances or nodes. This setup creates a redundant and fault-tolerant system that can withstand failures or outages. High Availability installations are suitable for production environments where continuous availability and uptime are essential requirements.</p> <p>Key Characteristics:</p> <ul> <li> <p>Multi-Node Deployment: Argo CD is distributed across multiple instances, ensuring that the system is not dependent on any single node.</p> </li> <li> <p>Enhanced Fault Tolerance: The redundancy of nodes enhances fault tolerance, making the system resilient to individual node failures.</p> </li> <li> <p>Critical for Production Environments: High Availability installations are crucial for production environments where uninterrupted service and minimal downtime are most important.</p> </li> </ul> <p>Considerations for Choosing Installation Type:</p> <ul> <li> <p>Environment Requirements: Choose the installation type based on the specific requirements of your environment. Production environments often demand High Availability for mission-critical applications.</p> </li> <li> <p>Resource Scaling: High Availability installations offer scalability and load distribution, making them suitable for managing larger workloads and user bases.</p> </li> <li> <p>Cost Implications: High Availability setups may involve more resources, potentially affecting infrastructure costs. Evaluate the cost implications against the benefits of improved availability.</p> </li> </ul> <p>Understanding the trade-offs between Non-High Availability and High Availability installations is crucial for aligning Argo CD deployments with the reliability and availability expectations of the target environment.</p> <p>Note</p> <p>Here we are going to focus on Non-High Availability installation</p> <p>Install ArgCDd using YAML manifest vs helm charts?</p> <p>Argo CD can be deployed on Azure Kubernetes Service (AKS) using either YAML manifests or Helm charts. In this lab, we'll leverage the Helm chart approach due to its simplicity and streamlined installation process.</p> <p>Helm, a Kubernetes package manager, facilitates the installation of Argo CD, providing an efficient and manageable deployment. To enhance automation and maintain Infrastructure as Code (IaC) principles, we'll deploy the Helm chart using terraform. Let's explore the step-by-step process:</p> <p></p>"},{"location":"argocd/2-install-argocd/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you've been tasked with configuring ArgoCD within an established Kubernetes cluster. The objective is to leverage ArgoCD for deploying manifests related to a Microservices architecture onto the Kubernetes cluster. ArgoCD's role is crucial in ensuring continuous monitoring and reconciliation, aligning the live environment with the desired state specified in Git. This implementation aims to enhance the efficiency of managing the Microservices architecture by automating deployment processes and maintaining consistency with the defined configurations stored in the version-controlled Git repository.</p>"},{"location":"argocd/2-install-argocd/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Configure terraform providers</li> <li>Step 2. Create a new namespace for ArgoCD</li> <li>Step 3. Install ArgoCD in AKS with helm-chart using terraform</li> <li>Step 4. Verify ArgoCD resources in AKS</li> <li>Step 5. Configure port forwarding for login screen</li> <li>Step 6. ArgoCD login with localhost</li> </ul>"},{"location":"argocd/2-install-argocd/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation of Argo CD in AKS using terraform, ensure you have the following prerequisites in place:</p> <ol> <li> <p>Azure Subscription: Sign up for an Azure subscription, if not already done.</p> </li> <li> <p>terraform: Install and configure terraform on your local machine, including the Helm, Kubernetes, and Azure providers.</p> </li> <li> <p>Azure CLI: Install Azure CLI for Azure service interaction.</p> </li> <li> <p>Kubectl: Install and set up kubectl for managing Kubernetes clusters.</p> </li> <li> <p>Kubernetes Cluster: Ensure you have a running Kubernetes cluster available for ArgoCD deployment.</p> </li> <li> <p>Git Repository: Maintain a Git repository storing manifests for your applications, serving as ArgoCD's deployment source.</p> </li> </ol>"},{"location":"argocd/2-install-argocd/#implementation-details","title":"Implementation Details","text":"<p>Let's delve into the step-by-step implementation details:</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"argocd/2-install-argocd/#step-1-configure-terraform-providers","title":"Step-1: Configure terraform providers","text":"<p>Launch Visual Studio Code and open your current terraform repository to begin working on your terraform configuration.</p> <p>In order to install any Helmcharts using terraform configuration we need to have following terraform providers.</p> <ul> <li>helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider </li> </ul> <p>terraform providers</p> <p>You can install the necessary providers by adding the following code in your Terraform configuration file:</p> <p>Let's update our existing <code>provider.tf</code> file with new kubernetes, helm and kubectl providers:</p> provider.tf<pre><code>terraform {\n\n  required_version = \"&gt;=0.12\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;2.0\"\n    }\n\n    azuread = {\n      version = \"&gt;= 2.26.0\" // https://github.com/terraform-providers/terraform-provider-azuread/releases\n    }\n     kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"&gt;= 2.0.3\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"&gt;= 2.1.0\"\n    }\n\n     kubectl = {\n      source  = \"gavinbunney/kubectl\"\n      version = \"&gt;= 1.7.0\"\n    }\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  #load_config_file       = false\n}\n\nprovider \"helm\" {\n  debug = true\n  kubernetes {\n    host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n\n  }\n}\nprovider \"kubectl\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  load_config_file       = false\n}\n</code></pre>"},{"location":"argocd/2-install-argocd/#step-2-create-namespace-for-argocd","title":"Step-2: Create namespace for argocd","text":"<p>Create a separate namespace for argocd where all argocd related resources will be created. let's create a new file called argocd.tf and copy the following configuration.</p> argocd.tf<pre><code>resource \"kubernetes_namespace\" \"argocd\" {  \n  metadata {\n    name = \"argocd\"\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output</p> <p><pre><code> + create\n\nterraform will perform the following actions:\n\n  # kubernetes_namespace.argocd will be created\n  + resource \"kubernetes_namespace\" \"argocd\" {\n      + id = (known after apply)\n\n      + metadata {\n          + generation       = (known after apply)\n          + name             = \"argocd\"\n          + resource_version = (known after apply)\n          + uid              = (known after apply)\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> run terraform apply <pre><code>terraform apply dev-plan\n</code></pre> <pre><code>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre></p>"},{"location":"argocd/2-install-argocd/#step-3-install-argocd-in-aks-with-helm-chart-using-terraform","title":"Step-3: Install argocd in AKS with helm-chart using terraform","text":"<p>Visit the official Argo CD Helm chart on the ArtifactHUB website: Argo CD Helm Chart.</p> <p>Click on the \"Install\" button to retrieve the necessary details for the Argo CD Helm chart installation.</p> <pre><code>repository = \"https://argoproj.github.io/argo-helm\"\nchart      = \"argo-cd\"\nversion    = \"5.24.1\"\n</code></pre> argocd.tf<pre><code># Install argocd helm chart using terraform\nresource \"helm_release\" \"argocd\" {\n  name       = \"argocd\"\n  repository = \"https://argoproj.github.io/argo-helm\"\n  chart      = \"argo-cd\"\n  version    = \"5.24.1\"\n  namespace  = kubernetes_namespace.argocd.metadata.0.name\n  depends_on = [\n    kubernetes_namespace.argocd\n  ]\n}\n</code></pre> <p>run terraform plan</p> <p><pre><code>terraform validate\nterraform fmt\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output <pre><code>  + create\n\nterraform will perform the following actions:\n.\n.\n.\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre></p> <p>run terraform apply</p> <p><pre><code>terraform apply dev-plan\n</code></pre> <pre><code>helm_release.argocd: Creating...\nhelm_release.argocd: Still creating... [10s elapsed]\nhelm_release.argocd: Still creating... [20s elapsed]\nhelm_release.argocd: Still creating... [30s elapsed]\nhelm_release.argocd: Still creating... [40s elapsed]\nhelm_release.argocd: Still creating... [50s elapsed]\nhelm_release.argocd: Still creating... [1m0s elapsed]\nhelm_release.argocd: Creation complete after 1m5s [id=argocd]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs\n</code></pre></p>"},{"location":"argocd/2-install-argocd/#step-4-verify-argocd-resources-in-aks","title":"Step 4. Verify ArgoCD resources in AKS.","text":"<p>Run the following <code>kubectl</code> commands to verify the Argo CD installation in the AKS cluster.</p> <pre><code>kubectl get all --namespace argocd\n</code></pre> <p>or</p> <p><pre><code>kubectl get namespace argocd\nkubectl get deployments -n argocd\nkubectl get pods -n argocd\nkubectl get services -n argocd\n</code></pre> expected output</p> <pre><code>NAME                                                    READY   STATUS    RESTARTS   AGE\npod/argocd-application-controller-0                     1/1     Running   0          2m21s\npod/argocd-applicationset-controller-848fc4dcfb-vp5rs   1/1     Running   0          2m21s\npod/argocd-dex-server-56888697cd-5vnlw                  1/1     Running   0          2m21s\npod/argocd-notifications-controller-5cd6fc4886-p6wr4    1/1     Running   0          2m21s\npod/argocd-redis-b54b4ccd8-kdmdv                        1/1     Running   0          2m21s\npod/argocd-repo-server-78998f9d78-xv64h                 1/1     Running   0          2m21s\npod/argocd-server-c799cf854-9pnbw                       1/1     Running   0          2m21s\n\nNAME                                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/argocd-applicationset-controller   ClusterIP   10.25.250.229   &lt;none&gt;        7000/TCP            2m21s\nservice/argocd-dex-server                  ClusterIP   10.25.247.199   &lt;none&gt;        5556/TCP,5557/TCP   2m21s\nservice/argocd-redis                       ClusterIP   10.25.211.159   &lt;none&gt;        6379/TCP            2m21s\nservice/argocd-repo-server                 ClusterIP   10.25.233.23    &lt;none&gt;        8081/TCP            2m21s\nservice/argocd-server                      ClusterIP   10.25.115.123   &lt;none&gt;        80/TCP,443/TCP      2m21s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argocd-applicationset-controller   1/1     1            1           2m21s\ndeployment.apps/argocd-dex-server                  1/1     1            1           2m21s\ndeployment.apps/argocd-notifications-controller    1/1     1            1           2m21s\ndeployment.apps/argocd-redis                       1/1     1            1           2m21s\ndeployment.apps/argocd-repo-server                 1/1     1            1           2m21s\ndeployment.apps/argocd-server                      1/1     1            1           2m21s\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argocd-applicationset-controller-848fc4dcfb   1         1         1       2m21s\nreplicaset.apps/argocd-dex-server-56888697cd                  1         1         1       2m21s\nreplicaset.apps/argocd-notifications-controller-5cd6fc4886    1         1         1       2m21s\nreplicaset.apps/argocd-redis-b54b4ccd8                        1         1         1       2m21s\nreplicaset.apps/argocd-repo-server-78998f9d78                 1         1         1       2m21s\nreplicaset.apps/argocd-server-c799cf854                       1         1         1       2m21s\n\nNAME                                             READY   AGE\nstatefulset.apps/argocd-application-controller   1/1     2m21s\n</code></pre> <p><code>kubectl get configmaps -n argocd</code></p> <p><pre><code>NAME                        DATA   AGE\nargocd-cm                   6      3m6s\nargocd-cmd-params-cm        29     3m6s\nargocd-gpg-keys-cm          0      3m6s\nargocd-notifications-cm     1      3m6s\nargocd-rbac-cm              3      3m6s\nargocd-ssh-known-hosts-cm   1      3m6s\nargocd-tls-certs-cm         0      3m6s\nkube-root-ca.crt            1      11m\n</code></pre> <code>kubectl get secrets -n argocd</code> <pre><code>NAME                           TYPE                 DATA   AGE\nargocd-initial-admin-secret    Opaque               1      3m7s\nargocd-notifications-secret    Opaque               0      3m23s\nargocd-secret                  Opaque               5      3m23s\nsh.helm.release.v1.argocd.v1   helm.sh/release.v1   1      3m24s\n</code></pre></p> <p><code>kubectl get ingress -n argocd</code></p> <p>output</p> <pre><code>No resources found in argocd namespace.\n</code></pre>"},{"location":"argocd/2-install-argocd/#step-5-port-forwarding-for-login-screen","title":"Step-5. port forwarding for login screen","text":"<p>To perform local login testing for Argo CD, you can use port forwarding. Here are the steps:</p> <ol> <li>Connect to your AKS cluster using kubectl</li> <li>Forward the Argo CD server port to your local machine: <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre> output <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\nHandling connection for 8080\n</code></pre></li> <li>Access the Argo CD web interface by visiting https://localhost:8080 in your web browser.</li> </ol> <p>Note</p> <p>Use this URL for before patching - https://localhost:8080 Use this URL for after patching - http://localhost:8080/</p> <p></p> <p>Note</p> <p>The port forwarding can be stopped by cancelling the command with CTRL + C.</p>"},{"location":"argocd/2-install-argocd/#step-6-argocd-login-with-localhost","title":"Step-6. ArgoCD login with localhost","text":"<p>how to get argocd-initial-admin-secret?</p> <p>The default username is <code>admin</code> and the password is auto generated and stored as clear text. You can retrieve this password using the following command:</p> <p>PowerShell</p> <p><pre><code>kubectl get secret -n argocd argocd-initial-admin-secret -o json | ConvertFrom-Json | select -ExpandProperty data | % { $_.PSObject.Properties | % { $_.password + [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_.Value)) } }\n</code></pre> output <pre><code>nJYQaAQnb8wldLZ9\n</code></pre> Bash</p> <p><pre><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> You can use the following credentials for the first-time login into argocd website.</p> <ul> <li>Username - default argocd username is admin</li> <li>Password - get default argocd password from aks-&gt; configuration-&gt;secrets-&gt; argocd-initial-admin-secret</li> </ul> <p><pre><code>Username- admin\nPassword- nJYQaAQnb8wldLZ9\n</code></pre> once you are able to successfully login into ArgoCD you will see the following screen.</p> <p></p> <p>Change the password</p> <p>To change the password in Argo CD, you can login to argocd portal -&gt; user Info -&gt; Update Password.</p> <p>Note</p> <p>If your password is saved in the Azure Key vault then keep this password same as Key Vault.</p> <p></p> <p>Change the password using Kubectl </p> <ol> <li>Connect to the Argo CD server using kubectl: <pre><code>kubectl exec -it &lt;argocd-pod-name&gt; -n argocd -- argocd login &lt;argocd-server&gt;\n</code></pre></li> <li>Change the password for the admin user: <pre><code>kubectl exec -it &lt;argocd-pod-name&gt; -n argocd -- argocd account update-password --current-password &lt;current-password&gt; --new-password &lt;new-password&gt;\n</code></pre></li> </ol>"},{"location":"argocd/2-install-argocd/#reference","title":"Reference","text":"<ul> <li>Argo CD Installation</li> </ul>"},{"location":"argocd/3-install-argocd-cli/","title":"Install &amp; Interact with ArgoCD\u00a0CLI","text":""},{"location":"argocd/3-install-argocd-cli/#introduction","title":"Introduction","text":"<p>Argo CD, a powerful GitOps continuous delivery tool for Kubernetes, provides two interfaces for interacting with its server: the Argo CD Command-Line Interface (CLI) and the Argo CD web UI. In this guide, we will focus on installing the Argo CD CLI.</p> <p>Before delving into the installation process, let's distinguish between the Argo CD CLI and the Argo CD web UI. </p> <p>Argo CD CLI:</p> <ul> <li>The Argo CD CLI is a command-line interface that can be used to interact with an Argo CD server.</li> <li>It is a lightweight and fast way to perform tasks such as deploying, updating, and managing applications in Argo CD.</li> <li>The CLI provides advanced features such as scripting, automation, and integration with other tools and systems.</li> <li>The CLI provides direct access to the Argo CD API, allowing users to perform any action that is supported by the API.</li> </ul> <p>Argo CD web UI:</p> <ul> <li>The Argo CD web UI is a graphical user interface that can be used to interact with an Argo CD server.</li> <li>It provides a visual representation of the applications and their current state in Argo CD, making it easy to manage and troubleshoot applications.</li> <li>The web UI provides an intuitive and user-friendly interface that requires no programming or scripting knowledge.</li> <li>It provides additional features such as visualization of the application dependencies and comparison of the application configurations.</li> </ul>"},{"location":"argocd/3-install-argocd-cli/#technical-scenario","title":"Technical Scenario","text":"<p>You, as a <code>DevSecOps Engineer</code>, have been tasked with setting up the Argo CD CLI on your local environment. This will empower you to utilize the Argo CD CLI for local Kubernetes cluster management, especially when the Argo CD Web UI interface encounters issues or limitations.</p>"},{"location":"argocd/3-install-argocd-cli/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Install Argo CD CLI</li> <li>Step 2. Access the Argo CD API Server</li> <li>Step 3. Login to Argo CD</li> <li>Step 4. Logout Argo CD</li> </ul>"},{"location":"argocd/3-install-argocd-cli/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation of Argo CD CLI, ensure you have the following prerequisites in place:</p> <ol> <li> <p>Azure Subscription: Sign up for an Azure subscription, if not already done.</p> </li> <li> <p>Azure CLI: Install Azure CLI for Azure service interaction.</p> </li> <li> <p>Kubectl: Install and set up kubectl for managing Kubernetes clusters.</p> </li> <li> <p>Kubernetes Cluster: Ensure you have a running Kubernetes cluster available for ArgoCD deployment.</p> </li> <li> <p>Argo CD Server: Confirm that the Argo CD server is deployed and accessible in your Kubernetes cluster.</p> </li> <li> <p>Access Permissions: Verify that you have the necessary permissions to access and manage resources within the Kubernetes cluster. This includes permissions to deploy and update applications.</p> </li> </ol>"},{"location":"argocd/3-install-argocd-cli/#step-1install-argo-cd-cli","title":"Step 1.Install Argo CD CLI","text":"<p>Install Argo CD CLI in windows using choco</p> <p>Here are the instructions to install Argo CD CLI in Windows using Chocolatey (choco):</p> <ol> <li>Open the Windows Command Prompt or PowerShell as an administrator.</li> <li>Install Chocolatey package manager by running the following command: <pre><code>Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n</code></pre></li> <li>Once Chocolatey is installed, run the following command to install the Argo CD CLI: <pre><code>choco install argocd-cli\n</code></pre> </li> <li>Wait for the installation to complete. Chocolatey will automatically download and install the latest version of the Argo CD CLI.</li> <li>Verify that the Argo CD CLI is installed correctly by running the following command: <pre><code>argocd version\n</code></pre> output <pre><code>argocd: v2.4.7+81630e6\n  BuildDate: 2022-07-18T21:49:23Z\n  GitCommit: 81630e6d5075ac53ac60457b51343c2a09a666f4\n  GitTreeState: clean\n  GoVersion: go1.18.3\n  Compiler: gc\n  Platform: windows/amd64\nargocd-server: v2.5.2+148d8da\n</code></pre></li> </ol> <p>This command should display the version of the Argo CD CLI installed on your system.</p> <p>That's it! You have successfully installed the Argo CD CLI in Windows using Chocolatey. You can now use the CLI to interact with your Argo CD installation.</p> <p>Install Argo CD CLI in Mac</p> <p>Here are the instructions to install the Argo CD CLI on Mac:</p> <ol> <li>Open the Terminal app on your Mac.</li> <li>Install the Homebrew package manager by running the following command: <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nOnce Homebrew is installed, run the following command to install the Argo CD CLI:\n</code></pre> <pre><code>brew install argocd\n</code></pre></li> <li>Wait for the installation to complete. Homebrew will automatically download and install the latest version of the Argo CD CLI.</li> <li>Verify that the Argo CD CLI is installed correctly by running the following command: <pre><code>argocd version\n</code></pre> This command should display the version of the Argo CD CLI installed on your system.</li> </ol> <p>That's it! You have successfully installed the Argo CD CLI on your Mac. You can now use the CLI to interact with your Argo CD installation.</p>"},{"location":"argocd/3-install-argocd-cli/#step-2-access-the-argo-cd-api-server","title":"Step 2. Access the Argo CD API Server","text":"<p>By default, the Argo CD API server is not exposed with an external IP. To access the API server, choose one of the following techniques to expose the Argo CD API server:</p> <p><code>Service Type Load Balancer</code> Change the argocd-server service type to LoadBalancer by running following command in bash </p> <p><pre><code>kubectl patch svc argocd-server -n argocd -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'\n</code></pre> output</p> <pre><code>service/argocd-server patched\n</code></pre> <p>reference - https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli</p> <p>verify the new external IP address assigned to <code>argocd-server</code> service</p> <p><pre><code>kubectl get svc -n argocd\n</code></pre> output <pre><code>NAME                               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\nargocd-applicationset-controller   ClusterIP      10.25.250.229   &lt;none&gt;          7000/TCP                     139m\nargocd-dex-server                  ClusterIP      10.25.247.199   &lt;none&gt;          5556/TCP,5557/TCP            139m\nargocd-redis                       ClusterIP      10.25.211.159   &lt;none&gt;          6379/TCP                     139m\nargocd-repo-server                 ClusterIP      10.25.233.23    &lt;none&gt;          8081/TCP                     139m\nargocd-server                      LoadBalancer   10.25.115.123   20.124.172.79   80:30119/TCP,443:30064/TCP   139m\n</code></pre> now you can browse the argocd with new public IP address.</p> <p></p>"},{"location":"argocd/3-install-argocd-cli/#step-3-login-to-argo-cd","title":"Step 3. Login to Argo CD","text":"<pre><code> argocd login --port-forward-namespace argocd 20.124.172.79\n</code></pre> <pre><code>WARNING: server certificate had error: x509: certificate signed by unknown authority. Proceed insecurely (y/n)? y\nE0304 17:01:12.568936  151380 portforward.go:378] error copying from remote stream to local connection: readfrom tcp4 127.0.0.1:33532-&gt;127.0.0.1:33534: write tcp4 127.0.0.1:33532-&gt;127.0.0.1:33534: wsasend: An established connection was aborted by the software in your host machine.\nUsername: admin\nPassword:\n'admin:login' logged in successfully\nContext '20.124.172.79' updated\n</code></pre> <p>Note</p> <p>keyboard copy paste was not working on command prompt, so I've right clicked on the mouse to enter the correct password here. also make sure that you keep the password in the notepad before start this lab, Install ArgoCD lab has command to get the admin password.</p> <p>now let's verify the argocd cli login is working as expected by running following commands.</p> <pre><code>argocd cluster list\n</code></pre> <pre><code>SERVER                          NAME        VERSION  STATUS   MESSAGE                                                  PROJECT\nhttps://kubernetes.default.svc  in-cluster           Unknown  Cluster has no applications and is not being monitored.\n</code></pre> <pre><code>argocd app list\n</code></pre> <p><pre><code>NAME  CLUSTER  NAMESPACE  PROJECT  STATUS  HEALTH  SYNCPOLICY  CONDITIONS  REPO  PATH  TARGET\n</code></pre> there are no apps deployed yet.</p> <p>Update the password using argocd cli</p> <pre><code>argocd account update-password\n</code></pre>"},{"location":"argocd/3-install-argocd-cli/#step-4-logout-argo-cd","title":"Step 4. Logout Argo CD","text":"<p>use the following command for exit from the context</p> <p><pre><code>argocd logout 20.124.172.79\n</code></pre> output</p> <pre><code>Logged out from '20.124.172.79'\n</code></pre> <p>verify the logout</p> <p><pre><code> argocd app list\n</code></pre> output</p> <pre><code>time=\"2023-03-04T17:10:28-08:00\" level=fatal msg=\"rpc error: code = Unauthenticated desc = no session information\"\n</code></pre> <p>That's it! You can now use the Argo CD CLI to interact with the Argo CD API server. Note that some API commands may require administrative privileges, so make sure you have the necessary permissions before using them.</p>"},{"location":"argocd/3-install-argocd-cli/#reference","title":"Reference","text":"<ul> <li>https://argo-cd.readthedocs.io/en/stable/cli_installation/</li> </ul>"},{"location":"argocd/argocd-project/","title":"Create new ArgoCD Project","text":""},{"location":"argocd/argocd-project/#introduction","title":"Introduction","text":"<p>Argo CD Projects explained</p> <p>ArgoCD Projects are a way to group related applications together in order to manage them as a single unit. When you create a project, you can define access control policies that limit which users or teams can access and deploy applications within the project. You can also define shared configuration settings that are applied to all applications within the project. This can help to ensure consistency across deployments and reduce the risk of misconfigurations.</p> <p>ArgoCD Projects provide a logical boundary for organizing and managing deployments. </p> <p>ArgoCD Projects are a key feature of ArgoCD that provide a powerful tool for managing deployments and enforcing security policies across complex environments.</p> <p>Projects provide a logical grouping of applications, which is useful when Argo CD is used by multiple teams.</p> <p>for example: </p> <ul> <li> <p>Project-1</p> <ul> <li>Application-1</li> <li>Application-2</li> <li>Application-3</li> <li>Application-4</li> </ul> </li> <li> <p>Project-2</p> <ul> <li>Application-5</li> <li>Application-6</li> <li>Application-7</li> <li>Application-8</li> </ul> </li> </ul> <p>Projects provide the following features:</p> <ul> <li>Restrict <code>what</code> may be deployed (trusted Git source repositories)</li> <li>Restrict <code>where</code> apps may be deployed to (destination AKS clusters and namespaces)</li> <li>Restrict what <code>kinds of objects</code> may or may not be deployed (e.g., RBAC, CRDs, DaemonSets, NetworkPolicy etc\u2026)</li> </ul> <p>By default, argo cd comes with \u201cdefault\u201d project.</p> <p>Default Project</p> <p>Every application belongs to a single project. If unspecified, an application belongs to the default project, which is created automatically and by default, permits deployments from any source repo to any cluster, and all resource Kinds. The default project can be modified, but not deleted. When initially created, it\u2019s specification is configured to be the most permissive:</p> <pre><code>spec:\n  sourceRepos:\n  - '*'\n  destinations:\n  - namespace: '*'\n    server: '*'\n  clusterResourceWhitelist:\n  - group: '*'\n    kind: '*'\n</code></pre> <p>Project can be created in 3 ways in ArgoCD:</p> <ul> <li>Declarative</li> <li>CLI</li> <li>Web UI</li> </ul>"},{"location":"argocd/argocd-project/#prerequisites","title":"Prerequisites","text":"<ul> <li>login to Azure</li> <li>Connect to AKS cluster</li> <li>Login to argocd</li> <li>Login to azure devops</li> </ul>"},{"location":"argocd/argocd-project/#implementation-details","title":"Implementation Details","text":"<p>In this exercise we will accomplish &amp; learn how to <code>Create a new project in ArgoCD using Web UI</code></p> <ol> <li>Log in to the ArgoCD web UI.</li> <li>Click on the \"Settings\" icon in the left sidebar.</li> <li>Select \"Projects\" from the dropdown menu.</li> <li>Click the \"New Project\" button.</li> <li>Enter a name for the new project.</li> <li>(Optional) Add a description for the project.</li> <li>Specify the source of the project. You can select a Git repository or a Helm chart repository as the source.</li> <li>(Optional) Set up SSO or RBAC for the project.</li> <li>Click \"Create\" to create the new project.</li> </ol> <p></p> <p></p> <p>Once the project is created, you can use it to manage and deploy applications. You can add applications to the project by either connecting to a Git repository or adding them manually.</p>"},{"location":"argocd/argocd-repo/","title":"Create new Repository in ArgoCD","text":""},{"location":"argocd/argocd-repo/#introduction","title":"Introduction","text":"<p>Argo CD repository explained</p> <p>Argo CD repository is a Git repository that contains Kubernetes manifests for the applications you want to deploy with Argo CD. The repository can be either public or private and can be hosted on various Git platforms such as Azure DevOps, GitHub, GitLab, Bitbucket, etc. Argo CD uses GitOps methodology to manage the deployment of applications to a Kubernetes cluster. With GitOps, the desired state of the applications is defined in Git repositories and Argo CD constantly monitors the repositories to ensure that the deployed applications are in sync with the desired state.</p> <p>In order to use Argo CD with a Git repository, you need to configure the repository as a source of truth for your applications. You can do this by adding the repository as a source in Argo CD's web interface or CLI. Argo CD can then pull the latest changes from the repository and apply them to the Kubernetes cluster, ensuring that the cluster is always in sync with the desired state defined in the repository.</p> <p>Argo CD supports various types of Kubernetes manifests including YAML, JSON, and Helm charts. You can organize your manifests into different directories or subdirectories within the repository to group your applications logically. You can also use tools like Kustomize to manage overlays and patches for your manifests.</p> <p>Argo CD repository is an essential component of the Argo CD deployment pipeline. By using Git repositories as a source of truth, you can ensure that the deployment of applications is fully auditable, version-controlled, and repeatable. This provides a consistent and reliable way to manage deployments in a Kubernetes cluster.</p>"},{"location":"argocd/argocd-repo/#implementation-details","title":"Implementation Details","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Create a new repo in ArgoCD</li> </ul>"},{"location":"argocd/argocd-repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>login to Azure</li> <li>Connect to AKS cluster</li> <li>Login to argocd</li> <li>Login to azure devops</li> </ul> <p>To configure a new repository as a source for Argo CD, follow these steps:</p> <ol> <li>Log in to the Argo CD web interface.</li> <li>Click on the \"Settings\" icon in the left sidebar.</li> <li>Select \"Repositories\" from the dropdown menu.</li> <li>Click the \"New Repository\" button.</li> <li>Enter a name for the repository and the URL to the Git repository.</li> <li>Specify the type of the repository: Git or Helm.</li> <li>(Optional) Set up SSH or HTTPS access to the repository.</li> <li>To use SSH access, you can add the SSH key to the repository using the Git platform's interface. Then, in Argo CD, choose \"SSH Private Key\" as the authentication type and enter the SSH private key.</li> <li>To use HTTPS access, you can enter the username and password for the repository in Argo CD, or you can set up a personal access token (PAT) and enter that instead.</li> <li>(Optional) Set up credentials for the repository if it requires authentication.</li> <li>If your repository requires authentication, you can enter the username and password or the PAT in the repository settings.</li> <li>(Optional) Enable automatic synchronization of the repository.</li> <li>If you want Argo CD to automatically synchronize the repository with the cluster when changes are made, you can enable automatic synchronization.</li> <li>Click \"Create\" to create the new repository.</li> </ol> <p>In this lab we are going to use (step-11) PAT of the Azure DevOps to connection to private Git repo.</p> <p>Steps to create personal access token from azure devops</p> <p>To create a personal access token (PAT) in Azure DevOps, follow these steps:</p> <ul> <li> <p>Go to your Azure DevOps organization or project and click on your profile picture on the top right corner.</p> </li> <li> <p>Select \"Security\" from the dropdown menu.</p> </li> <li> <p>Click on \"Personal access tokens\" and then \"New Token\".</p> </li> <li> <p>Choose a name for your token and select the appropriate organization and project.</p> </li> <li> <p>Select the desired scope for the token by checking the boxes next to the required permissions.</p> </li> <li> <p>Choose an expiration date or select \"Never\" if you want the token to never expire.</p> </li> <li> <p>Click \"Create\" to generate the token.</p> </li> <li> <p>Your token will be displayed on the screen. Copy the token and store it in a secure place.</p> </li> </ul> <p>Note: Be sure to keep your token safe and never share it with anyone else. You can use this token to authenticate your requests when accessing Azure DevOps APIs or when running command-line tools.</p> <p>After creation the PAT fill the following details </p> <ul> <li>Repository URL - copy &amp; paste azure DevOps URL</li> <li>Username - leave this empty for PAT scenario</li> <li>Password- copy &amp; paste the personal access token (PAT) from the azure devops</li> </ul> <p>Once the repository is configured as a source for Argo CD, you can use it to manage and deploy applications. You can add applications to the repository by either connecting to the Git repository or adding them manually. Argo CD will automatically detect changes in the repository and trigger a deployment when a new version of an application is available. You can also manually trigger deployments using the Argo CD web UI or CLI.</p>"},{"location":"argocd/argocd-repo/#reference","title":"Reference","text":"<ul> <li>https://argo-cd.readthedocs.io/en/stable/user-guide/private-repositories/</li> </ul>"},{"location":"argocd/deploy-app/","title":"Deploy Your First application with Argo CD","text":"<p>Argo CD Applications can be configured in three different ways: </p> <ul> <li>Using the Web UI</li> <li>Using the ArgoCD CLI</li> <li>Using Kubernetes Manifest files</li> </ul> <p>In this lab we will see all three methods.</p> <p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Creating application using ArgoCD CLI</li> <li>Task-2: Creating application using ArgoCD Web UI</li> <li>Task-3: Creating ArgoCD application using YAML</li> </ul>"},{"location":"argocd/deploy-app/#prerequisites","title":"Prerequisites","text":"<ul> <li>ArgoCD CLI</li> <li>AKS Cluster</li> </ul>"},{"location":"argocd/deploy-app/#task-1-creating-argocd-application-using-argocd-cli","title":"Task-1: Creating argocd application using ArgoCD CLI","text":"<p>To create an application with Argo CD using the argocd CLI, follow these steps:</p> <ul> <li>First, you need to install the argocd CLI. - already completed in previous lab</li> <li>Next, you need to log in to your Argo CD instance using the CLI. <pre><code>argocd login --insecure --port-forward-namespace argocd 20.124.172.79\n</code></pre> output <pre><code>Username: admin\nPassword: \nPassword: \n'admin:login' logged in successfully\nContext '20.124.172.79' updated\n</code></pre></li> </ul> <p>In this example I am going to start with guestbook application container created by ArgoCD team for testing.</p> <p>An example repository containing a guestbook application is available at https://github.com/argoproj/argocd-example-apps.git to demonstrate how Argo CD works.</p> <p>Run following command to create application with ArgoCD.</p> <p><pre><code>argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default\n</code></pre> output  <pre><code>application 'guestbook' created\n</code></pre> Now login into ArgoCD with following URL - http://20.124.172.79/</p> <p>Use following ArgoCD credentials for login.</p> <p><pre><code>Username- admin\nPassword- nJYQaAQnb8wldLZ9\n</code></pre> By default you will notice the application is outofsync status, we have to enable the auto-sync policy which is present inside the APP DETAILS by clicking on this demo application. after the Login to argocd.</p> <p></p> <p>Detailed view of the application </p> <p></p>"},{"location":"argocd/deploy-app/#task-2-creating-argocd-application-using-argocd-ui","title":"Task-2: Creating argocd application using argocd UI","text":"<p>To create an application with Argo CD using the Web UI, follow these steps:</p> <ul> <li>Log in to the Argo CD web UI by entering the URL of your Argo CD server into your web browser.</li> <li>Click on the \"New App\" button in the top-right corner of the screen.</li> <li>Fill out the application details, including the application name, project, source repository, and target cluster. <ul> <li>Application Name: A unique name for your application.</li> <li>Project: The project that this application belongs to.</li> <li>Repository URL: The URL of the Git repository that contains your application code.</li> <li>Revision: The Git revision to use for this deployment.</li> <li>Path: The path within the Git repository where your application manifests are located.</li> <li>Cluster: The Kubernetes cluster where your application will be deployed.</li> </ul> </li> <li>Configure your application's deployment settings, such as the container image, resource limits, and environment variables.</li> <li>Click on the \"Create\" button to create your application.</li> <li>Wait for Argo CD to synchronize your application with the target Kubernetes cluster. You can monitor the synchronization progress by clicking on the application in the Argo CD web UI.</li> <li>Once your application is synchronized, you can view its status and perform additional actions such as rolling back to a previous version, scaling up or down, or deploying to additional environments.</li> </ul> <p> </p> <p></p> <p></p> <p>That's it! You have now created an Argo CD application using the web UI. You can use the web UI to manage and monitor your applications, as well as perform other tasks such as configuring continuous delivery pipelines or integrating with external tools.</p> <p>Uou can now use some of the ArgoCD command for testing your ArgoCD CLI tool.</p> <pre><code>argocd cluster list\nargocd repo list\nargocd app list\nargocd app get guestbook.\nargocd app sync guestbook\nargocd logout 52.159.112.67\n</code></pre>"},{"location":"argocd/deploy-app/#task-3-creating-argocd-application-using-yaml-manifest","title":"Task-3: Creating argocd application using YAML manifest","text":"<p>Above two methods will be used only for quick testing but in real scenario you will actually create ArgoCD applications using YAML manifest by committing source code in Git repo.</p> <p>Create a YAML file containing the application configuration. Here's an example YAML manifest:</p> aspnet-api.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: aspnet-api\n  finalizers: []\nspec:\n  destination:\n    name: ''\n    namespace: sample\n    server: 'https://kubernetes.default.svc'\n  source:\n    path: sample/aspnet-api\n    repoURL: 'https://dev.azure.com/keesari/microservices/_git/argocd'\n    targetRevision: develop\n    kustomize:\n      images: []\n  project: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> <p>This YAML manifest defines an Argo CD application called <code>aspnet-api</code> that is sourced from a Git repository at https://dev.azure.com/keesari/microservices/_git/argocd. The application is configured to deploy to the default namespace of the Kubernetes cluster, and the synchronization policy is set to automated with pruning and self-healing enabled.</p> <p>Apply YAML file</p> <p><pre><code>kubectl apply -f aspnet-api.yaml\n</code></pre> This will create the Argo CD application on your Kubernetes cluster.</p> <p>Verify that the application has been created by running kubectl command:</p> <p>Wait for Argo CD to synchronize the application with your target Kubernetes cluster. You can monitor the synchronization progress by checking the application status using the Argo CD web UI.</p> <p>That's it! You have now created an Argo CD application using a YAML manifest. You can modify the YAML manifest to configure additional settings for your application, such as environment variables, resource limits, or deployment strategies.</p>"},{"location":"argocd/deploy-app/#reference","title":"Reference:","text":"<ul> <li>https://techcommunity.microsoft.com/t5/apps-on-azure-blog/getting-started-with-gitops-argo-and-azure-kubernetes-service/ba-p/3288595 - MSDN site</li> <li>https://github.com/argoproj/argocd-example-apps</li> <li>https://argo-cd.readthedocs.io/en/stable/getting_started/ - official argocd website</li> </ul>"},{"location":"argocd/register-cluster/","title":"Registering an AKS Cluster with Argo CD","text":"<p>Since we've already installed ArgoCD helm chart in AKS cluster, this AKS cluster is called \"in-cluster\" with URL - https://kubernetes.default.svc.</p> <p>Argo CD can be run in two modes:</p> <ul> <li>In-cluster mode - where it is deployed as a Kubernetes Deployment in the same cluster that it manages.</li> <li>External cluster mode -where it is deployed outside the Kubernetes cluster that it manages.</li> </ul> <p>in-cluster mode is the recommended way to run Argo CD as it provides better security, scalability, ease of management, and resilience. However, external cluster mode may be necessary in certain situations, such as managing multiple Kubernetes clusters or integrating with external systems.</p> <p>For in-cluster we don't need to do any registrations but in order to deploy applications to an external Kubernetes cluster, you will need to register an external K8s cluster with Argo CD.</p> <p>Also, note that you will not able to register a new K8 cluster in the Argo CD web UI. You can only register a new K8s cluster from the Argo CD CLI. </p>"},{"location":"argocd/register-cluster/#implementation-details","title":"Implementation Details","text":"<p>To register an Azure Kubernetes Service (AKS) cluster with Argo CD using the argocd CLI, follow these steps:</p> <ul> <li>First, you need to install the argocd CLI.</li> <li>Next, you need to log in to your Argo CD instance using the CLI. <pre><code>argocd login --insecure --port-forward-namespace argocd 20.124.172.79\n</code></pre> output <pre><code>Username: admin\nPassword: \nPassword: \n'admin:login' logged in successfully\nContext '20.124.172.79' updated\n</code></pre></li> <li>Create a Kubernetes context for your AKS cluster using the Azure CLI.  <pre><code>az aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster2-dev\" --admin\n</code></pre></li> <li>Use the argocd CLI to add your AKS cluster to Argo CD.  <pre><code>argocd cluster add aks-cluster2-dev\n</code></pre></li> <li>After completing the previous steps you can run argocd cluster list again or go into the portal. You will see your new cluster added. <pre><code>argocd cluster list\n</code></pre></li> </ul>"},{"location":"argocd/register-cluster/#reference","title":"Reference","text":""},{"location":"azure/1-iac/","title":"Infrastructure as Code (IaC)","text":"<p>Chapter-2: Create Azure Infrastructure with Terraform</p>"},{"location":"azure/1-iac/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":""},{"location":"azure/1-iac/#overview","title":"Overview","text":"<p>This is the second chapter of the book, In this second chapter you will learn on how to create the infrastructure in azure cloud, the azure resources created as part of this chapter will be used for deploying our microservices applications created in chapter-1.</p> <p>Creation of these Azure resources will be completely automated with IaC approach using Terraform configuration and azure DevOps pipelines; </p>"},{"location":"azure/1-iac/#infrastructure-as-code-iac_1","title":"Infrastructure as Code (IaC)","text":"<p>Let's quickly talk about Infrastructure as Code (IaC) in few lines here. </p> <p><code>Infrastructure as Code (IaC)</code> is an approach to managing IT infrastructure in which the infrastructure is defined and managed using code, rather than through manual configuration. IaC is a set of practices and tools that allow developers and operations teams to automate the process of deploying and managing infrastructure.</p> <p>With IaC, infrastructure is defined using either Terraform, YAML or JSON, which describes the desired state of the infrastructure. This code is then stored in a version control system like Azure DevOps, allowing teams to collaborate on changes and track changes over time. The code can then be used to provision infrastructure resources, such as servers, networks, and storage, databases in an automated and repeatable way.</p>"},{"location":"azure/1-iac/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level architecture of IaC process.</p> <p></p>"},{"location":"azure/1-iac/#benefits-of-iac","title":"Benefits of IaC","text":"<p>Some benefits of IaC include:</p> <ul> <li>Faster deployment: IaC allows for rapid and consistent deployment of infrastructure, reducing the time it takes to set up and configure environments.</li> <li>Improved reliability: By automating the deployment and management of infrastructure, IaC reduces the risk of human error and ensures consistency across environments.</li> <li>Increased scalability: IaC enables teams to easily scale infrastructure resources up or down as needed, without manual intervention.</li> <li>Better collaboration: By storing infrastructure code in version control, teams can collaborate on changes and track changes over time.</li> <li>Greater agility: IaC allows for more agile development and deployment, making it easier to iterate quickly and respond to changing requirements.</li> </ul> <p>There are several tools available for implementing IaC, including Terraform, AWS CloudFormation, and Ansible. These tools allow teams to define infrastructure as code and manage the deployment and configuration of infrastructure resources in an automated and repeatable way.</p>"},{"location":"azure/1-iac/#terraform","title":"Terraform","text":"<p>Here we've chosen Terraform because of following:</p> <p><code>Terraform</code> is a popular infrastructure as code tool that enables users to define, provision, and manage infrastructure resources across multiple cloud provider. Here are some advantages of Terraform over other tools:</p> <ul> <li>Multi-cloud support: Terraform supports multiple cloud providers, including AWS, Azure, Google Cloud, and more. This allows users to manage resources across multiple providers using a single configuration language and tool.</li> <li>Declarative syntax: Terraform uses a declarative syntax to define infrastructure resources. This makes it easy to understand and maintain infrastructure code over time, as well as to collaborate on changes with team members.</li> <li>Infrastructure as code best practices: Terraform follows infrastructure as code best practices, such as version control, code review, and testing, making it easier to manage infrastructure as part of a software development process.</li> <li>Modularity and reusability: Terraform allows users to define reusable modules that can be shared across multiple infrastructure projects. This promotes code reuse and simplifies the process of managing infrastructure resources.</li> <li>Community support: Terraform has a large and active community of users who contribute to the development of the tool, as well as share best practices, tips, and modules. This provides a wealth of resources and support for users of the tool.</li> </ul> <p>Terraform provides a powerful and flexible tool for managing infrastructure as code that is well-suited for complex and multi-cloud environments. Its declarative syntax, modularity, and community support make it a popular choice for teams looking to adopt infrastructure as code best practices.</p>"},{"location":"azure/1-microservices-architecture-on-aks/","title":"Microservices Architecture on AKS","text":"<p>To host and deploy the containerized microservices and databases created in the first chapter, we will utilize various Azure resources. These resources will be provisioned using the Infrastructure as Code (IaC) tool, Terraform. Additionally, we will explore the deployment process using Azure DevOps pipelines, Helm charts, and ArgoCD.</p> <p>We will establish a scalable and resilient infrastructure for hosting our containerized microservices applications. The combination of Azure resources, Terraform for infrastructure provisioning, Azure DevOps pipelines for deployment automation, Helm charts for application packaging, and ArgoCD for continuous delivery simplifies the management and deployment of our Microservices Architecture.</p> <p>Let's look into the details of each component and explore how they come together to create a robust and efficient Microservices Architecture on Azure Kubernetes Service (AKS).</p>"},{"location":"azure/1-microservices-architecture-on-aks/#high-level-architecture","title":"High Level Architecture","text":"<p>The high-level architecture of the Azure resources, which serves as a reference architecture in our labs, is visualized in the diagram below. This diagram provides an overview of the essential components and how they interact within the Microservices Architecture on AKS.</p> <p>Azure High Level Architecture</p> <p></p> <p>DevOps High Level Architecture</p> <p></p>"},{"location":"azure/1-microservices-architecture-on-aks/#azure-reference-architecture-components","title":"Azure Reference Architecture Components","text":"<p>Log Analytics workspace:</p> <p>A Log Analytics workspace enables us to store and analyze log data from various sources within the Microservices architecture. We will utilize this resource for centralized logging and monitoring of our services.</p> <p>Virtual network:</p> <p>All the services created in our labs will be secured within a private Virtual Network. We will demonstrate the Hub &amp; Spoke VNet model in the lab, which provides a scalable and isolated network environment for our microservices.</p> <p>Azure Container Registry (ACR):</p> <p>We will use Azure Container Registry to store our private Docker images, which will be deployed to the cluster. AKS can authenticate with ACR using its Azure AD identity, ensuring secure and controlled access to our container images.</p> <p>Application Gateway:</p> <p>The Application Gateway serves as the entry point for client requests from the public DNS. It acts as a reverse proxy and load balancer, routing requests to the appropriate backend services. With features like SSL/TLS termination and backend pool configuration, the Application Gateway helps to decouple clients from services.</p> <p>Azure Kubernetes Service (AKS):</p> <p>AKS is a fully managed Kubernetes service that offers a platform for deploying and scaling containerized applications. It provides a scalable, resilient, and easy-to-manage environment for deploying microservices. We will utilize AKS for deploying all our microservices in the lab.</p> <p>Azure Database:</p> <p>Azure Database for PostgreSQL - Flexible Server is a fully managed database service for running PostgreSQL on the Azure platform. We will create a PostgreSQL - Flexible Server instance to easily create, configure, and manage PostgreSQL databases on Azure.</p> <p>Azure Key Vault:</p> <p>We will leverage Azure Key Vault, a cloud-based service, to securely store and manage cryptographic keys, certificates, and other secrets related to our microservices applications. Azure Key Vault ensures the secure storage and access of sensitive information.</p> <p>Azure Redis Cache:</p> <p>Azure Redis Cache is a fully managed, in-memory data store based on the open-source Redis cache. We will utilize this service for caching application-specific data, enabling fast data access and low latency for improved performance.</p> <p>Azure Storage account:</p> <p>Azure Storage account is a Microsoft cloud storage service that allows us to store and retrieve large amounts of unstructured data. We will use Blob Storage, a component of Azure Storage, for various purposes. Further details about Blob Storage will be covered during the labs.</p> <p>Azure Active Directory (AAD):</p> <p>AAD will be used for managing identity and authentication between different services. It provides a secure and centralized approach for authentication and access control, ensuring that only authorized users and services can access our microservices.</p> <p>Azure Pipelines </p> <p>Azure Pipelines are part of the Azure DevOps Services and run automated builds, tests, and deployments.</p> <p>Helm Chart </p> <p>Helm is a package manager for Kubernetes, we will use helm charts for deploying our Microservices into AKS.</p> <p>ArgoCD </p> <p>ArgoCD is continuous delivery tool for Kubernetes applications. It provides a simple and automated way to manage and deploy applications to a Kubernetes cluster by using GitOps methodology. we will use the ArgoCD for deploying our Microservices into AKS.</p> <p>Azure Security &amp; Governance</p> <p>Apart from above services we will also focus on following services:</p> <ul> <li> <p><code>Azure Policy</code>: a service that allows us to create, assign, and manage policies for our Azure resources, ensuring that they comply with organizational standards and regulatory requirements.</p> </li> <li> <p><code>Azure Active Directory (AAD)</code>: a service that provides identity and access management (IAM) capabilities, enabling us to secure access to out Azure resources.</p> </li> <li> <p><code>Azure Monitor</code>: a service that allows us to collect, analyze, and act on telemetry data from our Azure resources and applications, enabling us to detect and diagnose security issues.</p> </li> </ul> <p>Important</p> <p>Each lab in this chapter covers only implementation details therefore it is always recommended to read the relevant MSDN documentation on particular azure service before start any lab so that is supper easy for you to work on these labs. you will see the links end of each lab for your reference.</p>"},{"location":"azure/1-microservices-architecture-on-aks/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Microservices architecture on Azure Kubernetes Service</li> </ul>"},{"location":"azure/10-app-gateway/","title":"Create Azure Application Gateway using terraform","text":""},{"location":"azure/10-app-gateway/#introduction","title":"Introduction","text":"<p>Azure Application Gateway is like the traffic cop of your web applications. Azure Application Gateway is a web traffic load balancer provided by Microsoft Azure. It acts as a central point for managing and optimizing the traffic to web applications, ensuring they are secure, highly available, and performant. </p> <p>In this lab, I will take a detailed walk-through to create an Azure Application Gateway using Terraform. I will also create a Public IP address for the Application Gateway using Terraform. Then, I will configure diagnostic settings to ensure robust monitoring and troubleshooting. Finally, I will validate these resources within the Azure portal to confirm that everything is functioning as expected.</p> <p>Here are some key features of Azure Application Gateway:</p> <ol> <li> <p>Load Balancing: It distributes incoming network traffic across multiple servers to ensure even utilization and prevent overloading any single server. This results in better performance and fault tolerance.</p> </li> <li> <p>Web Application Firewall (WAF): Azure Application Gateway comes with a built-in web application firewall that helps protect web applications from common web exploits and vulnerabilities. It acts as a security guard, monitoring and filtering incoming traffic.</p> </li> <li> <p>SSL Termination: It handles SSL/TLS encryption and decryption on behalf of web servers, relieving them of the computational load involved in these processes.</p> </li> <li> <p>URL-Based Routing: You can configure Application Gateway to route traffic based on the URL path, making it versatile for directing requests to different services or applications.</p> </li> <li> <p>Session Affinity: This feature ensures that client requests from the same user are directed to the same backend server. It's essential for applications that require user sessions, like online shopping carts.</p> </li> <li> <p>Auto Scaling: Application Gateway can automatically adjust its capacity based on changes in web traffic. This means it can handle traffic spikes during high-demand periods.</p> </li> <li> <p>Health Probing: It continually checks the health of backend servers. If a server becomes unhealthy, it stops sending traffic to that server until it recovers.</p> </li> <li> <p>Path-Based Routing: You can route traffic to different backend pools based on the URL path. This is useful for hosting multiple services under the same domain.</p> </li> <li> <p>Multi-Site Hosting: You can host multiple websites or applications on a single Application Gateway, providing an economical way to serve multiple applications.</p> </li> <li> <p>Custom Error Pages: It allows you to create custom error pages, providing a better user experience when something goes wrong.</p> </li> </ol> <p>Web application firewall A Web Application Firewall (WAF) is a security solution designed to protect web applications from various online threats and attacks such as <code>SQL injection</code> and <code>cross-site scripting</code>. It acts as a filter, monitoring and controlling the traffic between a web application and the clients (typically web browsers). The primary purpose of a WAF is to identify and block malicious traffic and unauthorized access attempts, ensuring the security and integrity of web applications.</p>"},{"location":"azure/10-app-gateway/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, you've been tasked with enhancing the security and performance of a critical web application hosted in the Azure cloud environment. The application is the backbone of our business operations, handling sensitive customer data and serving as the primary touchpoint for our clients. To ensure its reliability and resilience in the face of evolving cyber threats, you're considering the implementation of Azure Application Gateway with a Web Application Firewall (WAF).</p> <p>Azure Application Gateway, combined with a Web Application Firewall (WAF), offers an ideal solution to address these concerns. </p>"},{"location":"azure/10-app-gateway/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare variables for  Application Gateway</li> <li>Task-2: Create Public IP address for Application Gateway using Terraform</li> <li>Task-3: Create Azure Application Gateway using Terraform</li> <li>Task-4: Configure diagnostic settings for Application Gateway using Terraform</li> </ul> <p>Through these tasks, you will gain practical experience on Azure Application Gateway.</p>"},{"location":"azure/10-app-gateway/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of Application Gateway components:</p> <p></p>"},{"location":"azure/10-app-gateway/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet</li> <li>Basic knowledge of terraform and Azure concepts.</li> </ol>"},{"location":"azure/10-app-gateway/#implementation-details","title":"Implementation details","text":"<p>Let's delve into the step-by-step implementation details:</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/10-app-gateway/#task-1-define-and-declare-variables-for-application-gateway","title":"Task-1: Define and declare variables for Application Gateway","text":"<p>In this task, we will define and declare the necessary variables for creating the Azure Application Gateway. These variables will be used to specify the Application Gateway's settings and customize the values as per the environment requirements.</p> <p>Here's the table with the variable names, their descriptions, variable type and their default values:</p> Variable Name Description Type Default Value appgtw_name (Required) Specifies the name of the Application Gateway. string appgtw-appgtw1-dev appgtw_rg_name (Required) The name of the resource group. string rg-resourcegroup1-dev appgtw_location (Required) The Azure region for the Application Gateway. string North Central US appgtw_sku_size (Required) Name of the SKU for the Application Gateway. string Standard_v2 appgtw_sku_tier (Required) The tier of the SKU for the Application Gateway. string Standard_v2 appgtw_sku_capacity (Required) The capacity of the SKU for the Application Gateway. string 1 appgtw_pip_name (Required) Specifies the name of the Public IP. string pip-appgtw-dev pip_allocation_method (Required) Defines the allocation method for the IP address. string Static pip_sku (Optional) The SKU of the Public IP. string Basic <p>Variable declaration:</p> variables.tf<pre><code>// Public IP\nvariable \"public_ip_prefix\" {\n  type        = string\n  default     = \"pip\"\n  description = \"Prefix of the public ip prefix resource.\"\n}\nvariable \"appgtw_pip_name\" {\n  description = \"(Required) Specifies the name of the Public IP.\"\n  type        = string\n}\nvariable \"pip_allocation_method\" {\n  description = \" (Required) Defines the allocation method for this IP address.\"\n  type        = string\n  default     = \"Static\"\n  validation {\n    condition     = contains([\"Static\", \"Dynamic\"], var.pip_allocation_method)\n    error_message = \"The allocation method is invalid.\"\n  }\n}\nvariable \"pip_sku\" {\n  description = \"(Optional) The SKU of the Public IP. Accepted values are Basic and Standard. Defaults to Basic.\"\n  type        = string\n  default     = \"basic\"\n  validation {\n    condition     = contains([\"basic\", \"Standard\"], var.pip_sku)\n    error_message = \"The sku is invalid.\"\n  }\n}\n// Application Gateway\nvariable \"appgtw_prefix\" {\n  type        = string\n  default     = \"appgtw\"\n  description = \"Prefix of the Application Gateway prefix resource.\"\n}\nvariable \"appgtw_name\" {\n  description = \"(Required) Specifies the name of the Application Gateway.\"\n  type        = string\n}\nvariable \"appgtw_rg_name\" {\n  description = \"(Required) The name of the resource group in which to the Application Gateway should exist.\"\n  type        = string\n  default     = \"\"\n}\nvariable \"appgtw_location\" {\n  description = \"(Required) The Azure region where the Application Gateway should exist.\"\n  type        = string\n  default     = \"North Central US\"\n}\nvariable \"appgtw_sku_size\" {\n  description = \"(Required) The Name of the SKU to use for this Application Gateway. Possible values are Standard_Small, Standard_Medium, Standard_Large, Standard_v2, WAF_Medium, WAF_Large, and WAF_v2.\"\n  type        = string\n  default     = \"Standard_v2\"\n  validation {\n    condition     = contains([\"Standard_Small\", \"Standard_Medium\", \"Standard_Large\", \"Standard_v2\", \"WAF_Medium\", \"WAF_Large\", \"WAF_v2\"], var.appgtw_sku_size)\n    error_message = \"The sku size is invalid.\"\n  }\n}\nvariable \"appgtw_sku_tier\" {\n  description = \"(Required) The Tier of the SKU to use for this Application Gateway. Possible values are Standard, Standard_v2, WAF and WAF_v2.\"\n  type        = string\n  default     = \"Standard_v2\"\n  validation {\n    condition     = contains([\"Standard\", \"Standard_v2\", \"WAF\", \"WAF_v2\"], var.appgtw_sku_tier)\n    error_message = \"The sku tier is invalid.\"\n  }\n}\nvariable \"appgtw_sku_capacity\" {\n  description = \"(Required) The Capacity  of the SKU to use for this Application Gateway. When using a V1 SKU this value must be between 1 and 32, and 1 to 125 for a V2 SKU. This property is optional if autoscale_configuration is set.\"\n  type        = string\n  default     = \"1\"\n}\nvariable \"waf_config_firewall_mode\" {\n  description = \"(Required) The Web Application Firewall Mode. Possible values are Detection and Prevention.\"\n  type        = string\n  default     = \"Detection\"\n  validation {\n    condition     = contains([\"Detection\", \"Prevention\"], var.waf_config_firewall_mode)\n    error_message = \"The Web Application Firewall Mode is invalid.\"\n  }\n}\nvariable \"waf_config_enable\" {\n  description = \"(Required) Is the Web Application Firewall enabled?\"\n  type        = bool\n  default     = true\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># application gateway\nappgtw_name                         = \"appgtw1\"\nappgtw_sku_size                     = \"WAF_v2\"\nappgtw_sku_tier                     = \"WAF_v2\"\nappgtw_sku_capacity                 = 2\nappgtw_pip_name                     = \"appgtw\"\npip_allocation_method               = \"Static\"\npip_sku                             = \"Standard\"\n</code></pre>"},{"location":"azure/10-app-gateway/#task-2-create-public-ip-address-for-application-gateway-using-terraform","title":"Task-2: Create Public IP address for Application Gateway using Terraform","text":"<p>In this task, we will create a Public IP address to be used by the Application Gateway.</p> <p>appgateway.tf<pre><code># Create public ip address for Application Gateway\nresource \"azurerm_public_ip\" \"appgtw_pip\" {\n  name                = lower(\"${var.public_ip_prefix}-${var.appgtw_pip_name}-${local.environment}\")\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = var.location\n  allocation_method   = var.pip_allocation_method\n  sku                 = var.pip_sku\n\n  tags = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Application Gateway - Public IP address</p> <p></p>"},{"location":"azure/10-app-gateway/#task-3-create-azure-application-gateway-using-terraform","title":"Task-3: Create Azure Application Gateway using Terraform","text":"<p>In this task, we will use Terraform to create the Azure Application Gateway with the specified configuration.</p> <p>Create local variables for Application Gateway</p> appgateway.tf<pre><code># Create local variables for Application Gateway\nlocals {\n  gateway_ip_configuration_name  = \"${var.appgtw_name}-configuration\"\n  frontend_port_name             = \"${var.appgtw_name}-feport\"\n  frontend_ip_configuration_name = \"${var.appgtw_name}-feip\"\n  backend_address_pool_name      = \"${var.appgtw_name}-beap\"\n  backend_http_settings_name     = \"${var.appgtw_name}-be-http\"\n  http_listener_name             = \"${var.appgtw_name}-http-listner\"\n  request_routing_rule_name      = \"${var.appgtw_name}-rqrt-rule\"\n  # redirect_configuration_name    = \"${var.appgtw_name}-rdrcfg\"\n  # diag_appgtw_logs = [\n  #   \"ApplicationGatewayAccessLog\",\n  #   \"ApplicationGatewayPerformanceLog\",\n  #   \"ApplicationGatewayFirewallLog\",\n  # ]\n  # diag_appgtw_metrics = [\n  #   \"AllMetrics\",\n  # ]\n}\n</code></pre> <p>Create Application Gateway using terraform</p> <p>appgateway.tf<pre><code># Create Application Gateway using terraform\nresource \"azurerm_application_gateway\" \"appgtw\" {\n  name                = lower(\"${var.appgtw_prefix}-${var.appgtw_name}-${local.environment}\")\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = azurerm_resource_group.rg.location\n\n  sku {\n    name     = var.appgtw_sku_size\n    tier     = var.appgtw_sku_tier\n    capacity = var.appgtw_sku_capacity\n  }\n  waf_configuration {\n    firewall_mode    = var.waf_config_firewall_mode\n    enabled          = var.waf_config_enable\n    rule_set_version = 3.1\n  }\n  gateway_ip_configuration {\n    name      = local.gateway_ip_configuration_name\n    subnet_id = azurerm_subnet.appgtw.id\n  }\n  frontend_port {\n    name = local.frontend_port_name\n    port = 80\n  }\n  frontend_port {\n    name = \"httpsPort\"\n    port = 443\n  }\n  frontend_ip_configuration {\n    name                 = local.frontend_ip_configuration_name\n    public_ip_address_id = azurerm_public_ip.appgtw_pip.id\n  }\n\n  backend_address_pool {\n    name = local.backend_address_pool_name\n  }\n\n  backend_http_settings {\n    name                  = local.backend_http_settings_name\n    cookie_based_affinity = \"Disabled\"\n    path                  = \"/path1/\"\n    port                  = 80\n    protocol              = \"Http\"\n    request_timeout       = 60\n    # connection_draining { //TODO: review this\n    #   enabled = true\n    #   drain_timeout_sec = 30\n    # }\n  }\n\n  http_listener {\n    name                           = local.http_listener_name\n    frontend_ip_configuration_name = local.frontend_ip_configuration_name\n    frontend_port_name             = local.frontend_port_name\n    protocol                       = \"Http\"\n  }\n\n  request_routing_rule {\n    name                       = local.request_routing_rule_name\n    rule_type                  = \"Basic\"\n    http_listener_name         = local.http_listener_name\n    backend_address_pool_name  = local.backend_address_pool_name\n    backend_http_settings_name = local.backend_http_settings_name\n    priority                   = 1\n  }\n  tags = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags,\n      tags[\"ingress-for-aks-cluster-id\"],\n      tags[\"managed-by-k8s-ingress\"],\n      backend_address_pool,\n      backend_http_settings,\n      frontend_ip_configuration,\n      gateway_ip_configuration,\n      frontend_port,\n      http_listener,\n      probe,\n      request_routing_rule,\n      redirect_configuration,\n      ssl_certificate,\n      ssl_policy,\n      waf_configuration,\n      autoscale_configuration,\n      url_path_map,\n      rewrite_rule_set\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_subnet.appgtw,\n    azurerm_public_ip.appgtw_pip\n  ]\n}\n</code></pre> Run Terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Application gateway - Overview blade </p> <p></p> <p>Application gateway - Configuration</p> <p></p> <p>Application gateway - Web application firewall (WAF)</p> <p></p>"},{"location":"azure/10-app-gateway/#task-4-configure-diagnostic-settings-for-application-gateway-using-terraform","title":"Task-4: Configure diagnostic settings for Application Gateway using terraform","text":"<p>In this task, we will configure diagnostic settings to monitor and analyze the performance and behavior of the Application Gateway.</p> appgateway.tf<pre><code># Create Diagnostic Settings for Application Gateway\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_apptw\" {\n  name                       = \"${var.diag_prefix}-${azurerm_application_gateway.appgtw.name}\"\n  target_resource_id         = azurerm_application_gateway.appgtw.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  # dynamic \"log\" {\n  #   for_each = local.diag_appgtw_logs\n  #   content {\n  #     category = log.value\n\n  #     retention_policy {\n  #       enabled = false\n  #     }\n  #   }\n  # }\n\n  log {\n    category = \"ApplicationGatewayAccessLog\"\n    enabled  = true\n  }\n\n  log {\n    category = \"ApplicationGatewayFirewallLog\"\n    enabled  = true\n  }\n\n  log {\n    category = \"ApplicationGatewayPerformanceLog\"\n    enabled  = false\n  }\n\n  # dynamic \"metric\" {\n  #   for_each = local.diag_appgtw_metrics\n  #   content {\n  #     category = metric.value\n\n  #     retention_policy {\n  #       enabled = false\n  #     }\n  #   }\n  # }\n\n  metric {\n    category = \"AllMetrics\"\n    enabled  = false\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Application gateway - Diagnostic settings</p> <p></p>"},{"location":"azure/10-app-gateway/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Application Gateway documentation</li> <li>Microsoft MSDN - Web Application Firewall documentation</li> <li>Microsoft MSDN - Quickstart: Direct web traffic with Azure Application Gateway - Terraform</li> <li>Microsoft MSDN - Tutorial: Create an application gateway with a web application firewall using Terraform</li> <li>Azure Terraform Quickstart/101-application-gateway</li> <li>Terraform Registry - azurerm_public_ip</li> <li>Terraform Registry - azurerm_application_gateway</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Azure Terraform Quickstart/101-application-gateway</li> </ul>"},{"location":"azure/11-aks/","title":"Create Azure Kubernetes Service (AKS) using terraform","text":""},{"location":"azure/11-aks/#introduction","title":"Introduction","text":"<p>Azure Kubernetes Service (AKS) is a fully managed container orchestration service provided by Microsoft Azure. It allows you to easily deploy, scale, and manage containerized applications using Kubernetes.</p> <p>In this lab, I will walk you through the steps to create an Azure Kubernetes Service (AKS) using Terraform. Additionally, I will show you how to confirm its successful deployment through the Azure portal and provide insights on how to utilize it effectively. We will also cover how to validate your AKS cluster using Kubectl and set up access to Azure Container services.</p> <p>AKS provides a number of benefits, including:</p> <ul> <li> <p>Autoscaling: AKS automatically scales your cluster up or down based on the demand for your applications, so you only pay for what you use.</p> </li> <li> <p>High availability: AKS provides a highly available Kubernetes control plane, which ensures that the cluster remains operational even in the event of a failure.</p> </li> <li> <p>Simplified deployment: You can deploy a fully functional Kubernetes cluster with just a few clicks or commands, and AKS automatically provisions and manages the underlying infrastructure.</p> </li> <li> <p>Seamless integration: AKS integrates with other Azure services, such as Azure Active Directory, Azure Monitor, and Azure Virtual Networks, making it easy to incorporate AKS into your existing workflows.</p> </li> <li> <p>Security and compliance: AKS provides built-in security features, such as role-based access control, network security, and encryption at rest, to help you meet your security and compliance requirements.</p> </li> <li> <p>Monitoring and logging: AKS provides built-in monitoring and logging capabilities, allowing developers to quickly identify and troubleshoot issues in the cluster.</p> </li> <li> <p>Hybrid and multi-cloud support: AKS is built on open source Kubernetes, so you can run your workloads on any cloud or on-premises environment that supports Kubernetes.</p> </li> </ul> <p>To get started with AKS, we are going to use terraform to create a new Kubernetes cluster. Once you have a cluster, you can deploy containerized applications to it using Kubernetes manifests or Helm charts, and scale and manage your applications using the Kubernetes API or a variety of third-party tools.</p>"},{"location":"azure/11-aks/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you have been asked to create a new Azure Kubernetes services (AKS) cluster so that you can deploy containerized Microservice applications to it using Kubernetes manifests or Helm charts.</p>"},{"location":"azure/11-aks/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Configure variables for AKS</li> <li>Task-2: Create a new resource group for AKS</li> <li>Task-3: Create AKS user assigned identity</li> <li> <p>Task-3: Create a new AKS cluster using terraform    </p> </li> <li> <p>Task-5: Create diagnostics settings for AKS</p> </li> <li>Task-6: Review AKS Cluster resource in the portal</li> <li>Task-7: Validate AKS cluster running Kubectl</li> <li>Task-8: Allow AKS Cluster access to Azure Container</li> <li>Task-9: Lock AKS cluster resource group</li> </ul> <p>Through these tasks, you will gain practical experience on Azure Kubernetes Service (AKS).</p>"},{"location":"azure/11-aks/#architecture-diagram","title":"Architecture diagram","text":"<p>Here is the reference architecture diagram of Azure Kubernetes services.</p> <p></p>"},{"location":"azure/11-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Visual studio code</li> <li>Azure DevOps Project &amp; repo</li> <li>Terraform Foundation Setup</li> <li>Log Analytics workspace</li> <li>Virtual Network with subnet for AKS</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"azure/11-aks/#implementation-details","title":"Implementation details","text":"<p>Open the terraform project folder in Visual Studio code and creating new file named <code>aks.tf</code> for Azure Kubernetes services (AKS) specific azure resources;</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/11-aks/#task-1-define-and-declare-aks-variables","title":"Task-1: Define and declare AKS variables","text":"<p>This section covers list of variables used to create  Azure Kubernetes services (AKS) with detailed description and purpose of each variable with default values.</p> Variable Name Description Type Default Value aks_rg_name The name of the resource group in which to create the AKS string aks_rg_location Location in which to deploy the AKS string East US cluster_name Specifies the name of the AKS cluster string dns_prefix DNS prefix specified when creating the managed cluster string private_cluster_enabled Should this Kubernetes Cluster have its API server only exposed on internal IP addresses bool false azure_rbac_enabled Is Role Based Access Control based on Azure AD enabled bool true admin_group_object_ids A list of Object IDs of Azure Active Directory Groups which should have Admin Role on the Cluster list(string) [\"c63746fd-eb61-xx\"] role_based_access_control_enabled Is Role Based Access Control Enabled? bool true automatic_channel_upgrade The upgrade channel for this Kubernetes Cluster string stable aks_sku_tier The SKU Tier that should be used for this Kubernetes Cluster string Free kubernetes_version Specifies the AKS Kubernetes version string 1.23.12 default_node_pool_vm_size Specifies the vm size of the default node pool string Standard_B4ms default_node_pool_availability_zones Specifies the availability zones of the default node pool list(string) [\"1\", \"2\", \"3\"] network_docker_bridge_cidr Specifies the Docker bridge CIDR string 172.17.0.1/16 network_dns_service_ip Specifies the DNS service IP string 10.25.0.10 network_service_cidr Specifies the service CIDR string 10.25.0.0/16 network_plugin Specifies the network plugin of the AKS cluster string azure network_policy Specifies the network policy of the AKS cluster string azure outbound_type The outbound (egress) routing method which should be used for this Kubernetes Cluster string userDefinedRouting default_node_pool_name Specifies the name of the default node pool string agentpool default_node_pool_subnet_name Specifies the name of the subnet that hosts the default node pool string SystemSubnet default_node_pool_subnet_address_prefix Specifies the address prefix of the subnet that hosts the default node pool list(string) [\"10.0.0.0/20\"] default_node_pool_enable_auto_scaling Whether to enable auto-scaler bool true default_node_pool_enable_host_encryption Should the nodes in this Node Pool have host encryption enabled bool false default_node_pool_enable_node_public_ip Should each node have a Public IP Address bool false default_node_pool_max_pods The maximum number of pods that can run on each agent number 110 default_node_pool_node_labels A list of Kubernetes taints which should be applied to nodes in the agent pool map(any) {} default_node_pool_node_taints A map of Kubernetes labels which should be applied to nodes in this Node Pool list(string) [] default_node_pool_os_disk_type The type of disk which should be used for the Operating System string Ephemeral default_node_pool_max_count The maximum number of nodes which should exist within this Node Pool number 5 default_node_pool_min_count The minimum number of nodes which should exist within this Node Pool number 2 default_node_pool_node_count The initial number of nodes which should exist within this Node Pool number 2 aks_log_analytics_retention_days Specifies the number of days of the retention policy number 30 azure_policy Specifies the Azure Policy addon configuration object { enabled = false } http_application_routing Specifies the HTTP Application Routing addon configuration object { enabled = false } kube_dashboard Specifies the Kubernetes Dashboard addon configuration object { enabled = false } admin_username Specifies the Admin Username for the AKS cluster worker nodes string azadmin ssh_public_key Specifies the SSH public key used to access the cluster string - aks_tags Specifies the tags of the AKS map(any) {} <p>Variables Prefixes</p> <p>Here is the list of new prefixes used in this lab, read the description and default values.</p> variables_prefix.tf<pre><code>variable \"aks_prefix\" {\n  type        = string\n  default     = \"aks\"\n  description = \"Prefix of the AKS name that's combined with name of the AKS\"\n}\nvariable \"diag_prefix\" {\n  type        = string\n  default     = \"diag\"\n  description = \"Prefix of the Diagnostic Settings resource.\"\n}\n</code></pre> <p>Declare Variables</p> <p>Here is the list of new variables used in this lab,  read the description and default values carefully. </p> <p>Important</p> <p>There are lot of variables here, make sure that you read the MSDN documentation carefully on each property before using it.</p> <p>variables.tf<pre><code>// ========================== Azure Kubernetes services (AKS) ==========================\nvariable \"aks_rg_name\" {\n  description = \"(Required) The name of the resource group in which to create the AKS. Changing this forces a new resource to be created.\"\n  type        = string\n}\n\nvariable \"aks_rg_location\" {\n  description = \"Location in which to deploy the AKS\"\n  type        = string\n  default     = \"East US\"\n}\n\nvariable \"cluster_name\" {\n  description = \"(Required) Specifies the name of the AKS cluster.\"\n  type        = string\n}\n\nvariable \"dns_prefix\" {\n  description = \"(Optional) DNS prefix specified when creating the managed cluster. Changing this forces a new resource to be created.\"\n  type        = string\n}\n\nvariable \"private_cluster_enabled\" {\n  description = \"Should this Kubernetes Cluster have its API server only exposed on internal IP addresses? This provides a Private IP Address for the Kubernetes API on the Virtual Network where the Kubernetes Cluster is located. Defaults to false. Changing this forces a new resource to be created.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"azure_rbac_enabled\" {\n  description = \"(Optional) Is Role Based Access Control based on Azure AD enabled?\"\n  default     = true\n  type        = bool\n}\n\nvariable \"admin_group_object_ids\" {\n  description = \"(Optional) A list of Object IDs of Azure Active Directory Groups which should have Admin Role on the Cluster.\"\n  default     = [\"c63746fd-eb61-4733-b7ed-c7de19c17901\"]\n  type        = list(string)\n}\n\nvariable \"role_based_access_control_enabled\" {\n  description = \"(Required) Is Role Based Access Control Enabled? Changing this forces a new resource to be created.\"\n  default     = true\n  type        = bool\n}\n\nvariable \"automatic_channel_upgrade\" {\n  description = \"(Optional) The upgrade channel for this Kubernetes Cluster. Possible values are patch, rapid, and stable.\"\n  default     = \"stable\"\n  type        = string\n\n  validation {\n    condition     = contains([\"patch\", \"rapid\", \"stable\"], var.automatic_channel_upgrade)\n    error_message = \"The upgrade mode is invalid.\"\n  }\n}\n\nvariable \"aks_sku_tier\" {\n  description = \"(Optional) The SKU Tier that should be used for this Kubernetes Cluster. Possible values are Free and Paid (which includes the Uptime SLA). Defaults to Free.\"\n  default     = \"Free\"\n  type        = string\n\n  validation {\n    condition     = contains([\"Free\", \"Paid\"], var.aks_sku_tier)\n    error_message = \"The sku tier is invalid.\"\n  }\n}\n\nvariable \"kubernetes_version\" {\n  description = \"Specifies the AKS Kubernetes version\"\n  default     = \"1.23.12\"\n  type        = string\n}\n\nvariable \"default_node_pool_vm_size\" {\n  description = \"Specifies the vm size of the default node pool\"\n  default     = \"Standard_B4ms\"\n  type        = string\n}\n\nvariable \"default_node_pool_availability_zones\" {\n  description = \"Specifies the availability zones of the default node pool\"\n  default     = [\"1\", \"2\", \"3\"]\n  type        = list(string)\n}\n\nvariable \"network_docker_bridge_cidr\" {\n  description = \"Specifies the Docker bridge CIDR\"\n  default     = \"172.17.0.1/16\"\n  type        = string\n}\n\nvariable \"network_dns_service_ip\" {\n  description = \"Specifies the DNS service IP\"\n  default     = \"10.25.0.10\"\n  type        = string\n}\n\nvariable \"network_service_cidr\" {\n  description = \"Specifies the service CIDR\"\n  default     = \"10.25.0.0/16\"\n  type        = string\n}\n\nvariable \"network_plugin\" {\n  description = \"Specifies the network plugin of the AKS cluster\"\n  default     = \"azure\"\n  type        = string //kubnet -//CNI-azure \n}\n\nvariable \"network_policy\" {\n  description = \"Specifies the network policy of the AKS cluster\"\n  default     = \"azure\"\n  type        = string //azure or calico\n}\n\nvariable \"outbound_type\" {\n  description = \"(Optional) The outbound (egress) routing method which should be used for this Kubernetes Cluster. Possible values are loadBalancer and userDefinedRouting. Defaults to loadBalancer.\"\n  type        = string\n  default     = \"userDefinedRouting\"\n\n  validation {\n    condition     = contains([\"loadBalancer\", \"userDefinedRouting\"], var.outbound_type)\n    error_message = \"The outbound type is invalid.\"\n  }\n}\n\nvariable \"default_node_pool_name\" {\n  description = \"Specifies the name of the default node pool\"\n  default     = \"agentpool\"\n  type        = string\n}\n\nvariable \"default_node_pool_subnet_name\" {\n  description = \"Specifies the name of the subnet that hosts the default node pool\"\n  default     = \"SystemSubnet\"\n  type        = string\n}\n\nvariable \"default_node_pool_subnet_address_prefix\" {\n  description = \"Specifies the address prefix of the subnet that hosts the default node pool\"\n  default     = [\"10.0.0.0/20\"]\n  type        = list(string)\n}\n\nvariable \"default_node_pool_enable_auto_scaling\" {\n  description = \"(Optional) Whether to enable auto-scaler. Defaults to false.\"\n  type        = bool\n  default     = true\n}\n\nvariable \"default_node_pool_enable_host_encryption\" {\n  description = \"(Optional) Should the nodes in this Node Pool have host encryption enabled? Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"default_node_pool_enable_node_public_ip\" {\n  description = \"(Optional) Should each node have a Public IP Address? Defaults to false. Changing this forces a new resource to be created.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"default_node_pool_max_pods\" {\n  description = \"(Optional) The maximum number of pods that can run on each agent. Changing this forces a new resource to be created.\"\n  type        = number\n  default     = 110\n}\n\nvariable \"default_node_pool_node_labels\" {\n  description = \"(Optional) A list of Kubernetes taints which should be applied to nodes in the agent pool (e.g key=value:NoSchedule). Changing this forces a new resource to be created.\"\n  type        = map(any)\n  default     = {}\n}\n\nvariable \"default_node_pool_node_taints\" {\n  description = \"(Optional) A map of Kubernetes labels which should be applied to nodes in this Node Pool. Changing this forces a new resource to be created.\"\n  type        = list(string)\n  default     = []\n}\n\nvariable \"default_node_pool_os_disk_type\" {\n  description = \"(Optional) The type of disk which should be used for the Operating System. Possible values are Ephemeral and Managed. Defaults to Managed. Changing this forces a new resource to be created.\"\n  type        = string\n  default     = \"Ephemeral\"\n}\n\nvariable \"default_node_pool_max_count\" {\n  description = \"(Required) The maximum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be greater than or equal to min_count.\"\n  type        = number\n  default     = 5\n}\n\nvariable \"default_node_pool_min_count\" {\n  description = \"(Required) The minimum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be less than or equal to max_count.\"\n  type        = number\n  default     = 2\n}\n\nvariable \"default_node_pool_node_count\" {\n  description = \"(Optional) The initial number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be a value in the range min_count - max_count.\"\n  type        = number\n  default     = 2\n}\n\n# variable \"log_analytics_workspace_id\" {\n#   description = \"(Optional) The ID of the Log Analytics Workspace which the OMS Agent should send data to. Must be present if enabled is true.\"\n#   type        = string\n# }\n\n# variable \"tenant_id\" {\n#   description = \"(Required) The tenant id of the system assigned identity which is used by master components.\"\n#   type        = string\n# }\n\nvariable \"aks_log_analytics_retention_days\" {\n  description = \"Specifies the number of days of the retention policy\"\n  type        = number\n  default     = 30\n}\n\n# variable \"ingress_application_gateway\" {\n#   description = \"Specifies the Application Gateway Ingress Controller addon configuration.\"\n#   type = object({\n#     enabled      = bool\n#     gateway_id   = string\n#     gateway_name = string\n#     subnet_cidr  = string\n#     subnet_id    = string\n#   })\n#   default = {\n#     enabled      = false\n#     gateway_id   = null\n#     gateway_name = null\n#     subnet_cidr  = null\n#     subnet_id    = null\n#   }\n# }\n\nvariable \"azure_policy\" {\n  description = \"Specifies the Azure Policy addon configuration.\"\n  type = object({\n    enabled = bool\n  })\n  default = {\n    enabled = false\n  }\n}\n\nvariable \"http_application_routing\" {\n  description = \"Specifies the HTTP Application Routing addon configuration.\"\n  type = object({\n    enabled = bool\n  })\n  default = {\n    enabled = false\n  }\n}\n\nvariable \"kube_dashboard\" {\n  description = \"Specifies the Kubernetes Dashboard addon configuration.\"\n  type = object({\n    enabled = bool\n  })\n  default = {\n    enabled = false\n  }\n}\n\nvariable \"admin_username\" {\n  description = \"(Required) Specifies the Admin Username for the AKS cluster worker nodes. Changing this forces a new resource to be created.\"\n  type        = string\n  default     = \"azadmin\"\n}\n\nvariable \"ssh_public_key\" {\n  description = \"(Required) Specifies the SSH public key used to access the cluster. Changing this forces a new resource to be created.\"\n  type        = string\n  default     = \"ssh_pub_keys/azureuser.pub\"\n}\n\nvariable \"aks_tags\" {\n  description = \"(Optional) Specifies the tags of the AKS\"\n  type        = map(any)\n  default     = {}\n}\n</code></pre> Define variables</p> <p>Here is the list of new variables used in this lab.</p> <p><code>dev-variables.tfvar</code> - update this existing file for AKS values for development environment. These values will be different for each environments and it may be different for your requirements too.</p> dev-variables.tfvar<pre><code># Azure Kubernetes Service (AKS) \naks_rg_name                         = \"aks\"\naks_rg_location                     = \"East US\"\ncluster_name                        = \"cluster1\"\ndns_prefix                          = \"cluster1-dns\"\nprivate_cluster_enabled             = false\naks_sku_tier                        = \"Free\"\ndefault_node_pool_node_count        = 2\ndefault_node_pool_vm_size           = \"Standard_B4ms\"\n</code></pre> <p>output variables</p> <p>Here is the list of output variables used in this lab </p> output.tf<pre><code>// ========================== Azure Kubernetes services (AKS) ==========================\noutput \"aks_name\" {\n  value       = azurerm_kubernetes_cluster.aks.name\n  description = \"Specifies the name of the AKS cluster.\"\n}\n\noutput \"aks_id\" {\n  value       = azurerm_kubernetes_cluster.aks.id\n  description = \"Specifies the resource id of the AKS cluster.\"\n}\n\n\n# output \"aks_identity_principal_id\" {\n#   value       = azurerm_user_assigned_identity.aks_identity.principal_id\n#   description = \"Specifies the principal id of the managed identity of the AKS cluster.\"\n# }\n\noutput \"kubelet_identity_object_id\" {\n  value       = azurerm_kubernetes_cluster.aks.kubelet_identity.0.object_id\n  description = \"Specifies the object id of the kubelet identity of the AKS cluster.\"\n}\n\noutput \"kube_config_raw\" {\n  value       = azurerm_kubernetes_cluster.aks.kube_config_raw\n  sensitive   = true\n  description = \"Contains the Kubernetes config to be used by kubectl and other compatible tools.\"\n}\n\noutput \"aks_private_fqdn\" {\n  value       = azurerm_kubernetes_cluster.aks.private_fqdn\n  description = \"The FQDN for the Kubernetes Cluster when private link has been enabled, which is only resolvable inside the Virtual Network used by the Kubernetes Cluster.\"\n}\n\noutput \"aks_node_resource_group\" {\n  value       = azurerm_kubernetes_cluster.aks.node_resource_group\n  description = \"Specifies the resource id of the auto-generated Resource Group which contains the resources for this Managed Kubernetes Cluster.\"\n}\n</code></pre>"},{"location":"azure/11-aks/#task-2-create-a-resource-group-for-aks","title":"Task-2: Create a resource group for AKS","text":"<p>We will create separate resource group for AKS and related resources. add following terraform configuration in <code>aks.tf</code> file for creating AKS resource group.</p> <p>In this task, we will create Azure resource group by using the terraform </p> <p>aks.tf<pre><code># Create a new resource group\nresource \"azurerm_resource_group\" \"aks\" {\n  name     = lower(\"${var.rg_prefix}-${var.aks_rg_name}-${local.environment}\")\n  location = var.aks_rg_location\n  tags     = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/11-aks/#task-3-create-aks-user-assigned-identity","title":"Task-3: Create AKS user assigned identity","text":"<p>Use the following terraform configuration for creating user assigned identity which is going be used in AKS</p> <p>User assigned managed identities enable Azure resources to authenticate to cloud services (e.g. Azure Key Vault) without storing credentials in code. </p> <p>User Assigned Identity in Azure Container Registry provides improved security, simplified management, better integration with Azure services, RBAC, and better compliance, making it a beneficial feature for organizations that use AKS.</p> aks.tf<pre><code># Create User Assigned Identity used in AKS\nresource \"azurerm_user_assigned_identity\" \"aks_identity\" {\n  resource_group_name = azurerm_resource_group.aks.name\n  location            = azurerm_resource_group.aks.location\n  tags                = merge(local.default_tags)\n\n  name = \"${var.cluster_name}Identity}\"\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre>"},{"location":"azure/11-aks/#task-4-create-azure-kubernetes-services-aks-using-terraform","title":"Task-4: Create Azure Kubernetes Services (AKS) using terraform","text":"<p>To create an Azure Kubernetes Service (AKS) cluster using Terraform, you can can use the following terraform configuration:</p> aks.tf<pre><code># Create a new Azure Kubernetes Service (Cluster)\nresource \"azurerm_kubernetes_cluster\" \"aks\" {\n  name                             = lower(\"${var.aks_prefix}-${var.cluster_name}-${local.environment}\")\n  resource_group_name              = azurerm_resource_group.aks.name\n  location                         = azurerm_resource_group.aks.location\n  kubernetes_version               = var.kubernetes_version\n  dns_prefix                       = var.dns_prefix\n  private_cluster_enabled          = var.private_cluster_enabled\n  automatic_channel_upgrade        = var.automatic_channel_upgrade\n  sku_tier                         = var.aks_sku_tier\n  azure_policy_enabled             = true\n  enable_pod_security_policy       = false\n  http_application_routing_enabled = false\n  local_account_disabled           = false // true\n  open_service_mesh_enabled        = false\n\n  linux_profile {\n    admin_username = var.admin_username\n\n    ssh_key {\n      key_data = file(\"${var.ssh_public_key}\")\n    }\n  }\n\n  auto_scaler_profile {\n    balance_similar_node_groups      = false\n    empty_bulk_delete_max            = \"10\"\n    expander                         = \"random\"\n    max_graceful_termination_sec     = \"600\"\n    max_node_provisioning_time       = \"15m\"\n    max_unready_nodes                = 3\n    max_unready_percentage           = 45\n    new_pod_scale_up_delay           = \"0s\"\n    scale_down_delay_after_add       = \"10m\"\n    scale_down_delay_after_delete    = \"10s\"\n    scale_down_delay_after_failure   = \"3m\"\n    scale_down_unneeded              = \"10m\"\n    scale_down_unready               = \"20m\"\n    scale_down_utilization_threshold = \"0.5\"\n    scan_interval                    = \"10s\"\n    skip_nodes_with_local_storage    = false\n    skip_nodes_with_system_pods      = true\n  }\n\n  azure_active_directory_role_based_access_control {\n    admin_group_object_ids = [\"c63746fd-eb61-4733-b7ed-c7de19c17901\"]\n    tenant_id              = data.azurerm_subscription.current.tenant_id\n    azure_rbac_enabled     = var.role_based_access_control_enabled\n    managed                = true\n  }\n\n  default_node_pool {\n    name       = var.default_node_pool_name\n    node_count = var.default_node_pool_node_count\n    vm_size    = var.default_node_pool_vm_size\n    # availability_zones           = var.default_node_pool_availability_zones // TODO:Anji, need to discuss this later\n    enable_auto_scaling          = var.default_node_pool_enable_auto_scaling\n    enable_host_encryption       = var.default_node_pool_enable_host_encryption\n    enable_node_public_ip        = var.default_node_pool_enable_node_public_ip\n    fips_enabled                 = false\n    kubelet_disk_type            = \"OS\"\n    max_count                    = var.default_node_pool_max_count\n    max_pods                     = var.default_node_pool_max_pods\n    min_count                    = var.default_node_pool_min_count\n    node_labels                  = var.default_node_pool_node_labels\n    node_taints                  = var.default_node_pool_node_taints\n    only_critical_addons_enabled = false\n    os_disk_size_gb              = 128\n    os_sku                       = \"Ubuntu\"\n    vnet_subnet_id               = azurerm_subnet.aks.id\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n    # user_assigned_identity_id = azurerm_user_assigned_identity.aks_identity.id\n  }\n\n  oms_agent {\n    log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  }\n\n  tags = merge(local.default_tags, var.aks_tags)\n  lifecycle {\n    ignore_changes = [\n      tags,\n      # kube_admin_config,\n      # kube_config,\n      linux_profile,\n      # ingress_application_gateway\n    ]\n  }\n\n  # https://docs.microsoft.com/en-us/azure/aks/use-network-policies\n  network_profile {\n    load_balancer_sku = \"standard\"\n    network_plugin    = var.network_plugin\n    network_policy    = var.network_policy\n    # outbound_type     = var.outbound_type\n    # pod_cidr           = var.pod_cidr\n    service_cidr       = var.network_service_cidr\n    dns_service_ip     = var.network_dns_service_ip\n    docker_bridge_cidr = var.network_docker_bridge_cidr\n  }\n\n  # ingress_application_gateway {\n  #   gateway_id = azurerm_application_gateway.appgtw.id\n  # }\n\n  key_vault_secrets_provider {\n    secret_rotation_enabled = true\n    # secret_rotation_interval = 2\n  }\n\n  depends_on = [\n    azurerm_log_analytics_workspace.workspace,\n    azurerm_subnet.aks,\n    # azurerm_user_assigned_identity.aks_identity\n    # azurerm_application_gateway.appgtw,\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p> <p>ssh_public_key</p> <p>If you notice, we are using ssh_public_key here in our script.</p> <p>When you create an AKS cluster, you can specify an SSH public key to use for authentication to the nodes in the cluster. This allows you to securely connect to the nodes using SSH, which can be useful for tasks such as troubleshooting and debugging.</p> <p>This assumes that your SSH key pair is stored in the default location (~/.ssh/id_rsa and ~/.ssh/id_rsa.pub). If your key pair is stored in a different location, adjust the path accordingly.</p>"},{"location":"azure/11-aks/#task-5-create-diagnostics-settings-for-aks","title":"Task-5: Create Diagnostics Settings for AKS","text":"<p>we are going to use diagnostics settings for all kind of azure resources to manage logs and metrics etc... Let's create diagnostics settings for AKS for storing Logs and Metric with default retention of 30 days or as per the requirements.</p> aks.tf<pre><code># Create Diagnostics Settings for AKS\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_aks\" {\n  name                       = \"DiagnosticsSettings\"\n  target_resource_id         = azurerm_kubernetes_cluster.aks.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  log {\n    category = \"kube-apiserver\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"kube-audit\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"kube-audit-admin\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"kube-controller-manager\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"kube-scheduler\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"cluster-autoscaler\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"guard\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n  log {\n    category = \"cloud-controller-manager\"\n    enabled  = true\n\n    retention_policy {\n      days    = var.aks_log_analytics_retention_days\n      enabled = true\n    }\n  }\n  log {\n    category = \"csi-azuredisk-controller\"\n    enabled  = true\n\n    retention_policy {\n      days    = var.aks_log_analytics_retention_days\n      enabled = true\n    }\n  }\n  log {\n    category = \"csi-azurefile-controller\"\n    enabled  = true\n\n    retention_policy {\n      days    = var.aks_log_analytics_retention_days\n      enabled = true\n    }\n  }\n  log {\n    category = \"csi-snapshot-controller\"\n    enabled  = true\n\n    retention_policy {\n      days    = var.aks_log_analytics_retention_days\n      enabled = true\n    }\n  }\n\n  metric {\n    category = \"AllMetrics\"\n\n    retention_policy {\n      enabled = true\n      days    = var.aks_log_analytics_retention_days\n    }\n  }\n}\n</code></pre> <p>Important</p> <p>Make sure that you will properly review your requirements before enable Diagnostics setting, you may want to toggle off some of them which are not needed for your project so that you can save Log Analytics workspace cost here. </p> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/11-aks/#task-6-review-aks-cluster-resource-in-the-portal","title":"Task-6: Review AKS Cluster resource in the portal","text":"<p>Now it time to review all the azure resource created in the given resource group. Login into the azure portal and select the resource group. you will notice following resource in the resource group you've selected.</p> <p></p> <p>MC_ resource group</p> <p>In Azure Kubernetes Service (AKS), the MC_ resource group is an automatically created resource group that contains the Azure resources that are created as part of an AKS cluster. The MC_ prefix stands for \"Managed Cluster\".</p> <p>When you create an AKS cluster, you specify a name and a resource group for the cluster. For example, you might create a cluster named aks-cluster1-dev in a resource group named rg-aks-dev. When the cluster is created, Azure creates a new resource group with the name MC_rg-aks-dev_aks-cluster1-dev_. The  part of the name is the Azure region where the resources are deployed. <p>The MC_ resource group contains a variety of Azure resources that are created as part of the AKS cluster, including virtual machines. These resources are used to support the operation of the cluster and its workloads.</p> <p>It's worth noting that the MC_ resource group is managed by Azure and should not be modified or deleted directly. If you need to modify the resources in the MC_ resource group, you should do so through the AKS cluster itself or through the Azure Portal or Azure CLI.</p> <p>Here are the list of resource created automatically for you in the resource group:</p> <p></p>"},{"location":"azure/11-aks/#task-7-validate-aks-cluster-running-kubectl","title":"Task-7: Validate AKS cluster running Kubectl","text":"<p>As a beginner the best way to start interacting AKS is selecting your cluster and click on connect in the overview blade, you will notice bunch of kubectl commands which you can use to start.</p> <p></p> <p><pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n</code></pre> get nodes</p> <p><pre><code>kubectl get no\n</code></pre> output <pre><code>NAME                                STATUS   ROLES   AGE     VERSION\naks-agentpool-25316841-vmss000000   Ready    agent   5d20h   v1.23.12\naks-agentpool-25316841-vmss000001   Ready    agent   5d20h   v1.23.12\n</code></pre> get namespaces <pre><code> kubectl get namespace -A\n</code></pre> output <pre><code>NAME                STATUS   AGE\ndefault             Active   5d20h\ngatekeeper-system   Active   5d20h\nkube-node-lease     Active   5d20h\nkube-public         Active   5d20h\nkube-system         Active   5d20h\n</code></pre></p> <p>get pods <pre><code> kubectl get pods -A\n</code></pre> output <pre><code>NAMESPACE           NAME                                                READY   STATUS    RESTARTS   AGE\ngatekeeper-system   gatekeeper-audit-584d598c78-chx8v                   1/1     Running   0          14d\ngatekeeper-system   gatekeeper-controller-8684d59cc8-4wtgh              1/1     Running   0          14d\ngatekeeper-system   gatekeeper-controller-8684d59cc8-rtrgx              1/1     Running   0          14d\nkube-system         aks-secrets-store-csi-driver-4vlfk                  3/3     Running   0          14d\nkube-system         aks-secrets-store-csi-driver-w6fcv                  3/3     Running   0          14d\nkube-system         aks-secrets-store-provider-azure-glh4r              1/1     Running   0          18d\nkube-system         aks-secrets-store-provider-azure-v2vrl              1/1     Running   0          18d\nkube-system         azure-ip-masq-agent-gjz65                           1/1     Running   0          18d\nkube-system         azure-ip-masq-agent-lf45m                           1/1     Running   0          18d\nkube-system         azure-npm-fktnn                                     1/1     Running   0          18d\nkube-system         azure-npm-n4qmq                                     1/1     Running   0          18d\nkube-system         azure-policy-587845b8b9-72tws                       1/1     Running   0          14d\nkube-system         azure-policy-webhook-77b5f89f7b-hc2zb               1/1     Running   0          14d\nkube-system         cloud-node-manager-5qshr                            1/1     Running   0          17d\nkube-system         cloud-node-manager-n8gd4                            1/1     Running   0          17d\nkube-system         coredns-7959db464c-52ltx                            1/1     Running   0          17d\nkube-system         coredns-7959db464c-7bjjn                            1/1     Running   0          17d\nkube-system         coredns-autoscaler-5589fb5654-dbpcz                 1/1     Running   0          18d\nkube-system         csi-azuredisk-node-8dwns                            3/3     Running   0          14d\nkube-system         csi-azuredisk-node-qn52k                            3/3     Running   0          14d\nkube-system         csi-azurefile-node-qrvls                            3/3     Running   0          18d\nkube-system         csi-azurefile-node-wzt2j                            3/3     Running   0          18d\nkube-system         konnectivity-agent-cd99df756-l6gqp                  1/1     Running   0          18d\nkube-system         konnectivity-agent-cd99df756-pm4xs                  1/1     Running   0          18d\nkube-system         kube-proxy-7v5hf                                    1/1     Running   0          17d\nkube-system         kube-proxy-8vvzp                                    1/1     Running   0          17d\nkube-system         metrics-server-87c5578c4-j9gsg                      2/2     Running   0          17d\nkube-system         metrics-server-87c5578c4-tr8s8                      2/2     Running   0          17d\n</code></pre></p> <p>get all resource</p> <pre><code>kubectl get all -A \n</code></pre>"},{"location":"azure/11-aks/#task-8-allow-aks-cluster-access-to-azure-container","title":"Task-8: Allow AKS Cluster access to Azure Container","text":"<p>AKS cluster needs access to the ACR to pull your Microservices container images from the ACR.  you can allow an AKS cluster access to an Azure Container Registry (ACR) using role assignment. Here is the terraform configuration for this creating role assignment.</p> <p>aks.tf<pre><code># Allow AKS Cluster access to Azure Container Registry\nresource \"azurerm_role_assignment\" \"role_acrpull\" {\n  principal_id                     = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id\n  role_definition_name             = \"AcrPull\"\n  scope                            = azurerm_container_registry.acr.id\n  skip_service_principal_aad_check = true\n  depends_on = [\n    azurerm_container_registry.acr,\n    azurerm_kubernetes_cluster.aks\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/11-aks/#task-9-lock-the-resource-group","title":"Task-9: Lock the resource group","text":"<p>Finally, it is time to lock the resource group created part of this exercise, so that we can avoid the accidental deletion of the azure resources created here.</p> <p>aks.tf<pre><code># Lock the resource group\nresource \"azurerm_management_lock\" \"aks\" {\n  name       = \"CanNotDelete\"\n  scope      = azurerm_resource_group.aks.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource group can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_resource_group.aks,\n    azurerm_monitor_diagnostic_setting.diag_aks,\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>That's it! You now have a new AKS cluster deployed in Azure and ready for deploying deployment YAML files for your containerized microservices.</p>"},{"location":"azure/11-aks/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Kubernetes Service (AKS) documentation</li> <li>Microsoft MSDN - Network concepts for applications in Azure Kubernetes Service (AKS)</li> <li>Deploying a Kubernetes Cluster Using Azure Kubernetes Service</li> <li>Terraform Registry - azurerm_resource_group</li> <li>Terraform Registry - azurerm_user_assigned_identityapplicationkube{:target=\"_blank\"}</li> <li>Terraform Registry - azurerm_kubernetes_cluster</li> <li>Terraform Registry - azurerm_role_assignment</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_management_lock</li> <li>Azure Terraform Quickstart/301-aks-private-cluster</li> </ul>"},{"location":"azure/12-postgresql/","title":"Create Azure PostgreSQL - Flexible Server using terraform","text":""},{"location":"azure/12-postgresql/#introduction","title":"Introduction","text":"<p>Azure Database for PostgreSQL - Flexible Server is a fully managed database service on the Microsoft Azure cloud platform, designed to host PostgreSQL databases. It provides a scalable, secure, and cost-effective solution for deploying, managing, and scaling PostgreSQL-based applications.</p> <p>In this hands-on lab, I'll guide you through the process of creating an Azure PostgreSQL - Flexible Server using Terraform. We'll set up diagnostic settings to monitor this resource effectively and finally enhancing security through the use of private DNS zone.</p> <p>Key features of Azure Database for PostgreSQL - Flexible Server:</p> <ol> <li> <p>Managed Service: It is a fully managed database service, meaning Microsoft Azure takes care of routine database management tasks, allowing developers to focus on building applications.</p> </li> <li> <p>Open Source Compatibility: Based on the popular open-source PostgreSQL database engine, ensuring compatibility with PostgreSQL and support for a wide range of PostgreSQL features.</p> </li> <li> <p>Scalability: Offers flexible compute and storage configurations to scale resources based on application requirements, providing the ability to scale up or down as needed.</p> </li> <li> <p>High Availability: Provides built-in high availability with automatic backups and the ability to restore to any point in time within the retention period.</p> </li> <li> <p>Security Features:</p> <ul> <li>Supports Azure Active Directory authentication for enhanced security.</li> <li>Enables data encryption in transit and at rest.</li> <li>Firewall rules and Virtual Network Service Endpoints enhance network security.</li> </ul> </li> <li> <p>Automatic Patching: Azure handles routine maintenance tasks, including software patching, ensuring that the database is up-to-date and secure.</p> </li> <li> <p>Monitoring and Diagnostics: Integration with Azure Monitor provides real-time performance monitoring, diagnostics, and insights into the database's health and performance.</p> </li> <li> <p>Geo-replication: Allows for setting up read replicas in different Azure regions for improved read scalability and disaster recovery.</p> </li> <li> <p>Flexible Deployment Options: Supports deploying databases across different Azure regions and availability zones for better performance and fault tolerance.</p> </li> <li> <p>Developer Tools Integration: Seamless integration with popular developer tools and frameworks, making it easy for developers to work with their preferred tools.</p> </li> <li> <p>Cost Management: Provides cost-effective pricing models based on the chosen configuration, allowing users to optimize costs based on their application needs.</p> </li> <li> <p>Compatibility with Azure Services: Integrates with other Azure services, such as Azure Logic Apps, Azure Functions, and more, for building end-to-end solutions.</p> </li> </ol>"},{"location":"azure/12-postgresql/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, the task is to design and implement a database solution that aligns with the principles of microservices architecture. The solution should be robust, scalable, cost-effective, and includes key considerations such as scalability, high availability, security, geo-replication, and seamless integration with microservices.</p>"},{"location":"azure/12-postgresql/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare PostgreSQL - Flexible Server variables</li> <li>Task-2: Create an Azure resource group for PostgreSQL</li> <li>Task-3: Create or use existing Virtual Network </li> <li>Task-4: Create a subnet for PostgreSQL</li> <li>Task-5: Create private DNS zone for PostgreSQL </li> <li>Task-6: Associate PostgreSQL Private DNS zone with virtual network</li> <li>Task-7: Generate PostgreSQL admin random password &amp; store in Key Vault</li> <li>Task-8: Create Azure PostgreSQL - Flexible Server using Terraform</li> <li>Task-9: Configure Diagnostic settings for PostgreSQL - Flexible Server</li> <li>Task-10: Set a user or group as the AD administrator for a PostgreSQL Flexible Server.</li> <li>Task-11: Create new Databases in PostgreSQL Server</li> <li>Task-12: Create AD groups for database access</li> </ul> <p>Through these tasks, you will gain practical experience on Azure PostgreSQL - Flexible Server.</p>"},{"location":"azure/12-postgresql/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of PostgreSQL - Flexible Server:</p> <p></p>"},{"location":"azure/12-postgresql/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet</li> <li>Basic knowledge of Terraform and Azure concepts.</li> </ol>"},{"location":"azure/12-postgresql/#implementation-details","title":"Implementation details","text":"<p>Here's a step-by-step guide on how to create an Azure PostgreSQL - Flexible Server using Terraform</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/12-postgresql/#task-1-define-and-declare-postgresql-flexible-server-variables","title":"Task-1: Define and declare PostgreSQL - Flexible Server variables","text":"<p>In this task, we will define and declare the necessary variables for creating the Azure PostgreSQL - Flexible Server resource. These variables will be used to specify the resource settings and customize the values according to our requirements in each environment.</p> <p>This table presents the variables along with their descriptions, data types, and default values:</p> Variable Name Description Type Default Value <code>psql_prefix</code> Prefix of the PostgreSQL server name that's combined with the name of the PostgreSQL server <code>string</code> <code>\"psql\"</code> <code>psql_rg_name</code> (Required) The name of the Resource Group where the PostgreSQL Flexible Server should exist <code>string</code> <code>\"rg-postgresql-dev\"</code> <code>psql_location</code> (Required) The Azure Region where the PostgreSQL Flexible Server should exist <code>string</code> <code>\"East US\"</code> <code>psql_name</code> (Required) The name which should be used for this PostgreSQL Flexible Server <code>string</code> <code>\"psql-postgresql1-dev\"</code> <code>psql_sku_name</code> (Optional) The SKU Name for the PostgreSQL Flexible Server <code>string</code> <code>\"GP_Standard_D2s_v3\"</code> <code>psql_tags</code> (Optional) A mapping of tags which should be assigned to the PostgreSQL Flexible Server <code>map(any)</code> <code>{}</code> <code>psql_admin_password</code> (Optional) Admin password of the PostgreSQL server <code>string</code> <code>\"Test1234t\"</code> <code>psql_admin_login</code> (Optional) Admin username of the PostgreSQL server <code>string</code> <code>\"postgres\"</code> <code>psql_version</code> (Optional) The version of PostgreSQL Flexible Server to use <code>string</code> <code>\"15\"</code> <code>psql_storage_mb</code> (Optional) The max storage allowed for the PostgreSQL Flexible Server <code>string</code> <code>\"262144\"</code> <code>postgresql_configurations</code> (Optional) PostgreSQL configurations to enable <code>map(string)</code> See the default value in the code <p>Variable declaration:</p> variables.tf<pre><code>// ========================== PostgreSQL ==========================\n\nvariable \"psql_prefix\" {\n  type        = string\n  default     = \"psql\"\n  description = \"Prefix of the PostgreSQL server name that's combined with name of the PostgreSQL server.\"\n}\nvariable \"psql_rg_name\" {\n  description = \"(Required) The name of the Resource Group where the PostgreSQL Flexible Server should exist.\"\n  type        = string\n  default     = \"rg-postgresql-dev\"\n}\nvariable \"psql_location\" {\n  description = \"(Required) The Azure Region where the PostgreSQL Flexible Server should exist.\"\n  type        = string\n  default     = \"East US\"\n}\nvariable \"psql_name\" {\n  description = \"(Required) The name which should be used for this PostgreSQL Flexible Server.\"\n  type        = string\n  default     = \"psql-postgresql1-dev\"\n}\nvariable \"psql_sku_name\" {\n  description = \"(Optional) The SKU Name for the PostgreSQL Flexible Server. The name of the SKU, follows the tier + name pattern (e.g. B_Standard_B1ms, GP_Standard_D2s_v3, MO_Standard_E4s_v3). \"\n  type        = string\n  default     = \"GP_Standard_D2s_v3\"\n\n  validation {\n    condition     = contains([\"B_Standard_B1ms\", \"GP_Standard_D2s_v3\", \"MO_Standard_E4s_v3\"], var.psql_sku_name)\n    error_message = \"The value of the sku name property of the PostgreSQL is invalid.\"\n  }\n}\nvariable \"psql_tags\" {\n  description = \"(Optional) A mapping of tags which should be assigned to the PostgreSQL Flexible Server.\"\n  type        = map(any)\n  default     = {}\n}\nvariable \"psql_admin_password\" {\n  description = \"(Optional) Admin password of the PostgreSQL server\"\n  type        = string\n  default     = \"Test1234t\"\n}\nvariable \"psql_admin_login\" {\n  description = \"(Optional) Admin username of the PostgreSQL server\"\n  type        = string\n  default     = \"postgres\"\n}\nvariable \"psql_version\" {\n  description = \"(Optional) The version of PostgreSQL Flexible Server to use. Possible values are 11,12, 13, 14 and 15. Required when create_mode is Default.\"\n  type        = string\n  default     = \"15\"\n  validation {\n    condition     = contains([\"11\", \"12\", \"13\", \"14\", \"15\"], var.psql_version)\n    error_message = \"The value of the version property of the PostgreSQL is invalid.\"\n  }\n}\nvariable \"psql_storage_mb\" {\n  description = \"(Optional) The max storage allowed for the PostgreSQL Flexible Server. Possible values are 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4193280, 4194304, 8388608, 16777216 and 33553408.\"\n  type        = string\n  default     = \"262144\"\n}\nvariable \"postgresql_configurations\" {\n  description = \"PostgreSQL configurations to enable.\"\n  type        = map(string)\n  default = {\n    \"pgbouncer.enabled\" = \"true\",\n    \"azure.extensions\"  = \"PG_TRGM\"\n  }\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># PostgreSQL\npsql_rg_name                        = \"Postgresql\"\npsql_name                           = \"Postgresql1\"\npsql_sku_name                       = \"GP_Standard_D2s_v3\"\npsql_admin_login                    = \"postgres\"\npsql_admin_password                 = \"Test1234t\"\npsql_version                        = \"13\"\npsql_storage_mb                     = \"262144\"\n</code></pre>"},{"location":"azure/12-postgresql/#task-2-create-an-azure-resource-group-for-postgresql","title":"Task-2: Create an Azure resource group for PostgreSQL","text":"<p>Create a dedicated Azure Resource Group to logically group and manage the resources related to your PostgreSQL Flexible Server.</p> postgresql.tf<pre><code># Create an Azure resource group for PostgreSQL\nresource \"azurerm_resource_group\" \"rg_psql\" {\n  name     = \"${var.rg_prefix}-${var.psql_rg_name}-${local.environment}\"\n  location = var.psql_location\n  tags = merge(local.default_tags,\n    {\n      \"CreatedBy\" = \"Anji.Keesari\"\n  })\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - resource group</p> <p></p>"},{"location":"azure/12-postgresql/#task-3-create-an-azure-virtual-network","title":"Task-3: Create an Azure Virtual Network","text":"<p>Decide whether to use an existing Virtual Network or create a new one. A Virtual Network provides the network infrastructure for PostgreSQL Flexible Server.</p> postgresql.tf<pre><code># Create spoke virtual network\nresource \"azurerm_virtual_network\" \"vnet\" {\n  name                = lower(\"${var.vnet_prefix}-${var.spoke_vnet_name}-${local.environment}\")\n  address_space       = var.spoke_vnet_address_space\n  resource_group_name = azurerm_resource_group.vnet.name\n  location            = azurerm_resource_group.vnet.location\n\n  tags = merge(local.default_tags,\n    {\n      \"CreatedBy\" = \"Anji.Keesari\"\n  })\n  lifecycle {\n    ignore_changes = [\n      # tags,\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.vnet,\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - Virtual network</p> <p></p>"},{"location":"azure/12-postgresql/#task-4-create-an-azure-subnet-for-postgresql","title":"Task-4: Create an Azure Subnet for PostgreSQL","text":"<p>Create a subnet within the chosen Virtual Network to host your PostgreSQL Flexible Server. This is a dedicated subnet of the network for PostgreSQL Flexible Server..</p> postgresql.tf<pre><code>// Create a subnet for PostgreSQL\nresource \"azurerm_subnet\" \"psql\" {\n  name                                          = lower(\"${var.subnet_prefix}-${var.psql_subnet_name}\")\n  resource_group_name                           = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.vnet.name\n  address_prefixes                              = [var.psql_address_prefixes]\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  service_endpoints                             = [\"Microsoft.Storage\"]\n  delegation {\n    name = \"fs\"\n    service_delegation {\n      name = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n      actions = [\n        \"Microsoft.Network/virtualNetworks/subnets/join/action\",\n      ]\n    }\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - subnet</p> <p></p>"},{"location":"azure/12-postgresql/#task-5-create-a-private-dns-zone-for-postgresql","title":"Task-5: Create a Private DNS zone for PostgreSQL","text":"<p>Azure Private DNS offers a reliable and secure DNS service for your virtual network. It effectively manages and resolves domain names within the virtual network, eliminating the need for a custom DNS solution.</p> <p>When utilizing private network access in an Azure virtual network, it is mandatory to provide private DNS zone information for DNS resolution. For the creation of a new Azure Database for PostgreSQL Flexible Server with private network access, private DNS zones must be utilized during the configuration of flexible servers.</p> <p>In this lab, we will use terraform to create private DNS zones and incorporate them into the configuration of flexible servers with private access.</p> postgresql.tf<pre><code># Create private DNS zone for PostgreSQL \nresource \"azurerm_private_dns_zone\" \"psql_dns_zone\" {\n  name                = \"${var.psql_prefix}-${var.psql_name}-${local.environment}.private.postgres.database.azure.com\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags, var.psql_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - Private DNS zone</p> <p></p>"},{"location":"azure/12-postgresql/#task-6-associate-postgresql-private-dns-zone-with-virtual-network","title":"Task-6: Associate PostgreSQL Private DNS zone with virtual network","text":"<p>Associate the private DNS zone you created with the Virtual Network. This ensures that DNS queries for your PostgreSQL resources are resolved within the network.</p> postgresql.tf<pre><code># Associate PostgreSQL Private DNS zone with virtual network\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"psql_dns_zone_vnet_associate\" {\n  name                  = \"link_to_${azurerm_virtual_network.vnet.name}\"\n  resource_group_name   = azurerm_virtual_network.vnet.resource_group_name\n  private_dns_zone_name = azurerm_private_dns_zone.psql_dns_zone.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n\n  tags = merge(local.default_tags,var.psql_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n\n  depends_on = [\n    azurerm_virtual_network.vnet,\n    azurerm_private_dns_zone.psql_dns_zone\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - virtual_network_link</p> <p></p>"},{"location":"azure/12-postgresql/#task-7-generate-postgresql-admin-random-password-store-in-key-vault","title":"Task-7: Generate PostgreSQL admin random password &amp; store in Key Vault","text":"<p>In this task, we will generate a random strong password for the PostgreSQL admin. The password will be securely stored in Azure Key Vault, ensuring that it remains confidential and can be accessed programmatically when needed.</p> postgresql.tf<pre><code># Generate PostgreSQL admin random password\nresource \"random_password\" \"psql_admin_password\" {\n  length           = 20\n  special          = true\n  lower            = true\n  upper            = true\n  override_special = \"!#$\"\n}\n\n# Store PostgreSQL admin password in Azuure Key Vault\nresource \"azurerm_key_vault_secret\" \"psql_admin_password\" {\n  name         = \"postgres-db-password\"\n  value        = random_password.psql_admin_password.result\n  key_vault_id = azurerm_key_vault.kv.id\n  tags         = {}\n  depends_on = [\n    azurerm_key_vault.kv,\n    random_password.psql_admin_password,\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - PostgreSQL admin password</p> <p></p>"},{"location":"azure/12-postgresql/#task-8-create-azure-postgresql-flexible-server-using-terraform","title":"Task-8: Create Azure PostgreSQL - Flexible Server using Terraform","text":"<p>Utilize Terraform to deploy your Azure PostgreSQL Flexible Server. This includes specifying server settings, configuration options, and connection details.</p> <p>postgresql.tf<pre><code># Create the Azure PostgreSQL - Flexible Server using terraform\nresource \"azurerm_postgresql_flexible_server\" \"psql\" {\n  name                   = lower(\"${var.psql_prefix}-${var.psql_name}-${local.environment}\")\n  resource_group_name    = azurerm_resource_group.rg_psql.name\n  location               = azurerm_resource_group.rg_psql.location\n  version                = var.psql_version\n  delegated_subnet_id    = azurerm_subnet.psql.id\n  private_dns_zone_id    = azurerm_private_dns_zone.psql_dns_zone.id\n  administrator_login    = var.psql_admin_login\n  administrator_password = azurerm_key_vault_secret.psql_admin_password.value\n  # zone                    = \"1\"\n  storage_mb = var.psql_storage_mb\n\n  # Set the backup retention policy to 7 for non-prod, and 30 for prod\n  backup_retention_days = 7\n\n  sku_name = var.psql_sku_name\n  depends_on = [\n    azurerm_resource_group.rg_psql,\n    azurerm_subnet.psql,\n    azurerm_private_dns_zone.psql_dns_zone,\n    azurerm_key_vault_secret.psql_admin_password\n  ]\n  tags = merge(local.default_tags, var.psql_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags,\n      # private_dns_zone_id\n    ]\n  }\n}\n</code></pre> Run Terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - Overview blade </p> <p></p> <p>Azure PostgreSQL - Flexible Server - Private DNS Zone</p> <p></p>"},{"location":"azure/12-postgresql/#task-9-configure-diagnostic-settings-for-azure-postgresql-flexible-server","title":"Task-9: Configure Diagnostic Settings for Azure PostgreSQL - Flexible Server","text":"<p>Set up diagnostic settings to collect and store logs and metrics from your PostgreSQL Flexible Server. This is crucial for monitoring and troubleshooting.</p> postgresql.tf<pre><code># Create diagnostic settings for PostgreSQL server\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_psql\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_postgresql_flexible_server.psql.name}\")\n  target_resource_id         = azurerm_postgresql_flexible_server.psql.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  enabled_log {\n    category = \"PostgreSQLFlexDatabaseXacts\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category = \"PostgreSQLFlexQueryStoreRuntime\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category = \"PostgreSQLFlexQueryStoreWaitStats\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category = \"PostgreSQLFlexSessions\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category = \"PostgreSQLFlexTableStats\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category = \"PostgreSQLLogs\"\n\n    retention_policy {\n      days    = 0\n      enabled = true\n    }\n  }\n  metric {\n    category = \"AllMetrics\"\n    enabled  = true\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      # log\n    ]\n  }\n  depends_on = [\n    azurerm_postgresql_flexible_server.psql,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - Diagnostic Settings from left nav</p> <p></p>"},{"location":"azure/12-postgresql/#task-10-set-a-user-or-group-as-the-ad-administrator-for-a-postgresql-flexible-server","title":"Task-10: Set a user or group as the AD administrator for a PostgreSQL Flexible Server","text":"<p>Specify an Azure AD user or group as the administrator for your PostgreSQL Flexible Server, allowing them to manage the server.</p> <p>postgresql.tf<pre><code># Set a user or group as the AD administrator for a PostgreSQL Flexible Server.\ndata \"azuread_group\" \"azuread_psql_admin_group\" {\n  display_name     = \"psql-admin-group\"\n  security_enabled = true\n}\nresource \"azurerm_postgresql_flexible_server_active_directory_administrator\" \"psql\" {\n  server_name         = azurerm_postgresql_flexible_server.psql.name\n  resource_group_name = azurerm_resource_group.rg_psql.name\n  tenant_id           = data.azurerm_client_config.current.tenant_id\n  object_id           = data.azuread_group.azuread_psql_admin_group.object_id\n  principal_name      = data.azuread_group.azuread_psql_admin_group.display_name\n  principal_type      = \"Group\"\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - AD administrator </p> <p></p>"},{"location":"azure/12-postgresql/#task-11-create-new-databases-in-postgresql-server","title":"Task-11: Create new Databases in PostgreSQL Server","text":"<p>Use Terraform to create new databases within your PostgreSQL Flexible Server, providing segregated spaces for each database</p> <p>postgresql.tf<pre><code># create databases in PostgreSQL Server\nvariable \"database_names\" {\n  type    = list(string)\n  default = [\"database1\", \"database2\", \"database3\"]\n}\n\nresource \"azurerm_postgresql_flexible_server_database\" \"psql_db\" {\n  for_each  = { for name in var.database_names : name =&gt; name }\n  name      = each.key\n  server_id = azurerm_postgresql_flexible_server.psql.id\n  charset   = \"utf8\"\n  collation = \"en_US.utf8\"\n  depends_on = [\n    azurerm_postgresql_flexible_server.psql\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure PostgreSQL - Flexible Server - Databases</p> <p></p>"},{"location":"azure/12-postgresql/#task-12-create-ad-groups-for-database-access","title":"Task-12: Create AD groups for database access","text":"<p>Establish Azure AD groups to manage access to your databases. This enhances security and simplifies permissions management.</p> <p>postgresql.tf<pre><code># Create azure ad groups for database access\nresource \"azuread_group\" \"psql_ad_group\" {\n  for_each         = toset([\"readonly\", \"readwrite\", \"administrators\"])\n  display_name     = lower(\"${azurerm_postgresql_flexible_server.psql.name}-${each.key}-${local.environment}\")\n  owners           = [data.azurerm_client_config.current.object_id]\n  security_enabled = true\n\n  lifecycle {\n    ignore_changes = [owners]\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/12-postgresql/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Database for PostgreSQL documentation</li> <li>Microsoft MSDN - Deploy a PostgreSQL Flexible Server Database using Terraform</li> <li>Microsoft MSDN - Azure Database for PostgreSQL pricing</li> <li>Terraform Registry - azurerm_resource_group</li> <li>Terraform Registry - azurerm_virtual_network</li> <li>Terraform Registry - azurerm_subnet</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - random_password </li> <li>Terraform Registry - azurerm_key_vault_secret </li> <li>Terraform Registry - azurerm_postgresql_flexible_server</li> <li>Terraform Registry - azurerm_postgresql_flexible_server_database</li> <li>Terraform Registry - azurerm_postgresql_flexible_server_active_directory_administrator</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_management_lock</li> <li>Azure Terraform Quickstart/201-postgresql-fs-db</li> </ul>"},{"location":"azure/12.1-postgresql-access-control/","title":"Streamlining PostgreSQL Database Access Control Permissions with Azure AD Groups","text":""},{"location":"azure/12.1-postgresql-access-control/#introduction","title":"Introduction","text":"<p>PostgreSQL Database Access control permissions are something critical for every single project, ensuring that the right individuals have the appropriate level of access to the database resources they need. </p> <p>In this hands-on lab, I'll guide you through the process of creating and streamlining PostgreSQL Database access control permissions with Azure AD Groups.</p>"},{"location":"azure/12.1-postgresql-access-control/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Database Administrator</code> (DBA), the objective is to enhance the security and access control of our PostgreSQL databases through the effective utilization of Azure AD groups. This initiative aims to streamline user management and elevate overall database permission control.</p> <p>Streamlining PostgreSQL Database Access Control includes:</p> <p>Azure AD Integration: Ensure seamless integration with Azure Active Directory for our PostgreSQL database hosted on Azure.</p> <p>Role Assignment with Azure AD Groups: Allow DBAs to associate Azure AD groups with specific roles in PostgreSQL.</p> <p>User Onboarding and Permissions: Enable DBAs to onboard new users by adding them to relevant Azure AD groups.</p> <p>Access Revocation: Provide a mechanism for DBAs to remove users from Azure AD groups, promptly revoking their access to the PostgreSQL database.</p>"},{"location":"azure/12.1-postgresql-access-control/#fundamental-concepts","title":"Fundamental Concepts","text":"<p>Let's take look into some fundamental concepts before jumping into practical details:</p> <p>Default PostgreSQL Database Roles:</p> <p>When you set up an Azure Database for PostgreSQL server, it automatically configures three default roles. To identify these roles, you can execute the following command:</p> <pre><code>SELECT rolname FROM pg_roles;\n</code></pre> <p>azure_pg_admin: This role includes your server admin user. However, note that the server admin account doesn't inherently possess the <code>azure_superuser</code> role. It plays a crucial administrative role in managing the PostgreSQL server.</p> <p>azure_superuser (\"azuresu\"): Reserved exclusively for Microsoft due to the managed nature of this Platform as a Service (PaaS) offering. The <code>azure_superuser</code> role provides elevated privileges necessary for overseeing and maintaining the entire PostgreSQL service.</p> <p>Server Admin User (postgres): This represents the username linked to the administrator account you specified during the initial creation of your Azure Database for PostgreSQL. It's essential for day-to-day administrative tasks within your specific database.</p> <p>For example server admin user details will look like below:</p> <pre><code>- Default Username: postgres\n- Password: password1234 (Configured during PostgreSQL server creation)\n</code></pre> <p>In the PostgreSQL engine, privileges govern access to database objects. In Azure Database for PostgreSQL, the server admin user has the following privileges: LOGIN, NOSUPERUSER, INHERIT, CREATEDB, CREATEROLE, and REPLICATION.</p> <p>The server admin user can create new users and assign them to the azure_pg_admin role. Additionally, it can establish users and roles with varying privilege levels, enabling access to specific databases and schemas based on organizational needs.</p>"},{"location":"azure/12.1-postgresql-access-control/#architecture-diagram","title":"Architecture diagram","text":"<p>The following reference diagram from Microsoft MSDN documentation explains the highrarchy of roles and access controls within the PostgreSQL - Flexible Server:</p> <p></p> <p>This diagram from Microsoft illustrates how the authentication process works.</p> <p>as per this diagram we can create two categories of roles/users</p> <pre><code>#postgreSQL side\nPostgreSQL Admin\nPostgreSQL user\n\n# Azure AD side \nAzure AD PostgreSQL Admin\nAzure AD PostgreSQL user\n</code></pre>"},{"location":"azure/12.1-postgresql-access-control/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Azure subscription.</li> <li>Azure database for PostgreSQL Flexible server already created</li> <li>Basic knowledge of PostgreSQL Flexible server</li> </ol>"},{"location":"azure/12.1-postgresql-access-control/#implementation-details","title":"Implementation details","text":"<p>Here's a step-by-step guide on how to streamlining PostgreSQL Database access control permissions with Azure AD Groups.</p>"},{"location":"azure/12.1-postgresql-access-control/#task-1-enable-azure-ad-authentication","title":"Task-1: Enable Azure AD Authentication","text":"<p>In this task, we will enable Azure AD authentication by configuring the Azure AD settings on the PostgreSQL servers.</p>"},{"location":"azure/12.1-postgresql-access-control/#task-2-create-azure-ad-administrator-group-for-postgresql-database","title":"Task-2: Create Azure AD administrator group for PostgreSQL database","text":"<p>Creating an Azure Active Directory (Azure AD) group for PostgreSQL database administrators involves establishing a dedicated group within the Azure AD to manage permissions and access control for users who is responsible for PostgreSQL administration activities.</p> <p>Only one Microsoft Entra administrator (a user or group) can be configured for an Azure Database for PostgreSQL server at any time.</p> <p>Only a Microsoft Entra administrator for PostgreSQL can initially connect to the Azure Database for PostgreSQL using a Microsoft Entra account. The Active Directory administrator can configure subsequent Microsoft Entra database users.</p>"},{"location":"azure/12.1-postgresql-access-control/#task-3-create-azure-ad-groups","title":"Task-3: Create Azure AD Groups","text":"<p>This step involves creating Azure AD groups to efficiently manage access permissions for application developers working with 'database1' across different environments. These groups play a pivotal role in streamlining the assignment of access privileges, promoting a structured and secure approach to database management.</p> <p>Azure AD Group Configuration:</p> <p>Development Environment:</p> <ul> <li><code>psql-dev-database1-readonly</code>: Read-only access group during development.</li> <li><code>psql-dev-database1-readwrite</code>: Read and write access group for developers during development.</li> <li><code>psql-dev-database1-administrators</code>: Administrative rights group for 'database1' in the development environment.</li> </ul> <p>Testing Environment:</p> <ul> <li><code>psql-test-database1-readonly</code>: Read-only access group during testing.</li> <li><code>psql-test-database1-readwrite</code>: Read and write access group for developers during testing.</li> <li><code>psql-test-database1-administrators</code>: Administrative rights group for 'database1' in the testing environment.</li> </ul> <p>Production Environment:</p> <ul> <li><code>psql-prod-database1-readonly</code>: Read-only access group in the production environment.</li> <li><code>psql-prod-database1-readwrite</code>: Read and write access group for developers in the production environment.</li> <li><code>psql-prod-database1-administrators</code>: Administrative rights group for 'database1' in the production environment.</li> </ul>"},{"location":"azure/12.1-postgresql-access-control/#task-4-mapping-grants-to-azure-ad-roles","title":"Task-4: Mapping Grants to Azure AD Roles:","text":"<p>In this step, we define specific roles and associated access privileges for 'database1.' These roles ensure a granular control mechanism, aligning permissions with distinct user responsibilities.</p> <p>Read-Only Role</p> <ul> <li>Role Name: <code>psql-dev-database1-readonly</code></li> <li>Purpose: This role is designed for users who require read-only access to 'database1.' Members of this group can only execute SELECT queries.</li> <li>Grants:</li> <li><code>SELECT</code>: Allows users in this role to retrieve data from tables within 'database1.'</li> </ul> <p>Read-Write Role</p> <ul> <li>Role Name: <code>psql-dev-database1-readwrite</code></li> <li>Purpose: This role is intended for users with the need to read and modify data in 'database1.'</li> <li>Grants:</li> <li><code>SELECT</code>: Permits users to retrieve data.</li> <li><code>INSERT</code>: Allows users to add new records.</li> <li><code>UPDATE</code>: Enables users to modify existing records.</li> <li><code>DELETE</code>: Grants the ability to remove records.</li> </ul> <p>Administrator Role</p> <ul> <li>Role Name: <code>psql-dev-database1-administrator</code></li> <li>Purpose: This high-privilege role is specifically reserved for database architects and administrators with comprehensive control over 'database1.'</li> <li>Grants:</li> <li><code>SELECT</code>: Allows for data retrieval.</li> <li><code>INSERT</code>: Permits the addition of new records.</li> <li><code>UPDATE</code>: Enables modification of existing records.</li> <li><code>DELETE</code>: Grants the ability to remove records.</li> <li><code>TRUNCATE</code>: Provides the authority to remove all records from a table.</li> <li><code>REFERENCES</code>: Allows the creation of foreign key references.</li> <li><code>TRIGGER</code>: Permits the creation of triggers.</li> <li><code>CREATE</code>: Enables the creation of database objects.</li> <li><code>CONNECT</code>: Grants the ability to connect to the database.</li> <li><code>TEMPORARY</code>: Allows the creation of temporary tables.</li> <li><code>EXECUTE</code>: Enables the execution of stored procedures and functions.</li> <li><code>USAGE</code>: Permits the usage of database objects.</li> <li><code>SET</code>: Allows the setting of configuration parameters.</li> <li><code>ALTER SYSTEM</code>: Enables modifications to the system configuration.</li> </ul>"},{"location":"azure/12.1-postgresql-access-control/#task-5-create-postgresql-groups","title":"Task-5: Create PostgreSQL groups","text":""},{"location":"azure/13-key-vault/","title":"Create Azure Key Vault using Terraform","text":""},{"location":"azure/13-key-vault/#introduction","title":"Introduction","text":"<p>Azure Key Vault is a cloud service provided by Microsoft Azure that allows you to securely store and manage sensitive information such as secrets, encryption keys, and certificates. It provides a centralized and secure storage location for managing application secrets, connection strings, passwords, and cryptographic keys used in containerized microservices applications and services.</p> <p>In this hands-on lab, I'll guide you through the process of creating an Azure Key Vault using Terraform. We'll set up diagnostic settings to monitor this resource effectively. also, we'll ensure that access policies are configured to allow developers access to the secrets stored in the Azure Key Vault and finally enhancing security through the use of private endpoints.</p>"},{"location":"azure/13-key-vault/#key-features","title":"Key Features","text":"<p>Here are some key features of Azure Key Vault:</p> <ol> <li> <p>Secrets Management: Azure Key Vault can be used to securely store and manage sensitive information such as API keys, passwords, and connection strings.</p> </li> <li> <p>Key Management: It supports the creation, import, storage, and management of cryptographic keys used for encryption, decryption, and signing of data.</p> </li> <li> <p>Certificate Management: Key Vault enables the management of X.509 certificates, making it easier to provision and manage SSL/TLS certificates used for securing applications.</p> </li> <li> <p>Role-Based Access Control (RBAC): Access to Key Vault resources is controlled through Azure Active Directory (Azure AD). RBAC allows you to grant specific permissions to users, groups, or applications.</p> </li> <li> <p>Integration with Azure Services: Key Vault seamlessly integrates with other Azure services such as AKS in our case, making it easy to use stored secrets, keys, and certificates in microservice applications</p> </li> </ol> <p>Azure Key Vault enhances the security of microservice applications by reducing the need to store sensitive information in the source code or configuration files. Instead, microservice applications can retrieve sensitive information from Key Vault at runtime.</p>"},{"location":"azure/13-key-vault/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, I need to design and implement a robust solution to address the secure storage and management of sensitive information within our microservices architecture hosted in Azure Kubernetes Service (AKS). This solution should ensure that application secrets, connection strings, passwords, and cryptographic keys are stored centrally and accessed securely by our services.</p>"},{"location":"azure/13-key-vault/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare Azure Key Vault variables</li> <li>Task-2: Create Azure Key Vault using Terraform</li> <li>Task-3: Configure diagnostic settings for Azure Key Vault using Terraform</li> <li>Task-4: Configure access policy for developer in Azure Key Vault</li> <li>Task-5: Restrict Access Using Private Endpoint</li> <li>Task-5.1: Configure the Private DNS Zone</li> <li>Task-5.2: Create a Virtual Network Link Association</li> <li>Task-5.3: Create a Private Endpoint Using Terraform</li> <li>Task-5.4: Validate private link connection using <code>nslookup</code> or <code>dig</code></li> <li>Task-6: Created azure Key Vault secrets for sensitive information</li> </ul> <p>Through these tasks, you will gain practical experience on Azure Key Vault.</p>"},{"location":"azure/13-key-vault/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of key vault usage:</p> <p></p>"},{"location":"azure/13-key-vault/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet - for configuring a private endpoint.</li> <li>Basic knowledge of Terraform and Azure concepts.</li> </ol>"},{"location":"azure/13-key-vault/#implementation-details","title":"Implementation details","text":"<p>Here's a step-by-step guide on how to create an Azure Key Vault using Terraform</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/13-key-vault/#task-1-define-and-declare-azure-key-vault-variables","title":"Task-1: Define and declare Azure Key vault variables","text":"<p>In this task, we will define and declare the necessary variables for creating the Azure Key vault resource. These variables will be used to specify the resource settings and customize the values according to our requirements in each environment.</p> <p>This table presents the variables along with their descriptions, data types, and default values:</p> Variable Name Description Type Default Value kv_prefix Prefix of the Azure key vault. string \"kv\" kv_name (Required) Specifies the name of the key vault. string \"keyvault1\" kv_resource_group_name (Required) Specifies the resource group name of the key vault. string \"replace me\" (use if you need a separate resource group) kv_location (Required) Specifies the location where the key vault will be deployed. string \"replace me\" (use if you need a separate location for the key vault) tenant_id (Required) The Azure Active Directory tenant ID for authenticating requests to the key vault. string \"replace me\" (use if you need a separate tenant id) kv_owner_object_id (Required) Object IDs of the key vault owners who need access to the key vault. string \"d0abdc5c-2ba6-4868-8387-d700969c786f\" kv_sku_name (Required) The Name of the SKU used for this Key Vault. Possible values are standard and premium. string \"standard\" kv_tags (Optional) Specifies the tags of the key vault. map(any) {} enabled_for_deployment (Optional) Boolean flag specifying whether Azure Virtual Machines are permitted to retrieve certificates stored as secrets from the key vault. bool false enabled_for_disk_encryption (Optional) Boolean flag specifying whether Azure Disk Encryption is permitted to retrieve secrets from the vault and unwrap keys. bool false enabled_for_template_deployment (Optional) Boolean flag specifying whether Azure Resource Manager is permitted to retrieve secrets from the key vault. bool false enable_rbac_authorization (Optional) Boolean flag specifying whether Azure Key Vault uses Role-Based Access Control (RBAC) for authorization of data actions. bool false purge_protection_enabled (Optional) Is Purge Protection enabled for this Key Vault? bool false soft_delete_retention_days (Optional) The number of days that items should be retained once soft-deleted. This value can be between 7 and 90 (the default) days. number 30 bypass (Required) Specifies which traffic can bypass the network rules. Possible values are AzureServices and None. string \"AzureServices\" kv_default_action (Required) The Default Action to use when no rules match from ip_rules / virtual_network_subnet_ids. Possible values are Allow and Deny. string \"Allow\" kv_ip_rules (Optional) One or more IP Addresses or CIDR Blocks that should be able to access the Key Vault. list [] kv_virtual_network_subnet_ids (Optional) One or more Subnet IDs that should be able to access this Key Vault. list [] (use if virtual networking is provisioned separately) kv_log_analytics_workspace_id Specifies the log analytics workspace id. string \"replace me\" (use if log analytics is provisioned separately) kv_log_analytics_retention_days Specifies the number of days of the retention policy. number 7 kv_key_permissions_full List of full key permissions. Must be one or more from the following: backup, create, decrypt, delete, encrypt, get, import, list, purge, recover, restore, sign, unwrapKey, update, verify, and wrapKey. list [\"Backup\", \"Create\", \"Decrypt\", \"Delete\", \"Encrypt\", \"Get\", \"Import\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Sign\", \"UnwrapKey\", \"Update\", \"Verify\", \"WrapKey\", \"Release\", \"Rotate\", \"GetRotationPolicy\", \"SetRotationPolicy\"] kv_secret_permissions_full List of full secret permissions. Must be one or more from the following: backup, delete, get, list, purge, recover, restore, and set. list [\"Backup\", \"Delete\", \"Get\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Set\"] kv_certificate_permissions_full List of full certificate permissions. Must be one or more from the following: backup, create, delete, deleteissuers, get, getissuers, import, list, listissuers, managecontacts, manageissuers, purge, recover, restore, setissuers, and update. list [\"Backup\", \"Create\", \"Delete\", \"DeleteIssuers\", \"Get\", \"GetIssuers\", \"Import\", \"List\", \"ListIssuers\", \"ManageContacts\", \"ManageIssuers\", \"Purge\", \"Recover\", \"Restore\", \"SetIssuers\", \"Update\"] kv_storage_permissions_full List of full storage permissions. Must be one or more from the following: backup, delete, deletesas, get, getsas, list, listsas, purge, recover, regeneratekey, restore, set, setsas, and update. list [\"Backup\", \"Delete\", \"DeleteSAS\", \"Get\", \"GetSAS\", \"List\", \"ListSAS\", \"Purge\", \"Recover\", \"RegenerateKey\", \"Restore\", \"Set\", \"SetSAS\", \"Update\"] <p>Variable declaration:</p> variables.tf<pre><code>// ========================== Key Vault ==========================\ndata \"azurerm_client_config\" \"current\" {}\n\nvariable \"kv_prefix\" {\n  type        = string\n  default     = \"kv\"\n  description = \"Prefix of the Azure key vault.\"\n}\nvariable \"kv_name\" {\n  description = \"(Required) Specifies the name of the key vault.\"\n  type        = string\n  default     = \"keyvault1\"\n}\n\nvariable \"kv_resource_group_name\" {\n  description = \"(Required) Specifies the resource group name of the key vault.\"\n  type        = string\n  default     = \"replace me\" # use this if you need separate resource group for key vault\n}\n\nvariable \"kv_location\" {\n  description = \"(Required) Specifies the location where the key vault will be deployed.\"\n  type        = string\n  default     = \"replace me\" # use this if you need separate location for key vault\n}\n\nvariable \"tenant_id\" {\n  description = \"(Required) The Azure Active Directory tenant ID that should be used for authenticating requests to the key vault.\"\n  type        = string\n  default     = \"replace me\" # use this if you need separate tenant id for key vault\n}\n\nvariable \"kv_owner_object_id\" {\n  description = \"(Required) object ids of the key vault owners who needs access to key vault.\"\n  type        = string\n  default     = \"d0abdc5c-2ba6-4868-8387-d700969c786f\"\n}\nvariable \"kv_sku_name\" {\n  description = \"(Required) The Name of the SKU used for this Key Vault. Possible values are standard and premium.\"\n  type        = string\n  default     = \"standard\"\n\n  validation {\n    condition     = contains([\"standard\", \"premium\"], var.kv_sku_name)\n    error_message = \"The value of the sku name property of the key vault is invalid.\"\n  }\n}\n\nvariable \"kv_tags\" {\n  description = \"(Optional) Specifies the tags of the key vault\"\n  type        = map(any)\n  default     = {}\n}\n\nvariable \"enabled_for_deployment\" {\n  description = \"(Optional) Boolean flag to specify whether Azure Virtual Machines are permitted to retrieve certificates stored as secrets from the key vault. Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enabled_for_disk_encryption\" {\n  description = \" (Optional) Boolean flag to specify whether Azure Disk Encryption is permitted to retrieve secrets from the vault and unwrap keys. Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enabled_for_template_deployment\" {\n  description = \"(Optional) Boolean flag to specify whether Azure Resource Manager is permitted to retrieve secrets from the key vault. Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"enable_rbac_authorization\" {\n  description = \"(Optional) Boolean flag to specify whether Azure Key Vault uses Role Based Access Control (RBAC) for authorization of data actions. Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"purge_protection_enabled\" {\n  description = \"(Optional) Is Purge Protection enabled for this Key Vault? Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"soft_delete_retention_days\" {\n  description = \"(Optional) The number of days that items should be retained for once soft-deleted. This value can be between 7 and 90 (the default) days.\"\n  type        = number\n  default     = 30\n}\n\nvariable \"bypass\" {\n  description = \"(Required) Specifies which traffic can bypass the network rules. Possible values are AzureServices and None.\"\n  type        = string\n  default     = \"AzureServices\"\n\n  validation {\n    condition     = contains([\"AzureServices\", \"None\"], var.bypass)\n    error_message = \"The valut of the bypass property of the key vault is invalid.\"\n  }\n}\n\nvariable \"kv_default_action\" {\n  description = \"(Required) The Default Action to use when no rules match from ip_rules / virtual_network_subnet_ids. Possible values are Allow and Deny.\"\n  type        = string\n  default     = \"Allow\"\n\n  validation {\n    condition     = contains([\"Allow\", \"Deny\"], var.kv_default_action)\n    error_message = \"The value of the default action property of the key vault is invalid.\"\n  }\n}\n\nvariable \"kv_ip_rules\" {\n  description = \"(Optional) One or more IP Addresses, or CIDR Blocks which should be able to access the Key Vault.\"\n  default     = []\n}\n\nvariable \"kv_virtual_network_subnet_ids\" {\n  description = \"(Optional) One or more Subnet ID's which should be able to access this Key Vault.\"\n  default     = [] # use this if virtual networking provisioned separately\n}\n\nvariable \"kv_log_analytics_workspace_id\" {\n  description = \"Specifies the log analytics workspace id\"\n  type        = string\n  default     = \"replace me\" # use this if log anaytics provisioned separately\n}\n\nvariable \"kv_log_analytics_retention_days\" {\n  description = \"Specifies the number of days of the retention policy\"\n  type        = number\n  default     = 7\n}\nvariable \"kv_key_permissions_full\" {\n  type        = list(string)\n  description = \"List of full key permissions, must be one or more from the following: backup, create, decrypt, delete, encrypt, get, import, list, purge, recover, restore, sign, unwrapKey, update, verify and wrapKey.\"\n  default     = [\"Backup\", \"Create\", \"Decrypt\", \"Delete\", \"Encrypt\", \"Get\", \"Import\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Sign\", \"UnwrapKey\", \"Update\", \"Verify\", \"WrapKey\", \"Release\", \"Rotate\", \"GetRotationPolicy\", \"SetRotationPolicy\"]\n}\n\nvariable \"kv_secret_permissions_full\" {\n  type        = list(string)\n  description = \"List of full secret permissions, must be one or more from the following: backup, delete, get, list, purge, recover, restore and set\"\n  default     = [\"Backup\", \"Delete\", \"Get\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Set\"]\n\n}\n\nvariable \"kv_certificate_permissions_full\" {\n  type        = list(string)\n  description = \"List of full certificate permissions, must be one or more from the following: backup, create, delete, deleteissuers, get, getissuers, import, list, listissuers, managecontacts, manageissuers, purge, recover, restore, setissuers and update\"\n  default     = [\"Backup\", \"Create\", \"Delete\", \"DeleteIssuers\", \"Get\", \"GetIssuers\", \"Import\", \"List\", \"ListIssuers\", \"ManageContacts\", \"ManageIssuers\", \"Purge\", \"Recover\", \"Restore\", \"SetIssuers\", \"Update\"]\n}\n\nvariable \"kv_storage_permissions_full\" {\n  type        = list(string)\n  description = \"List of full storage permissions, must be one or more from the following: backup, delete, deletesas, get, getsas, list, listsas, purge, recover, regeneratekey, restore, set, setsas and update\"\n  default     = [\"Backup\", \"Delete\", \"DeleteSAS\", \"Get\", \"GetSAS\", \"List\", \"ListSAS\", \"Purge\", \"Recover\", \"RegenerateKey\", \"Restore\", \"Set\", \"SetSAS\", \"Update\", ]\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># key vault\nkv_name                             = \"keyvault1\" \nkv_sku_name                         = \"standard\"\nkv_owner_object_id                  = \"d0abdc5c-2ba6-4868-8387-d700969c7111\"\n</code></pre>"},{"location":"azure/13-key-vault/#task-2-create-azure-key-vault-using-terraform","title":"Task-2: Create Azure Key Vault using Terraform","text":"<p>In this task, we will use Terraform to create the Azure Key Vault instance with the desired configuration.</p> <p>keyvault.tf<pre><code># Create Azure Key Vault using terraform\nresource \"azurerm_key_vault\" \"kv\" {\n  name                            = lower(\"${var.kv_prefix}-${var.kv_name}-${local.environment}\")\n  resource_group_name             = azurerm_resource_group.rg.name\n  location                        = azurerm_resource_group.rg.location\n  tenant_id                       = data.azurerm_client_config.current.tenant_id\n  sku_name                        = var.kv_sku_name\n  enabled_for_disk_encryption     = var.enabled_for_disk_encryption\n  enabled_for_deployment          = var.enabled_for_deployment\n  enabled_for_template_deployment = var.enabled_for_template_deployment\n  enable_rbac_authorization       = var.enable_rbac_authorization\n  purge_protection_enabled        = var.purge_protection_enabled\n  soft_delete_retention_days      = var.soft_delete_retention_days\n  timeouts {\n    delete = \"60m\"\n  }\n\n  # network_acls {\n  #   bypass                     = var.bypass\n  #   default_action             = var.default_action\n  #   ip_rules                   = var.ip_rules\n  #   virtual_network_subnet_ids = var.virtual_network_subnet_ids\n  # }\n  access_policy {\n    tenant_id = data.azurerm_client_config.current.tenant_id\n    object_id = data.azurerm_client_config.current.object_id\n\n    certificate_permissions = var.kv_certificate_permissions_full\n    key_permissions         = var.kv_key_permissions_full\n    secret_permissions      = var.kv_secret_permissions_full\n    storage_permissions     = var.kv_storage_permissions_full\n  }\n  network_acls {\n    default_action = \"Allow\"\n    bypass         = \"AzureServices\"\n  }\n\n  tags = merge(local.default_tags, var.kv_tags)\n  lifecycle {\n    ignore_changes = [\n      access_policy,\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n    data.azurerm_client_config.current\n  ]\n}\n</code></pre> Run Terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Key Vault - Overview blade </p> <p></p>"},{"location":"azure/13-key-vault/#task-3-configure-diagnostic-settings-for-azure-key-vault-using-terraform","title":"Task-3: Configure diagnostic settings for Azure Key Vault using terraform","text":"<p>By configuring diagnostic settings, we can monitor and analyze the behavior of the Azure Key Vault instance.</p> keyvault.tf<pre><code># create diagnostic setting for key vault\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_kv\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_key_vault.kv.name}\")\n  target_resource_id         = azurerm_key_vault.kv.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  enabled_log {\n    category = \"AuditEvent\"\n\n    retention_policy {\n      days    = 0\n      enabled = true\n    }\n  }\n  enabled_log {\n    category = \"AzurePolicyEvaluationDetails\"\n\n    retention_policy {\n      days    = 0\n      enabled = true\n    }\n  }\n\n  metric {\n    category = \"AllMetrics\"\n    retention_policy {\n      enabled = true\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      log_analytics_destination_type,\n    ]\n  }\n  depends_on = [\n    azurerm_key_vault.kv,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Key vault - Diagnostic settings from left nav</p> <p></p>"},{"location":"azure/13-key-vault/#task-4-configure-access-policy-for-developer-in-azure-key-vault","title":"Task-4: Configure access policy for developer in Azure Key Vault","text":"<p>Azure Access Policies in Key Vault are essential for developer who is managing  &amp; who can perform operations on the secrets, keys, and certificates stored in the Key Vault. </p> <p>keyvault.tf<pre><code># provide access to the developer who is working on terraform for validation\nresource \"azurerm_key_vault_access_policy\" \"access_policy_developer\" {\n  key_vault_id = azurerm_key_vault.kv.id\n\n  tenant_id = data.azurerm_client_config.current.tenant_id\n  object_id = var.kv_owner_object_id\n\n  certificate_permissions = var.kv_certificate_permissions_full\n  key_permissions         = var.kv_key_permissions_full\n  secret_permissions      = var.kv_secret_permissions_full\n  storage_permissions     = var.kv_storage_permissions_full\n\n\n  depends_on = [\n    azurerm_key_vault.kv,\n    data.azurerm_client_config.current\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure key vault - Azure Access Policies</p> <p></p>"},{"location":"azure/13-key-vault/#task-5-restrict-access-using-private-endpoint","title":"Task-5: Restrict Access Using Private Endpoint","text":"<p>To enhance security and limit access to an Azure Key Vault , you can utilize private endpoints and Azure Private Link. This approach assigns virtual network private IP addresses to the Key Vault endpoints, ensuring that network traffic between clients on the virtual network and the Key vault's private endpoints traverses a secure path on the Microsoft backbone network, eliminating exposure from the public internet.</p> <p>Additionally, you can configure DNS settings for the Key vault's private endpoints, allowing clients and services in the network to access the Key vault using its fully qualified domain name, such as <code>privatelink.vaultcore.azure.net</code>.</p> <p>This section guides you through configuring a private endpoint for your Key Vault using Terraform.</p>"},{"location":"azure/13-key-vault/#task-51-configure-the-private-dns-zone","title":"Task-5.1: Configure the Private DNS Zone","text":"keyvault.tf<pre><code># Create private DNS zone for key vault\nresource \"azurerm_private_dns_zone\" \"pdz_kv\" {\n  name                = \"privatelink.vaultcore.azure.net\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags)\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the Private DNS zone configuration</p> <p></p>"},{"location":"azure/13-key-vault/#task-52-create-a-virtual-network-link-association","title":"Task-5.2: Create a Virtual Network Link Association","text":"keyvault.tf<pre><code># Create private virtual network link to spoke vnet\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"kv_pdz_vnet_link\" {\n  name                  = \"privatelink_to_${azurerm_virtual_network.vnet.name}\"\n  resource_group_name   = azurerm_resource_group.vnet.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n  private_dns_zone_name = azurerm_private_dns_zone.pdz_kv.name\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.vnet,\n    azurerm_virtual_network.vnet,\n    azurerm_private_dns_zone.pdz_kv\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the Virtual network links configuration.</p> <p></p>"},{"location":"azure/13-key-vault/#task-53-create-a-private-endpoint-using-terraform","title":"Task-5.3: Create a Private Endpoint Using Terraform","text":"keyvault.tf<pre><code># Create private endpoint for key vault\nresource \"azurerm_private_endpoint\" \"pe_kv\" {\n  name                            = lower(\"${var.private_endpoint_prefix}-${azurerm_key_vault.kv.name}\")  \n  location            = azurerm_key_vault.kv.location\n  resource_group_name = azurerm_key_vault.kv.resource_group_name\n  subnet_id           = azurerm_subnet.jumpbox.id\n  tags                = merge(local.default_tags, var.kv_tags)\n\n  private_service_connection {\n    name                           = \"pe-${azurerm_key_vault.kv.name}\"    \n    private_connection_resource_id = azurerm_key_vault.kv.id\n    is_manual_connection           = false\n    subresource_names              = var.pe_kv_subresource_names\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = \"default\" # var.pe_kv_private_dns_zone_group_name\n    private_dns_zone_ids = [azurerm_private_dns_zone.pdz_kv.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_key_vault.kv,\n    azurerm_private_dns_zone.pdz_kv\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the endpoint configuration by navigating to <code>Key Vault -&gt; Networking -&gt; Private access</code> \u2014 you will see the new private endpoint details.</p> <p>Navigate to <code>Private endpoint -&gt; Overview</code> to verify the Virtual network/subnet and Network interface.</p> <p></p> <p>Navigate to <code>Private endpoint -&gt; DNS Configuration</code> to verify the Network Interface and Configuration name.</p> <p></p> <p>Navigate to <code>Network interface -&gt; Overview</code> to verify the private IP address attached to properties.</p>"},{"location":"azure/13-key-vault/#task-54-validate-private-link-connection-using-nslookup-or-dig","title":"Task-5.4: Validate private link connection using <code>nslookup</code> or <code>dig</code>","text":"<p>Connecting from internal VM (private access):</p> <p><pre><code>nslookup privatelink.vaultcore.azure.net\n</code></pre> output</p> <pre><code># expect the private IP Address: 10.64.3.6\n</code></pre> <p>Connecting from external (public access):</p> <pre><code>nslookup privatelink.vaultcore.azure.net\n</code></pre> <p>output</p> <pre><code># should not expect the private IP Address: 10.64.3.6\n</code></pre> <p>This process ensures that the private link connection is successfully established and allows expected private IP address associated with our resource in the private virtual network.</p>"},{"location":"azure/13-key-vault/#task-6-created-azure-key-vault-secrets-for-sensitive-information","title":"Task-6: Created azure Key Vault secrets for sensitive information","text":"<p>Let's create some sample Azure Key Vault secrets for sensitive information related to our PostgreSQL databases and storage account. These secrets will be securely stored and can be accessed programmatically from containerized microservices applications, ensuring robust security practices.</p> <p>Create Key Vault secrets for each sensitive information:</p> <p>a. PostgreSQL Database Password</p> keyvault.tf<pre><code># generate random password for postgreSQL admin password\nresource \"random_password\" \"psql_admin_password\" {\n  length           = 20\n  special          = true\n  lower            = true\n  upper            = true\n  override_special = \"!#$\" //\"!#$%&amp;*()-_=+[]{}&lt;&gt;:?\"\n}\n\n# Create key vault secret for postgres database password\nresource \"azurerm_key_vault_secret\" \"secret_1\" {\n  name         = \"postgres-db-password\"\n  value        = random_password.psql_admin_password.result\n  key_vault_id = azurerm_key_vault.kv.id\n  tags         = {}\n  depends_on = [\n    azurerm_key_vault.kv,\n  ]\n}\n</code></pre> <p>b. PostgreSQL Database Hostname</p> keyvault.tf<pre><code># Create key vault secret for postgres database hostname\nresource \"azurerm_key_vault_secret\" \"secret_2\" {\n  name  = \"postgres-db-hostname\"\n  value = \"psql-postgresql1-dev.postgres.database.azure.com\"\n  # value        = \"${azurerm_postgresql_flexible_server.psql.name}.postgres.database.azure.com\"\n  key_vault_id = azurerm_key_vault.kv.id\n  tags         = {}\n  depends_on = [\n    azurerm_key_vault.kv,\n    # azurerm_postgresql_flexible_server.psql\n  ]\n}\n</code></pre> <p>c. Database1 Connection String</p> keyvault.tf<pre><code># Create key vault secret for database1-connection-string\nresource \"azurerm_key_vault_secret\" \"secret_3\" {\n  name  = \"database1-db-connection-string\"\n  value = \"User ID=postgres;Password=xxxxxx;Host=psql-postgresql1-dev.postgres.database.azure.com;database=database1;Port=5432;\"\n  # value        = \"User ID=postgres;Password=${azurerm_postgresql_flexible_server.psql.administrator_password};Host=${azurerm_postgresql_flexible_server.psql.name}.postgres.database.azure.com;database=database1;Port=5432;\"\n  key_vault_id = azurerm_key_vault.kv.id\n  tags         = {}\n  depends_on = [\n    azurerm_key_vault.kv,\n    # azurerm_postgresql_flexible_server.psql\n  ]\n}\n</code></pre> <p>d. Storage Account Access Key</p> keyvault.tf<pre><code># Create key vault secret for storage account accesskey\nresource \"azurerm_key_vault_secret\" \"secret_4\" {\n  name         = \"storage-account-accesskey\"\n  value        = azurerm_storage_account.st.primary_access_key\n  key_vault_id = azurerm_key_vault.kv.id\n  tags         = {}\n  depends_on = [\n    azurerm_key_vault.kv,\n    azurerm_storage_account.st\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Key vault secrets:</p> <p></p>"},{"location":"azure/13-key-vault/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Key Vault documentation</li> <li>Terraform Registry - azurerm_key_vault</li> <li>Terraform Registry - azurerm_key_vault_access_policy</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - azurerm_private_endpoint</li> <li>Terraform Registry - azurerm_key_vault_secret</li> <li>Azure Terraform Quickstart/101-key-vault-key</li> </ul>"},{"location":"azure/14-redis-cache/","title":"Create Azure Cache for Redis using Terraform","text":""},{"location":"azure/14-redis-cache/#introduction","title":"Introduction","text":"<p>Azure Cache for Redis is a fully managed, in-memory data store that offers high throughput and low-latency access to cached data. In this lab, we'll utilize Terraform's Infrastructure as Code (IaC) capabilities to provision and configure an Azure Cache for Redis resource.</p> <p>In this lab, I will walk through the steps to create an Azure Cache for Redis using Terraform. I'll also configure diagnostic settings to monitor its performance effectively. To ensure the security of our Azure Cache for Redis, I'll establish a private endpoint, thereby securing it from public access. Finally, we'll validate these resources within the Azure portal to confirm that everything is functioning as expected.</p>"},{"location":"azure/14-redis-cache/#key-features","title":"key features","text":"<ol> <li> <p>High Performance: Azure Cache for Redis is designed for high throughput and low-latency access to cached data. It's based on the popular open-source Redis cache, making it extremely fast and efficient.</p> </li> <li> <p>In-Memory Data Store: It stores data in memory, which allows for lightning-fast data retrieval. This is particularly useful for caching frequently accessed data to reduce latency.</p> </li> <li> <p>Fully Managed Service: Azure Cache for Redis is a fully managed service. Azure takes care of tasks like patching, monitoring, and backups, so you can focus on your application.</p> </li> <li> <p>Data Persistence: It offers both non-persistent and persistent caching options. You can configure it to store data on disk for data durability.</p> </li> <li> <p>Scaling: Azure Cache for Redis provides the ability to scale your cache horizontally by adding or removing cache nodes to handle increased load.</p> </li> <li> <p>Security: It supports various security features, including Virtual Network service endpoints, SSL encryption, and Azure AD integration, to keep your data secure.</p> </li> <li> <p>Advanced Data Structures: Redis supports various data structures like strings, hashes, lists, sets, sorted sets, bitmaps, and geospatial indexes, making it versatile for different types of data.</p> </li> <li> <p>Diagnostic and Monitoring: It integrates with Azure Monitor and Azure Diagnostics, providing detailed insights into cache usage and performance.</p> </li> </ol>"},{"location":"azure/14-redis-cache/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, you've been tasked with providing a caching mechanism to enhance performance and reduce the database load in a Microservices Architecture. Azure Cache for Redis offers an ideal solution by enabling us to store frequently accessed data in memory for rapid retrieval.</p>"},{"location":"azure/14-redis-cache/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare Azure Cache for Redis variables.</li> <li>Task-2: Create Azure Cache for Redis using Terraform.</li> <li>Task-3: Configure diagnostic settings for Azure Cache for Redis using Terraform.</li> <li>Task-4: Securing an Azure Cache for Redis instance</li> <li>Task-4.1: Create a private DNS zone for Redis Cache using Terraform.</li> <li>Task-4.2: Create a virtual network link to associate the Redis private DNS zone with a VNet.</li> <li>Task-4.3: Configure a private endpoint for Azure Cache for Redis using Terraform.</li> <li>Task-4.4: Validate private link connection using <code>nslookup</code> or <code>dig</code></li> </ul> <p>Through these tasks, you will gain practical experience on Azure Cache for Redis.</p>"},{"location":"azure/14-redis-cache/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of Azure Cache for Redis</p> <p></p>"},{"location":"azure/14-redis-cache/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet - for configuring a private endpoint.</li> <li>Basic knowledge of terraform and azure concepts.</li> </ol>"},{"location":"azure/14-redis-cache/#implementation-details","title":"Implementation details","text":"<p>Now, let's delve into the step-by-step implementation details:</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/14-redis-cache/#task-1-define-and-declare-azure-cache-for-redis-variables","title":"Task-1: Define and declare Azure Cache for Redis variables","text":"<p>In this task, we will define and declare the necessary variables for creating the azure cache for redis resource.</p> <p>This table presents the variables along with their descriptions, data types, and default values:</p> Variable Name Description Type Default Value redis_cache_enabled (Optional) Whether to enable or disable redis_cache resource creations. bool true redis_cache_prefix Prefix of the Redis cache name that's combined with the name of the Redis Cache. string redis redis_cache_name (Required) The name of the Redis instance. string redis1 redis_cache_sku (Required) The SKU of Redis to use. Possible values are Basic, Standard, and Premium. string Basic redis_cache_capacity (Required) The size of the Redis cache to deploy. Valid values for a SKU family of C (Basic/Standard) are 0-6, and for P (Premium) family are 1-5. string 1 redis_cache_family (Required) The SKU family/pricing group to use. Valid values are C (for Basic/Standard SKU family) and P (for Premium). string C request_message (Optional) Specifies a message passed to the owner of the remote resource when the private endpoint attempts to establish the connection to the remote resource. string null redis_public_network_access_enabled (Optional) Whether or not public network access is allowed for this Redis Cache. true means this resource could be accessed by both the public and private endpoint. false means only private endpoint access is allowed. Defaults to false. bool false redis_enable_authentication (Optional) If set to false, the Redis instance will be accessible without authentication. Defaults to true. bool true redis_pe_core_enabled (Optional) Enable core subscription private endpoint. bool false private_endpoint_prefix Prefix of the Private Endpoint name that's combined with the name of the Private Endpoint. string pe <p>Variable declaration:</p> variables.tf<pre><code>variable \"redis_cache_enabled\" {\n  description = \"(Optional) Whether to enable or disable redis_cache resource creations\"\n  type        = bool\n  default     = true\n}\nvariable \"redis_cache_prefix\" {\n  type        = string\n  default     = \"redis\"\n  description = \"Prefix of the Redis cache name that's combined with name of the Redis Cache.\"\n}\n\nvariable \"redis_cache_name\" {\n  description = \"(Required) The name of the Redis instance.\"\n  type        = string\n}\n\nvariable \"redis_cache_sku\" {\n  description = \" (Required) The SKU of Redis to use. Possible values are Basic, Standard and Premium.\"\n  type        = string\n}\n\nvariable \"redis_cache_capacity\" {\n  description = \"(Required) The size of the Redis cache to deploy. Valid values for a SKU family of C (Basic/Standard) are 0, 1, 2, 3, 4, 5, 6, and for P (Premium) family are 1, 2, 3, 4, 5.\"\n  type        = string\n}\n\nvariable \"redis_cache_family\" {\n  description = \" (Required) The SKU family/pricing group to use. Valid values are C (for Basic/Standard SKU family) and P (for Premium)\"\n  type        = string\n}\nvariable \"request_message\" {\n  description = \"(Optional) Specifies a message passed to the owner of the remote resource when the private endpoint attempts to establish the connection to the remote resource.\"\n  type        = string\n  default     = null\n}\n\nvariable \"redis_public_network_access_enabled\" {\n  description = \" (Optional) Whether or not public network access is allowed for this Redis Cache. true means this resource could be accessed by both public and private endpoint. false means only private endpoint access is allowed. Defaults to true.\"\n  type        = bool\n  default     = false\n}\nvariable \"redis_enable_authentication\" {\n  description = \" (Optional) If set to false, the Redis instance will be accessible without authentication. Defaults to true.\"\n  type        = bool\n  default     = true\n}\nvariable \"redis_pe_core_enabled\" {\n  description = \" (Optional) Enable core subscription private endpoint\"\n  type        = bool\n  default     = false\n}\nvariable \"private_endpoint_prefix\" {\n  type        = string\n  default     = \"pe\"\n  description = \"Prefix of the Private Endpoint name that's combined with name of the Private Endpoint.\"\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># Redis Cache\nredis_cache_name                   = \"redis1\"\nredis_cache_capacity               = 1\nredis_cache_family                 = \"C\" \nredis_cache_sku                    = \"Basic\"\nredis_public_network_access_enabled= true\nredis_enable_authentication        = true\nredis_pe_core_enabled              = true\n</code></pre>"},{"location":"azure/14-redis-cache/#task-2-create-azure-cache-for-redis-using-terraform","title":"Task-2: Create Azure Cache for Redis using terraform","text":"<p>In this task, we will use terraform to create the azure cache for redis instance with the desired configuration.</p> <p>redis_cache.tf<pre><code># Create Azure Cache for Redis using terraform\nresource \"azurerm_redis_cache\" \"redis\" {\n  # count                         = var.redis_cache_enabled ? 1 : 0\n  name                          = lower(\"${var.redis_cache_prefix}-${var.redis_cache_name}-${local.environment}\")\n  resource_group_name           = azurerm_resource_group.rg.name\n  location                      = azurerm_resource_group.rg.location\n  capacity                      = var.redis_cache_capacity\n  family                        = var.redis_cache_family\n  sku_name                      = var.redis_cache_sku\n  enable_non_ssl_port           = false\n  minimum_tls_version           = \"1.2\"\n  public_network_access_enabled = var.redis_public_network_access_enabled\n  # subnet_id = azurerm_subnet.redis.id\n  tags = merge(local.default_tags)\n  redis_configuration {\n    enable_authentication = var.redis_enable_authentication\n  }\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n  ]\n}\n</code></pre> run terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Note</p> <p>azure cache for redis creation takes close to 30 mins.</p> <p><pre><code>azurerm_redis_cache.redis[0]: Creation complete after 25m14s\n</code></pre> Azure Cache for Redis - Overview blade  </p> <p>Azure Cache for Redis - Console </p> <p>Azure Cache for Redis - Console command </p>"},{"location":"azure/14-redis-cache/#task-3-configure-diagnostic-settings-for-azure-cache-for-redis-using-terraform","title":"Task-3: Configure diagnostic settings for Azure Cache for Redis using terraform","text":"<p>By configuring diagnostic settings, we can monitor and analyze the performance and behavior of the Azure Cache for Redis instance, helping us optimize its usage.</p> redis_cache.tf<pre><code># Create diagnostic settings for Azure Cache for Redis\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_redis\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_redis_cache.redis.name}\")\n  target_resource_id         = azurerm_redis_cache.redis.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  log {\n    category = \"ConnectedClientList\"\n    enabled  = true\n  }\n\n  metric {\n    category = \"AllMetrics\"\n    enabled  = false\n\n  }\n  lifecycle {\n    ignore_changes = [\n      log\n    ]\n  }\n\n  depends_on = [\n    azurerm_redis_cache.redis,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Cache for Redis - diagnostic settings from left nav </p> <p>Azure Cache for Redis - diagnostic settings </p>"},{"location":"azure/14-redis-cache/#task-4-securing-an-azure-cache-for-redis-instance","title":"Task-4: Securing an Azure Cache for Redis instance","text":"<p>To enhance security and limit access to an Azure Cache for Redis , you can utilize <code>Private Endpoints</code> and <code>Azure Private Link</code>. This approach assigns virtual network private IP addresses to the Azure Cache for Redis endpoints, ensuring that network traffic between clients on the virtual network and the Azure Cache for Redis's private endpoints traverses a secure path on the Microsoft backbone network, eliminating exposure from the public internet.</p> <p>For more details: Azure Cache for Redis with Azure Private Link</p> <p>You can restrict public access to the private endpoint of your cache by disabling the <code>PublicNetworkAccess</code> flag.</p> <p>Private endpoint is supported on cache tiers Basic, Standard, Premium, and Enterprise. We recommend using private endpoint instead of VNets. Private endpoints are easy to set up or remove, are supported on all tiers, and can connect your cache to multiple different VNets at once.</p> <p>before creating private endpoint we need to make sure that following variable is updated to false.</p> <pre><code>redis_public_network_access_enabled= false\n</code></pre> <p>Here are step-by-step instructions to achieve this:</p>"},{"location":"azure/14-redis-cache/#task-41-create-private-dns-zone-for-redis-cache-using-terraform","title":"Task-4.1: Create private DNS zone for Redis Cache using terraform","text":"<p>This private DNS zone will enable us to access the Redis Cache using a custom domain name within our virtual network.</p> <p>private_dns.tf<pre><code># Create private DNS zone for Redis Cache\nresource \"azurerm_private_dns_zone\" \"pdz_redis\" {\n  name                = \"privatelink.redis.cache.windows.net\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Cache for Redis - Private DNS zone</p> <p></p>"},{"location":"azure/14-redis-cache/#task-42-create-virtual-network-link-to-associate-redis-private-dns-zone-to-vnet","title":"Task-4.2: Create virtual network link to associate redis private DNS zone to vnet","text":"<p>In this task, we will create a virtual network link to associate the Redis private DNS zone with our virtual network. This link enables DNS resolution for the Redis Cache within the virtual network.</p> <p>private_dns.tf<pre><code># Create private virtual network link to vnet\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"redis_pdz_vnet_link\" {\n  name                  = \"privatelink_to_${azurerm_virtual_network.hub_vnet.name}\"\n  resource_group_name   = azurerm_resource_group.vnet.name\n  virtual_network_id    = azurerm_virtual_network.hub_vnet.id\n  private_dns_zone_name = azurerm_private_dns_zone.pdz_redis.name\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.vnet,\n    azurerm_virtual_network.hub_vnet,\n    azurerm_private_dns_zone.pdz_redis\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Cache for Redis - Private DNS zone - Virtual network links</p> <p></p> <p>Azure Cache for Redis - Private DNS zone - Virtual network links details</p> <p></p> <p>By creating a virtual network link, we enable DNS resolution for the Redis Cache within our virtual network, allowing seamless communication.</p>"},{"location":"azure/14-redis-cache/#task-43-configure-private-endpoint-for-azure-cache-for-redis-using-terraform","title":"Task-4.3: Configure private endpoint for Azure Cache for Redis using terraform","text":"<p>By configuring a private endpoint, we ensure that the Azure Cache for Redis instance is accessible securely within the virtual network, minimizing exposure to the public internet.</p> <p>redis_cache.tf<pre><code># Create private endpoint for Azure Cache for Redis\nresource \"azurerm_private_endpoint\" \"pe_redis_core\" {\n  name                = lower(\"${var.private_endpoint_prefix}-${azurerm_redis_cache.redis.name}\")\n  location            = azurerm_redis_cache.redis.location\n  resource_group_name = azurerm_redis_cache.redis.resource_group_name\n  subnet_id           = azurerm_subnet.jumpbox.id\n  tags                = merge(local.default_tags)\n\n  private_service_connection {\n    name                           = \"pe-${azurerm_redis_cache.redis.name}\"\n    private_connection_resource_id = azurerm_redis_cache.redis.id\n    is_manual_connection           = false\n    subresource_names              = [\"redisCache\"]\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = \"default\"\n    private_dns_zone_ids = [azurerm_private_dns_zone.pdz_redis.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_subnet.jumpbox,\n    azurerm_redis_cache.redis,\n    azurerm_private_dns_zone.pdz_redis\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Azure Cache for Redis - Private endpoint</p> <p></p> <p>Azure Cache for Redis - Private endpoint </p> <p>Azure Cache for Redis - Private endpoint - Network interface </p> <p>By following these steps, we've secured your Azure Cache for Redis instance from public access. It's now only accessible through the private endpoint in our specified Virtual Network.</p>"},{"location":"azure/14-redis-cache/#task-44-validate-private-link-connection-using-nslookup-or-dig","title":"Task-4.4: Validate private link connection using <code>nslookup</code> or <code>dig</code>","text":"<p>To validate the private link connection, connect to the <code>virtual machine</code> or <code>Jump server</code>  you set up in the virtual network. Run a utility such as <code>nslookup</code> or <code>dig</code> to look up the private IP address of your Azure Cache for Redis over the private link. </p> <p>This will ensures that the private link connection is successfully established and allows for the verification of the expected private IP address associated with the Azure Cache for Redis in the given virtual network.</p> <p>Validate using <code>dig</code> example:</p> <p>Connecting from internal VM (private access):</p> <p>Run the <code>dig</code> utility to look up the private IP address (<code>10.64.3.7</code>) of your registry over the private link:</p> <pre><code>dig redis-redis1-dev.privatelink.redis.cache.windows.net\n</code></pre> <p>output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; redis-redis1-dev.privatelink.redis.cache.windows.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 49084\n;; flags: qr rd ad; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 0\n;; WARNING: recursion requested but not available\n\n;; QUESTION SECTION:\n;redis-redis1-dev.privatelink.redis.cache.windows.net. IN A\n\n;; ANSWER SECTION:\nredis-redis1-dev.privatelink.redis.cache.windows.net. 0 IN A 10.64.3.7\n\n;; Query time: 10 msec\n;; SERVER: 172.30.80.1#53(172.30.80.1)\n;; WHEN: Tue Dec 26 19:48:20 UTC 2023\n;; MSG SIZE  rcvd: 140\n</code></pre> <p>Connecting from external (public access)</p> <p><pre><code>dig acr1dev.azurecr.io\n</code></pre> output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; redis-redis1-dev.privatelink.redis.cache.windows.net\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 32265\n;; flags: qr rd ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0\n;; WARNING: recursion requested but not available\n\n;; QUESTION SECTION:\n;redis-redis1-dev.privatelink.redis.cache.windows.net. IN A\n\n;; ANSWER SECTION:\nredis-redis1-dev.privatelink.redis.cache.windows.net. 0 IN CNAME ncus-47771-571104577.northcentralus.cloudapp.azure.com.\nncus-47771-571104577.northcentralus.cloudapp.azure.com. 0 IN A 52.159.79.20\n\n;; Query time: 30 msec\n;; SERVER: 172.29.48.1#53(172.29.48.1)\n;; WHEN: Tue Dec 26 11:47:58 PST 2023\n;; MSG SIZE  rcvd: 262\n</code></pre> <p>Validate using <code>nslookup</code> example:</p> <p>Connecting from internal VM (private access):</p> <p><pre><code>nslookup redis-redis1-dev.privatelink.redis.cache.windows.net\n</code></pre> output</p> <pre><code>Server:  UnKnown\nAddress:  168.63.129.16\n\nNon-authoritative answer:\nName:    redis-redis1-dev.privatelink.redis.cache.windows.net\nAddress:  10.64.3.7\n</code></pre> <p>Connecting from external VM (public access):</p> <pre><code>nslookup redis-redis1-dev.privatelink.redis.cache.windows.net\n</code></pre> <p>output</p> <pre><code>Server:  UnKnown\nAddress:  192.168.1.1\n\nNon-authoritative answer:\nName:    ncus-47771-571104577.northcentralus.cloudapp.azure.com\nAddress:  52.159.79.20\nAliases:  redis-redis1-dev.privatelink.redis.cache.windows.net\n</code></pre> <p>This process ensures that the private link connection is successfully established and allows expected private IP address associated with our resource in the private virtual network.</p>"},{"location":"azure/14-redis-cache/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Cache for Redis Documentation</li> <li>Microsoft MSDN - Tutorial: Secure access to an Azure Cache for Redis instance from a virtual network</li> <li>Microsoft MSDN - Azure Private DNS Zone Overview</li> <li>Terraform Registry - azurerm_redis_cache</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - azurerm_private_endpoint</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> </ul>"},{"location":"azure/15-storage-account/","title":"Create Storage Account using Terraform","text":""},{"location":"azure/15-storage-account/#introduction","title":"Introduction","text":"<p>Azure <code>Storage</code> is a cloud-based storage solution provided by Microsoft Azure, offering a scalable, secure, and highly available storage infrastructure for various types of data. Azure Storage allows users to store and retrieve data in the cloud using various storage services, and at the core of this storage ecosystem is the <code>Azure Storage Account</code>.</p> <p>In this hands-on lab, I'll guide you through the process of creating an azure storage account using Terraform. we'll cover the creation of a storage account container and the configuration of diagnostic settings. setting up a files share, and enhancing security through the use of private endpoints.</p> <p>Through these tasks, you will gain practical experience on azure storage account.</p>"},{"location":"azure/15-storage-account/#key-features","title":"Key Features","text":"<p>Key Features of azure storage account includes:</p> <ul> <li> <p>Scalability: azure storage accounts provide a highly scalable solution, enabling users to scale their storage needs as their data requirements grow. The storage capacity can dynamically scale up or down based on demand.</p> </li> <li> <p>Durable and Highly Available: Azure Storage ensures the durability and availability of data through redundancy. Data is replicated across multiple data centers to protect against hardware failures and ensure data integrity.</p> </li> <li> <p>Data Redundancy Options: Users can choose redundancy options such as Locally Redundant Storage (LRS), Geo-Redundant Storage (GRS), Zone-Redundant Storage (ZRS), and more, depending on their specific needs for data redundancy and availability.</p> </li> <li> <p>Blob Storage: Azure Storage supports various data types, and Blob Storage is designed for storing large amounts of unstructured data, such as documents, images, videos, and logs.</p> </li> <li> <p>File Storage: Azure Storage provides a fully managed file share in the cloud, accessible via the Server Message Block (SMB) protocol. This is ideal for organizations that need a scalable file share that can be accessed from multiple virtual machines.</p> </li> <li> <p>Table Storage: A NoSQL data store for semi-structured data, Azure Table Storage is suitable for applications that require high-speed access to large amounts of data.</p> </li> <li> <p>Queue Storage: Azure Queue Storage is a message queuing service that enables communication between application components. It provides a scalable message queue for asynchronous processing.</p> </li> <li> <p>Security and Access Control: Azure Storage offers robust security features, including encryption at rest, encryption in transit, and role-based access control (RBAC) to manage access permissions.</p> </li> <li> <p>Integration with Azure Services: Azure Storage seamlessly integrates with various Azure services, making it a central component for applications deployed in the Azure cloud.</p> </li> </ul> <p>In the context of Microservices architecture and Azure Kubernetes Service (AKS), azure storage account plays a pivotal role in providing scalable and reliable storage solutions for the diverse data requirements of containerized microservices-based applications. </p>"},{"location":"azure/15-storage-account/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, I need to design and implement a robust solution to address persistent storage challenges in my Microservices Architecture deployed on Azure Kubernetes Service (AKS). Using azure storage account, specifically Azure File Storage, becomes handy to provide Persistent Volumes (PV),Persistent Volumes claims(PVC) as per the need in my Microservices. Additionally, I plan to utilize Storage Account containers for capturing Azure Event Hub (Kafka) messages, ensuring a seamless and well-integrated storage strategy within the broader microservices ecosystem.</p>"},{"location":"azure/15-storage-account/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare azure storage account variables</li> <li>Task-2: Create azure storage account using terraform</li> <li>Task-3: Create azure storage account container using terraform</li> <li>Task-4: Configure diagnostic settings for azure storage account using terraform</li> <li>Task-5: Configure diagnostic settings for azure storage account container using terraform</li> <li>Task-6: Create storage account's files share using terraform</li> <li>Task-7: Restrict Access Using Private Endpoint</li> <li>Task-7.1: Configure the Private DNS Zone</li> <li>Task-7.2: Create a Virtual Network Link Association</li> <li>Task-7.3: Create Private Endpoints for azure Storage</li> <li>Task-7.4: Validate private link connection using nslookup or dig</li> </ul>"},{"location":"azure/15-storage-account/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of Storage Account usage:</p> <p></p>"},{"location":"azure/15-storage-account/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet - for configuring a private endpoint.</li> <li>Basic knowledge of terraform and Azure concepts.</li> </ol>"},{"location":"azure/15-storage-account/#implementation-details","title":"Implementation details","text":"<p>Here's a step-by-step guide on how to create an azure storage account using terraform</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/15-storage-account/#task-1-define-and-declare-azure-storage-account-variables","title":"Task-1: Define and declare azure storage account variables","text":"<p>In this task, we will define and declare the necessary variables for creating the azure storage account resource. </p> <p>This table presents the variables along with their descriptions, and default values:</p> Variable Name Description Default Value storage_name (Required) Specifies the name of the storage account \"storage1\" storage_account_kind (Optional) Specifies the account kind of the storage account \"StorageV2\" storage_access_tier (Optional) Defines the access tier for BlobStorage, FileStorage, and StorageV2 accounts. Valid options are Hot and Cool, defaults to Hot. \"Hot\" storage_account_tier (Optional) Specifies the account tier of the storage account \"Standard\" storage_allow_blob_public_access (Optional) Specifies the public access type for blob storage false storage_replication_type (Optional) Specifies the replication type of the storage account \"LRS\" storage_is_hns_enabled (Optional) Specifies whether Hierarchical Namespace (HNS) is enabled for the storage account false storage_default_action Allow or disallow public access to all blobs or containers in the storage accounts. The default interpretation is true for this property. \"Allow\" storage_ip_rules Specifies IP rules for the storage account [] storage_virtual_network_subnet_ids Specifies a list of resource ids for subnets [] storage_kind (Optional) Specifies the kind of the storage account \"\" storage_container_name (Required) The name of the Container within the Blob Storage Account where Kafka messages should be captured \"container1\" storage_file_share_name (Required) The name of the File Share within the Storage Account where Files should be stored \"file-share-1\" storage_tags (Optional) Specifies the tags of the storage account {} pe_blob_subresource_names (Optional) Specifies subresource names to which the Private Endpoint can connect for Blob [\"blob\"] pe_blob_private_dns_zone_group_name (Required) Specifies the Name of the Private DNS Zone Group for Blob \"BlobPrivateDnsZoneGroup\" <p>Variable declaration:</p> variables.tf<pre><code>// ========================== storage account variables ==========================\nvariable \"storage_name\" {\n  description = \"(Required) Specifies the name of the storage account\"\n  default     = \"storage1\"\n  type        = string\n}\nvariable \"storage_account_kind\" {\n  description = \"(Optional) Specifies the account kind of the storage account\"\n  default     = \"StorageV2\"\n  type        = string\n\n  validation {\n    condition     = contains([\"Storage\", \"StorageV2\", \"BlobStorage\", \"BlockBlobStorage\", \"FileStorage\"], var.storage_account_kind)\n    error_message = \"The account kind of the storage account is invalid.\"\n  }\n}\nvariable \"storage_access_tier\" {\n  description = \"(Optional) Defines the access tier for BlobStorage, FileStorage and StorageV2 accounts. Valid options are Hot and Cool, defaults to Hot.\"\n  default     = \"Hot\"\n  type        = string\n\n  validation {\n    condition     = contains([\"Hot\", \"Cool\"], var.storage_access_tier)\n    error_message = \"The access tier of the storage account is invalid.\"\n  }\n}\nvariable \"storage_account_tier\" {\n  description = \"(Optional) Specifies the account tier of the storage account\"\n  default     = \"Standard\"\n  type        = string\n\n  validation {\n    condition     = contains([\"Standard\", \"Premium\"], var.storage_account_tier)\n    error_message = \"The account tier of the storage account is invalid.\"\n  }\n}\nvariable \"storage_allow_blob_public_access\" {\n  description = \"(Optional) Specifies the public access type for blob storage\"\n  default     = false\n  type        = bool\n}\nvariable \"storage_replication_type\" {\n  description = \"(Optional) Specifies the replication type of the storage account\"\n  default     = \"LRS\"\n  type        = string\n\n  validation {\n    condition     = contains([\"LRS\", \"GRS\", \"RAGRS\", \"ZRS\", \"GZRS\", \"RAGZRS\"], var.storage_replication_type)\n    error_message = \"The replication type of the storage account is invalid.\"\n  }\n}\nvariable \"storage_is_hns_enabled\" {\n  description = \"(Optional) Specifies the replication type of the storage account\"\n  default     = false\n  type        = bool\n}\nvariable \"storage_default_action\" {\n  description = \"Allow or disallow public access to all blobs or containers in the storage accounts. The default interpretation is true for this property.\"\n  default     = \"Allow\"\n  type        = string\n}\nvariable \"storage_ip_rules\" {\n  description = \"Specifies IP rules for the storage account\"\n  default     = []\n  type        = list(string)\n}\nvariable \"storage_virtual_network_subnet_ids\" {\n  description = \"Specifies a list of resource ids for subnets\"\n  default     = []\n  type        = list(string)\n}\nvariable \"storage_kind\" {\n  description = \"(Optional) Specifies the kind of the storage account\"\n  default     = \"\"\n}\nvariable \"storage_container_name\" {\n  description = \" (Required) The name of the Container within the Blob Storage Account where kafka messages should be captured\"\n  type        = string\n  default     = \"container1\"\n}\nvariable \"storage_file_share_name\" {\n  description = \" (Required) The name of the File Share within the Storage Account where Files should be stored\"\n  type        = string\n  default     = \"file-share-1\"\n}\nvariable \"storage_tags\" {\n  description = \"(Optional) Specifies the tags of the storage account\"\n  type        = map(any)\n  default     = {}\n}\nvariable \"pe_blob_subresource_names\" {\n  description = \"(Optional) Specifies a subresource names which the Private Endpoint is able to connect to Blob.\"\n  type        = list(string)\n  default     = [\"blob\"]\n}\nvariable \"pe_blob_private_dns_zone_group_name\" {\n  description = \"(Required) Specifies the Name of the Private DNS Zone Group for Blob. \"\n  type        = string\n  default     = \"BlobPrivateDnsZoneGroup\"\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># storage account\nstorage_access_tier                = \"Hot\"\nstorage_account_kind               = \"StorageV2\"\nstorage_replication_type           = \"RAGRS\"\nstorage_account_tier               = \"Standard\"\nstorage_allow_blob_public_access   = true\nstorage_container_name             = \"container1\"\nstorage_file_share_name            = \"file-share-1\"\n</code></pre>"},{"location":"azure/15-storage-account/#task-2-create-azure-storage-account-using-terraform","title":"Task-2: Create azure storage account using terraform","text":"<p>In this task, we will use terraform to create the azure storage account instance with the desired configuration.</p> <p>storage.tf<pre><code># create azure storage account\nresource \"azurerm_storage_account\" \"st\" {\n  name                     = \"st${var.storage_name}${local.environment}\"\n  resource_group_name      = azurerm_resource_group.rg.name\n  location                 = azurerm_resource_group.rg.location\n  access_tier              = var.storage_access_tier\n  account_kind             = var.storage_account_kind\n  account_tier             = var.storage_account_tier\n  account_replication_type = var.storage_replication_type\n  is_hns_enabled           = var.storage_is_hns_enabled\n  tags                     = merge(local.default_tags, var.storage_tags)\n  network_rules {\n    default_action             = (length(var.storage_ip_rules) + length(var.storage_virtual_network_subnet_ids)) &gt; 0 ? \"Deny\" : var.storage_default_action\n    ip_rules                   = var.storage_ip_rules\n    virtual_network_subnet_ids = var.storage_virtual_network_subnet_ids\n  }\n\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg\n  ]\n}\n</code></pre> Run terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>azure storage account - Overview blade </p> <p></p>"},{"location":"azure/15-storage-account/#task-3-create-azure-storage-account-container-using-terraform","title":"Task-3: Create azure storage account <code>container</code> using terraform","text":"<p>In this task, we will use terraform to create the azure storage account container </p> <p>storage.tf<pre><code># storage account container for Azure Event Hub capture\nresource \"azurerm_storage_container\" \"st_container_eh\" {\n  name                  = lower(\"${var.storage_container_name}\")\n  storage_account_name  = azurerm_storage_account.st.name\n  container_access_type = \"private\"\n  depends_on = [\n    azurerm_storage_account.st\n  ]\n}\n</code></pre> run terraform validation and formatting:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>azure storage account container</p> <p></p> <p></p>"},{"location":"azure/15-storage-account/#task-4-configure-diagnostic-settings-for-azure-storage-account-using-terraform","title":"Task-4: Configure diagnostic settings for azure storage account using terraform","text":"<p>By configuring diagnostic settings, we can monitor and analyze the behavior of the azure storage account instance.</p> storage.tf<pre><code># Create diagnostic settings for Storage Account\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_st\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_storage_account.st.name}\")\n  target_resource_id         = azurerm_storage_account.st.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  metric {\n    category = \"Transaction\"\n    retention_policy {\n      enabled = true\n    }\n  }\n\n  lifecycle {\n    ignore_changes = [\n      log,\n      metric\n    ]\n  }\n  depends_on = [\n    azurerm_storage_account.st,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>azure storage account - diagnostic settings from left nav</p> <p></p>"},{"location":"azure/15-storage-account/#task-5-configure-diagnostic-settings-for-azure-storage-account-container-using-terraform","title":"Task-5: Configure diagnostic settings for azure storage account container using terraform","text":"<p>By configuring diagnostic settings, we can monitor and analyze the behavior of the azure storage account container.</p> storage.tf<pre><code># Create diagnostic settings at the blob level\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_st_container\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_storage_account.st.name}-blob\")\n  target_resource_id         = \"${azurerm_storage_account.st.id}/blobServices/default/\"\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  log {\n    category = \"StorageRead\"\n    enabled  = true\n  }\n\n  log {\n    category = \"StorageWrite\"\n    enabled  = true\n  }\n\n  log {\n    category = \"StorageDelete\"\n    enabled  = true\n  }\n\n  metric {\n    category = \"Transaction\"\n  }\n\n  lifecycle {\n    ignore_changes = [\n      log,\n      metric\n    ]\n  }\n  depends_on = [\n    azurerm_storage_account.st,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>azure storage account container- diagnostic settings from left nav</p> <p></p>"},{"location":"azure/15-storage-account/#task-6-create-storage-accounts-files-share-using-terraform","title":"Task-6: Create  storage account's <code>files share</code> using terraform","text":"<p>Azure Storage File Share is commonly used for scenarios such as file sharing among multiple VMs, storing configuration files, hosting application data, and providing a centralized location for shared content in the cloud.</p> storage.tf<pre><code># Create  storage account's `files share` using terraform\nresource \"azurerm_storage_share\" \"azure_storage\" {\n  name                 = var.storage_file_share_name\n  storage_account_name = azurerm_storage_account.st.name\n  quota                = 10\n  depends_on = [\n    azurerm_storage_account.st\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p> <p></p>"},{"location":"azure/15-storage-account/#task-7-restrict-access-using-private-endpoint","title":"Task-7: Restrict access using Private Endpoint","text":"<p>To enhance security and limit access to an Azure storage account , you can utilize private endpoints and Azure Private Link. This approach assigns virtual network private IP addresses to the storage account endpoints, ensuring that network traffic between clients on the virtual network and the storage account's private endpoints traverses a secure path on the Microsoft backbone network, eliminating exposure from the public internet.</p> <p>Additionally, you can configure DNS settings for the storage account's private endpoints, allowing clients and services in the network to access the storage account using its fully qualified domain name, such as <code>privatelink.blob.core.windows.net</code>.</p> <p>This section guides you through configuring a private endpoint for your storage account using Terraform.</p>"},{"location":"azure/15-storage-account/#task-71-configure-the-private-dns-zone","title":"Task-7.1: Configure the Private DNS Zone","text":"storage.tf<pre><code># Create private DNS zone for blob storage account\nresource \"azurerm_private_dns_zone\" \"pdz_blob\" {\n  name                = \"privatelink.blob.core.windows.net\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags)\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the private DNS zone configuration.</p> <p></p>"},{"location":"azure/15-storage-account/#task-72-create-a-virtual-network-link-association","title":"Task-7.2: Create a Virtual Network Link Association","text":"storage.tf<pre><code># Create private virtual network link to prod vnet\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"blob_pdz_vnet_link\" {\n  name                  = \"privatelink_to_${azurerm_virtual_network.vnet.name}\"\n  resource_group_name   = azurerm_resource_group.vnet.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n  private_dns_zone_name = azurerm_private_dns_zone.pdz_blob.name\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.vnet,\n    azurerm_virtual_network.vnet,\n    azurerm_private_dns_zone.pdz_blob\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the Virtual network links configuration.</p> <p></p>"},{"location":"azure/15-storage-account/#task-73-create-private-endpoints-for-azure-storage","title":"Task-7.3: Create Private Endpoints for azure Storage","text":"<p>By configuring Create private endpoints for Azure Storage, we can secure storage account from public access.</p> storage.tf<pre><code># Create private endpoint for blob storage account\nresource \"azurerm_private_endpoint\" \"pe_blob\" {\n  name                = lower(\"${var.private_endpoint_prefix}-${azurerm_storage_account.st.name}\")\n  location            = azurerm_storage_account.st.location\n  resource_group_name = azurerm_storage_account.st.resource_group_name\n  subnet_id           = azurerm_subnet.jumpbox.id\n  tags                = merge(local.default_tags, var.storage_tags)\n\n  private_service_connection {\n    name                           = lower(\"${var.private_endpoint_prefix}-${azurerm_storage_account.st.name}\")\n    private_connection_resource_id = azurerm_storage_account.st.id\n    is_manual_connection           = false\n    subresource_names              = var.pe_blob_subresource_names\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = var.pe_blob_private_dns_zone_group_name\n    private_dns_zone_ids = [azurerm_private_dns_zone.pdz_blob.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_storage_account.st,\n    azurerm_private_dns_zone.pdz_blob\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> </p> <p></p> <p></p>"},{"location":"azure/15-storage-account/#task-74-validate-private-link-connection-using-nslookup-or-dig","title":"Task-7.4: Validate private link connection using <code>nslookup</code> or <code>dig</code>","text":"<p>Connecting from internal VM (private access):</p> <p><pre><code>nslookup privatelink.blob.core.windows.net\nnslookup ststorage1dev.privatelink.blob.core.windows.net\n</code></pre> output</p> <pre><code># expect the private IP Address: 10.64.3.6\n</code></pre> <p>Connecting from external (public access):</p> <pre><code>nslookup privatelink.blob.core.windows.net\nnslookup ststorage1dev.privatelink.blob.core.windows.net\n</code></pre> <p>output</p> <pre><code># should not expect the private IP Address: 10.64.3.6\n</code></pre> <p>This process ensures that the private link connection is successfully established and allows expected private IP address associated with our resource in the private virtual network.</p>"},{"location":"azure/15-storage-account/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Blob Storage documentation</li> <li>Microsoft MSDN - Create a storage account</li> <li>Microsoft MSDN - Storage account overview</li> <li>Microsoft MSDN - Create a container</li> <li>Microsoft MSDN - Use private endpoints for Azure Storage</li> <li>Terraform Registry - azurerm_storage_account</li> <li>Terraform Registry - azurerm_storage_container</li> <li>Terraform Registry - azurerm_storage_share</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - azurerm_private_endpoint</li> </ul>"},{"location":"azure/16-event-hubs-part-1/","title":"Azure Event Hubs for Apache Kafka Introduction Part-1","text":""},{"location":"azure/16-event-hubs-part-1/#what-is-azure-event-hubs","title":"What is Azure Event Hubs?","text":"<p>Azure Event Hubs is a cloud-based, scalable data streaming platform provided by Microsoft Azure. It is designed to ingest and process large volumes of streaming data from various sources in real-time. Event Hubs is part of the Azure messaging services and is particularly well-suited for scenarios involving event-driven architectures and big data analytics.</p>"},{"location":"azure/16-event-hubs-part-1/#components-of-azure-event-hubs","title":"Components of Azure Event Hubs","text":"<p>Azure Event Hubs consists of several key functional components that collectively enable the ingestion, processing, and analysis of real-time streaming data. Each component plays a specific role in the end-to-end processing of streaming data.</p> <p>Here are the key components:</p> <ol> <li> <p>Namespace:    A namespace is a container for Event Hubs, providing a scoping container for the event hubs within it. It is used to create a unique DNS name that applications can use to connect to the event hubs.</p> </li> <li> <p>Event Hubs:    Event Hubs are the main event streaming entities within a namespace. They act as a buffer that stores the streaming data before it is ingested by consuming applications. Each event hub has multiple partitions to allow for parallel processing.</p> </li> <li> <p>Partitions:    Event Hubs use partitions to enable parallel processing of events. Each partition is an ordered sequence of events, and the total throughput of an event hub is the sum of the throughputs of its partitions. Partitions provide horizontal scale and allow multiple consumers to process events concurrently.</p> </li> <li> <p>Producers:    Producers are responsible for publishing or sending events to an event hub. They can be applications, devices, or services that generate the streaming data.</p> </li> <li> <p>Consumers:    Consumers are applications or services that subscribe to event hubs to process and analyze the incoming streaming data. Multiple consumers can independently read from the same partition to achieve parallel processing.</p> </li> <li> <p>Consumer Groups:    Consumer groups provide a way to isolate different streams of events within an event hub. Each consumer group maintains its own cursor or offset in the event stream, allowing consumers in one group to progress independently from consumers in another group.</p> </li> <li> <p>Event Data:    Event data represents the payload or content of an event. It could be in various formats such as JSON, Avro, or plain text, depending on the application's requirements.</p> </li> <li> <p>Capture:     Capture is a feature that allows you to automatically capture and store the streaming data in a designated Azure Blob Storage or Azure Data Lake Storage account. This feature facilitates data retention, further analysis, and long-term storage.</p> </li> <li> <p>Auto-Inflate:     Auto-Inflate is a feature that dynamically adjusts the number of throughput units based on the incoming workload, ensuring that the system can handle varying data volumes without manual intervention.</p> </li> <li> <p>Azure Schema Registry       Azure Schema Registry is a centralized repository for managing schemas in event-driven and messaging-centric applications. </p> </li> </ol> <p>The following diagram illustrates the key components of azure event hubs architecture</p> <p></p>"},{"location":"azure/16-event-hubs-part-1/#why-azure-event-hubs","title":"Why Azure Event Hubs?","text":"<p>Here are some key reasons why Azure Event Hubs is a good choice for handling massive streams of events and data:</p> <ol> <li> <p>Real-time Data Streaming:    Azure Event Hubs serves as the  real-time data streaming from the legacy system to our new containerized microservices. This  ensuring that our organization stays responsive and adaptive to dynamic data changes.</p> </li> <li> <p>Event-driven Architecture:    Leveraging Azure Event Hubs enables us to embrace an event-driven architecture. This approach allows us to decouple components, ensuring that each microservice can independently consume data events relevant to its specific business domain.</p> </li> <li> <p>Scalability and Elasticity:    Azure Event Hubs provides the scalability and elasticity needed to handle large volumes of data events efficiently. This is crucial as our organization evolves, ensuring that our architecture can seamlessly grow to accommodate increased workloads.</p> </li> <li> <p>Publisher-Subscriber Model:    The publisher applications in our legacy system publish data events to dedicated event hubs within Azure Event Hubs. This follows a publisher-subscriber model, ensuring a structured and organized migration of data from the legacy system.</p> </li> <li> <p>Schema Consistency:    The utilization of serialization frameworks like Avro, coupled with Azure Schema Registry, guarantees consistency in data representation. This ensures that data maintains a uniform schema throughout its journey from the legacy system to the new microservices environment.</p> </li> <li> <p>Business Domain Relevance:    Consumer applications on the new microservices platform subscribe to specific event hubs, allowing them to receive real-time updates relevant to their designated business domains. This targeted approach enhances the efficiency and relevance of data consumption.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#can-you-explain-apache-kafka-on-azure-event-hubs","title":"Can you explain Apache Kafka on Azure Event Hubs?","text":"<p>Apache Kafka on Azure Event Hubs is a managed service that combines the capabilities of Apache Kafka, a popular open-source distributed streaming platform, with the scalability and ease of use of Azure Event Hubs. This service is designed to provide a Kafka-compatible interface for developers who are familiar with Kafka, while also taking advantage of the benefits of Azure's fully managed cloud platform.</p> <p>Key capabilities</p> <ol> <li> <p>Kafka Protocol Compatibility: This service supports the Kafka protocol, allowing you to use existing Kafka applications, tools, and libraries with minimal modifications. It provides a familiar Kafka experience for developers who are already familiar working with Kafka clusters.</p> </li> <li> <p>Managed Service: Unlike traditional self-hosted Kafka clusters, Apache Kafka on Azure Event Hubs is a fully managed service. This means that Microsoft Azure takes care of the infrastructure, maintenance, and operational tasks, allowing you to focus more on your application development and less on managing the underlying Kafka infrastructure.</p> </li> <li> <p>Scalability: Leveraging the underlying scalability of Azure Event Hubs, this service can handle high-throughput, real-time event streaming. It supports automatic scaling and can handle large volumes of data, making it suitable for scenarios with varying workloads.</p> </li> <li> <p>Integration with Azure Ecosystem: Apache Kafka on Azure Event Hubs seamlessly integrates with other Azure services. This includes services like Azure Functions, Azure Stream Analytics, Azure Logic Apps, and Azure Databricks, allowing you to build comprehensive and scalable event-driven applications.</p> </li> <li> <p>Azure Security and Compliance: The service benefits from Azure's security features, including Azure AD-based authentication, encryption in transit and at rest, and compliance with various industry standards and regulations.</p> </li> <li> <p>Multi-Protocol Support: While it is Kafka-compatible, Azure Event Hubs also supports other protocols, enabling interoperability with various Azure services and simplifying integration into diverse application architectures.</p> </li> <li> <p>Geo-replication: Azure Event Hubs provides geo-replication capabilities, allowing you to replicate your Kafka-enabled event streams across Azure regions for improved disaster recovery and high availability.</p> </li> </ol> <p>By offering Apache Kafka on Azure Event Hubs, Microsoft aims to provide organizations with the flexibility to use Kafka-based architectures in a managed and scalable environment, taking advantage of the benefits of Azure's cloud platform. This service is particularly useful for scenarios involving real-time data streaming, event-driven architectures, and applications that rely on the Kafka ecosystem.</p>"},{"location":"azure/16-event-hubs-part-1/#azure-event-hubs-vs-apache-kafka","title":"Azure Event Hubs vs Apache Kafka","text":"<p>The following table maps concepts between Kafka and Event Hubs.</p> Kafka Concept Event Hubs Concept Cluster Namespace Topic An Event Hub Partition Partition Consumer Group Consumer Group Offset Offset <p>This table provides a concise overview of the key differences between Azure Event Hubs and Apache Kafka across various features and considerations.</p> Feature Azure Event Hubs Apache Kafka Managed Service Fully managed by Microsoft Azure Can be self-hosted or deployed on various cloud services Ecosystem Compatibility Kafka-compatible endpoint; not a native Kafka service Native Kafka service with a rich ecosystem and community Integration with Azure Services Seamless integration with Azure services Integration may require additional configurations Ease of Scaling Automatic scaling and dynamic throughput adjustment Requires manual intervention for scaling Security and Compliance Built-in security features; supports compliance Security features need manual configuration; compliance may require additional effort Deployment Flexibility Managed service with limited customization Offers more flexibility in deployment Pricing Model Consumption-based pricing Pricing may vary based on deployment model and provider Managed Kafka Connect Offers managed Apache Kafka Connect Requires manual configuration and management Global Distribution Supports global distribution for multi-region deployment Requires manual setup for achieving multi-region deployment"},{"location":"azure/16-event-hubs-part-1/#what-is-azure-event-hubs-namespace","title":"What is Azure Event Hubs namespace?","text":"<p>An Azure Event Hubs namespace is a comprehensive management container that not only organizes event hubs but also provides essential features for access control, network integration, and security, making it a crucial administrative and operational construct for working with Azure Event Hubs.</p> <ol> <li> <p>Management Container: An Event Hubs namespace is described as a management container specifically designed for event hubs (or topics, in the context of Kafka). This reinforces the idea that the namespace serves as an organizational unit for grouping and managing related event hubs.</p> </li> <li> <p>DNS-Integrated Network Endpoints: The namespace provides DNS-integrated network endpoints. This means that each namespace is associated with a unique DNS name, making it accessible via a unique domain name. This DNS name is used for addressing and interacting with the resources within the namespace.</p> </li> <li> <p>Access Control and Network Integration Features:</p> <ul> <li>IP Filtering: The namespace supports IP filtering, allowing you to control and restrict access based on IP addresses.</li> <li>Virtual Network Service Endpoint: This feature enables integration with Azure Virtual Networks, providing a more secure and private communication channel between resources within the virtual network and the Event Hubs namespace.</li> <li>Private Link: Private Link allows you to access the Event Hubs namespace over a private, dedicated connection instead of over the public internet. This enhances the security of data transfer.</li> </ul> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#what-are-partitions","title":"What are Partitions?","text":"<p>Event Hubs organizes sequences of events into one or more partitions. A partition can be thought of as a commit log that holds event data, including the body of the event, user-defined properties, metadata (such as offset and timestamp), and its position in the stream sequence.</p> <p>partitions in Azure Event Hubs play a crucial role in enabling scalable and parallel processing of events. The choice of the number of partitions and the effective use of partition keys impact the system's throughput, scalability, and the ability to process events in an organized manner.</p> <p>Advantages of Using Partitions:</p> <ul> <li>Parallel Processing: Partitions help with processing large volumes of events by allowing multiple parallel logs to be used for the same event hub. This multiplies the available raw I/O throughput capacity.</li> <li>Scalability: Partitions enable the scaling out of processing capacity. Your applications can handle the volume of events by having multiple processes, and partitions help feed those processes while ensuring each event has a clear processing owner.</li> </ul>"},{"location":"azure/16-event-hubs-part-1/#what-is-shared-access-policies","title":"What is Shared Access Policies?","text":"<p>Shared Access Policies in Azure Event Hubs are security constructs that allow you to control and manage access to your Event Hubs resources. They provide a way to grant specific permissions to clients, applications, or devices without exposing the primary access key. Each shared access policy defines a set of permissions and is associated with a name, which is used when generating Shared Access Signature (SAS) tokens.</p> <p>Access Policy Types:</p> <p>The available access policy types include:</p> <ul> <li>Send: Allows sending events to an Event Hub.</li> <li>Listen: Allows receiving events from an Event Hub.</li> <li>Manage: Provides management operations such as creating and deleting Event Hubs, and managing consumer groups.</li> </ul> <p>RootManageSharedAccessKey</p> <p>In Azure Event Hubs, the RootManageSharedAccessKey Shared Access Signature (SAS) policy is a predefined access policy that holds the highest level of privileges and is associated with the primary key of the Event Hubs namespace. This policy is automatically created when you create an Event Hubs namespace, and it has full control over all aspects of the namespace, including the ability to manage and revoke access keys, create and delete Event Hubs, and perform administrative tasks.</p> <p>Note</p> <p>Security Best Practices: It is recommended to avoid using the RootManageSharedAccessKey directly in applications or services unless absolutely necessary. Instead, best practices involve creating more granular Shared Access Policies with the minimum required permissions for specific scenarios.</p>"},{"location":"azure/16-event-hubs-part-1/#what-is-event-publishers","title":"What is Event publishers?","text":"<p><code>Event publishers</code> refer to entities that send data to an Azure Event Hub. The term \"event publishers\" is used synonymously with \"event producers.\" These entities are responsible for generating and transmitting events or messages to an Event Hub, which is a central component of Azure Event Hubs.</p> <p>Here are key points about event publishers:</p> <ol> <li> <p>Definition: An event publisher is any entity that produces and sends data to an Event Hub.</p> </li> <li> <p>Protocols: Event publishers can use different protocols to publish events to Azure Event Hubs. The supported protocols include HTTPS, AMQP 1.0 (Advanced Message Queuing Protocol), and the Kafka protocol.</p> </li> <li> <p>Authentication and Authorization: Event publishers need to authenticate and obtain authorization to publish events to an Event Hub. They can use Microsoft Entra ID based authorization with OAuth2-issued JWT tokens or an Event Hub-specific Shared Access Signature (SAS) token to gain publishing access. These mechanisms ensure secure and controlled access to the Event Hub.</p> </li> <li> <p>Token-based Authentication: The use of OAuth2-issued JWT tokens or Event Hub-specific Shared Access Signature (SAS) tokens implies a token-based authentication approach. This means that event publishers must present a valid token as part of the authentication process to establish their identity and permissions.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#what-is-event-consumers","title":"What is Event consumers?","text":"<p><code>Event consumers</code> in the context of Azure Event Hubs are entities or applications that receive and process events from an Event Hub. These consumers subscribe to specific topic within an Event Hub namespace and are responsible for handling the incoming stream of events in real-time. Event consumers play a crucial role in event-driven architectures by enabling downstream processing, analysis, or storage of the streaming data.</p> <p>Here are key points about event consumers in Azure Event Hubs:</p> <ol> <li> <p>Subscription to Topic:    Event consumers subscribe to one or more Topics within an Event Hub namespace. Each topic in cluster acts as an independent stream of events, and consumers can read from multiple topics concurrently to achieve parallel processing.</p> </li> <li> <p>Parallel Processing:     By having multiple consumers reading from different topic, parallel processing is achieved. This allows for horizontal scaling of event processing, enhancing the overall throughput of the system.</p> </li> <li> <p>Offset Management:    Event consumers maintain an offset, which is a pointer to the last successfully processed event in a topic. The offset is used to keep track of the position in the event stream. It ensures that events are processed in order and that no events are missed.</p> </li> <li> <p>Consumer Groups:    Event consumers are organized into consumer groups, each identified by a unique name. Consumer groups enable multiple independent streams of processing within an Event Hub namespace. Each consumer group maintains its offset per topic, allowing for independent and parallel processing by different groups.</p> </li> <li> <p>Integration with Downstream Services:    Event consumers often integrate with downstream services, such as databases, analytics platforms, or other event-driven components. This integration allows for further processing, analysis, or storage of the event data.</p> </li> <li> <p>Fault Tolerance:    Event consumers need to be designed for fault tolerance. This includes handling transient failures, implementing retries, and ensuring that the processing logic can recover gracefully in case of disruptions.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#what-is-event-retention","title":"What is Event retention?","text":"<p>Event retention refers to the duration for which events or messages are stored within the system. It represents the period during which the system retains the event data, making it available for consumers to retrieve. Event retention is a crucial aspect of event-driven architectures, allowing organizations to balance the need for historical data analysis with storage considerations.</p> <p>Here are key points about event retention:</p> <ol> <li> <p>Configurability: Event retention is typically a configurable setting that allows the user to specify how long events should be retained in the event streaming system. </p> </li> <li> <p>Retention Period: The retention period is the amount of time that events are kept in the system. </p> </li> <li> <p>Maximum Retention Period: Event streaming platforms often have a maximum retention period beyond which events are automatically purged. This maximum retention period is determined by the platform's design and storage capabilities.</p> </li> <li> <p>Automatic Removal: Events are automatically removed from the system when the retention period for each event expires. Once an event is older than the specified retention period, it is no longer available for retrieval by consumers.</p> </li> <li> <p>Impact on Storage: The configured event retention period has an impact on the storage requirements for the event streaming system. Longer retention periods require more storage space to retain historical events.</p> </li> <li> <p>Archival Strategies: In scenarios where organizations need to retain events beyond the configured retention period, they may employ archival strategies. This could involve storing events in long-term storage solutions, such as Azure Storage or Azure Data Lake.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#explain-capture-events","title":"Explain Capture events:","text":"<p>In Azure Event Hubs, the capture feature allows you to automatically capture and store the streaming data (events) sent to an Event Hub into a designated Azure Blob Storage or Azure Data Lake Storage account. This feature is known as \"Event Hubs Capture.\" It is particularly useful for scenarios where you need to retain, analyze, or process historical data for compliance, auditing, or analytics purposes.</p> <p>Here are the key aspects of capturing events in Azure Event Hubs:</p> <ol> <li> <p>Configuration:    To enable event capture, you need to configure Event Hubs Capture settings for your Event Hub. This includes specifying the Azure Storage account (either Blob Storage or Data Lake Storage) where the captured data will be stored.</p> </li> <li> <p>Storage Path and Container:    When configuring Event Hubs Capture, you define a storage path pattern that determines the organization of the captured data within the specified storage account. You also specify the storage container where the captured data will be stored.</p> </li> <li> <p>File Naming:    Event Hubs Capture allows you to define a naming pattern for the captured files. This pattern can include placeholders such as <code>{Namespace}</code>, <code>{EventHub}</code>, <code>{PartitionId}</code>, and <code>{Year}</code>, enabling dynamic naming based on the content and source of the events.</p> </li> <li> <p>File Formats:    The captured events are stored in files, and you can choose between two common file formats: Avro and Apache Parquet. Avro is a compact binary format, while Parquet is a columnar storage format that provides efficient compression and query performance.</p> </li> <li> <p>Retention Policy:    Event Hubs Capture allows you to set a retention policy for the captured data. This defines how long the captured data should be retained in the storage account before it is automatically deleted.</p> </li> <li> <p>Data Accessibility:    Once events are captured, they become accessible in the specified storage account. You can then use various tools and services to analyze, process, or visualize the captured data. For example, you might use Azure Databricks, Azure Synapse Analytics, or other analytics tools to gain insights from the captured events.</p> </li> <li> <p>Use Cases:    Event Hubs Capture is valuable for scenarios where historical data analysis is required. It can be used for compliance auditing, tracking changes over time, debugging, and troubleshooting. The captured data can also be used for reprocessing events or replaying them into a different system.</p> </li> <li> <p>Capture Status Monitoring:    Azure provides monitoring capabilities for Event Hubs Capture through Azure Monitor. You can monitor the status of capture, track successful and failed captures, and set up alerts based on specific criteria.</p> </li> <li> <p>Integration with Other Azure Services:     The captured events can be seamlessly integrated with other Azure services for further processing or analysis. For example, you might integrate the captured data with Azure Stream Analytics, Azure Databricks, or Azure Data Factory for advanced analytics and insights.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#consumer-groups","title":"Consumer groups","text":"<p>Consumer groups provide a logical grouping of consumers that read data from an event hub or Kafka topic.</p> <p>Consumer groups enable multiple consuming applications to read the same streaming data independently at their own pace with their offsets.</p> <p>This parallelization allows distributing the workload among multiple consumers while maintaining the order of messages within each partition.</p> <p>It is recommended to have only one active receiver on a partition within a consumer group to avoid processing duplicate events.</p> <p>consumer groups in Azure Event Hubs provide a flexible and scalable mechanism for parallelizing the consumption of streaming data, accommodating various downstream applications, and managing the details of event processing. The recommendation for having one active receiver per partition in a consumer group helps maintain order and avoid duplicate event processing.</p>"},{"location":"azure/16-event-hubs-part-1/#application-groups","title":"Application groups","text":"<p>An application group is a collection of one or more client applications that interact with the Event Hubs data plane. Each application group can be scoped to a single Event Hubs namespace or event hubs (entity) within a namespace.</p> <p>Application groups are logical entities that are created at the namespace level. Therefore, client applications interacting with event hubs don't need to be aware of the existence of an application group. Event Hubs can associate any client application to an application group by using the identifying condition.</p>"},{"location":"azure/16-event-hubs-part-1/#what-is-avro-format-in-apache-kafka","title":"What is Avro format in apache kafka?","text":"<p>In Apache Kafka, Avro is a binary serialization format that is often used in combination with the Confluent Schema Registry or  Azure Schema Registry. Avro provides a compact, fast, and schema-based serialization approach, making it well-suited for efficient data representation and compatibility in Kafka. Here are key aspects of Avro format in Apache Kafka:</p> <ol> <li> <p>Schema-Driven Serialization:    Avro is schema-driven, meaning that data is serialized in accordance with a schema that describes the structure of the data. The schema is often defined using JSON, and it is included with the serialized data.</p> </li> <li> <p>Compact Binary Format:    Avro uses a compact binary format for serialization, resulting in smaller payload sizes compared to other serialization formats like JSON or XML. This compactness is beneficial for reducing network bandwidth and storage requirements in Kafka.</p> </li> <li> <p>Schema Evolution:    Avro supports schema evolution, allowing changes to the schema over time without breaking compatibility with existing data. This is crucial in Kafka scenarios where producers and consumers may evolve independently.</p> </li> <li> <p>Interoperability:    Avro is designed for interoperability, enabling data to be exchanged between systems implemented in different programming languages. This flexibility aligns with the distributed nature of Kafka, where producers and consumers may be written in diverse languages.</p> </li> <li> <p>Apache Avro Project:    Avro is an open-source project that is part of the Apache Software Foundation. It is widely used in the big data ecosystem and is supported by various programming languages and frameworks.</p> </li> <li> <p>Confluent Schema Registry:    The Confluent Schema Registry is often used in conjunction with Avro in Kafka. The Schema Registry serves as a centralized repository for storing and managing Avro schemas. It allows producers and consumers to register, retrieve, and evolve schemas dynamically.</p> </li> <li> <p>Schema Compatibility:    The Schema Registry ensures schema compatibility between producers and consumers. When data is produced or consumed, the Schema Registry validates that the schema aligns with the expected format, preventing data mismatches and enhancing data quality.</p> </li> <li> <p>Code Generation:    Avro often involves code generation based on the schema. Code can be generated in different programming languages to facilitate the serialization and deserialization processes, improving performance and ensuring compatibility.</p> </li> <li> <p>Avro Serialization in Kafka Producers:    Kafka producers can use Avro for serialization when sending messages to Kafka topics. The Avro serialization ensures that the data conforms to a predefined schema and is compatible with consumers that expect the same schema.</p> </li> <li> <p>Avro Deserialization in Kafka Consumers:     Kafka consumers, on the other hand, can deserialize Avro-encoded messages using the associated schema. This allows consumers to interpret the structure of the data correctly.</p> </li> <li> <p>Performance Considerations:     Avro's compact binary format and schema-based approach contribute to efficient serialization and deserialization, making it a performant choice for handling large volumes of data in Kafka.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-1/#what-is-azure-schema-registry","title":"What is Azure Schema Registry?","text":"<p>Azure Schema Registry is a feature of Azure Event Hubs that offers a centralized repository for managing schemas in event-driven and messaging-centric applications. Here are key points to understand about Azure Schema Registry:</p> <ol> <li> <p>Centralized Schema Repository: Azure Schema Registry serves as a central location to store and manage schemas for the data exchanged between producer and consumer applications within the Azure Event Hubs ecosystem.</p> </li> <li> <p>Flexibility in Data Exchange: The registry provides flexibility for producer and consumer applications, allowing them to exchange data without the need to manage and share the schema explicitly. This simplifies the communication process between different components of an application.</p> </li> <li> <p>Governance Framework: Azure Schema Registry includes a governance framework that facilitates the management of reusable schemas. This framework defines relationships between schemas through a grouping construct known as schema groups.</p> </li> <li> <p>Schema-Driven Serialization Frameworks: The registry supports schema-driven serialization frameworks, such as Apache Avro. This approach involves moving serialization metadata into shared schemas, which can help reduce the per-message overhead. Unlike tagged formats such as JSON, each message doesn't need to carry metadata, as the schema provides the necessary type information and field names.</p> </li> </ol>"},{"location":"azure/16-event-hubs-part-2/","title":"Create Azure Event Hubs for Apache Kafka using Terraform Part-2","text":""},{"location":"azure/16-event-hubs-part-2/#introduction","title":"Introduction","text":"<p>Azure Event Hubs is a cloud-based, scalable data streaming platform provided by Microsoft Azure. It is designed to ingest and process large volumes of streaming data from various sources in real-time. Event Hubs is part of the Azure messaging services and is particularly well-suited for scenarios involving event-driven architectures and big data analytics.</p> <p>In this hands-on lab, I'll guide you through the process of creating an <code>Azure Event Hub namespace</code>, <code>Azure event hubs</code> using terraform, create diagnostic settings, create shared access policies and finally enhancing security through the use of private endpoints.</p>"},{"location":"azure/16-event-hubs-part-2/#technical-scenario-use-case","title":"Technical Scenario (Use case)","text":"<p>As a <code>Cloud Architect</code>, I have been tasked with designing a robust solution for a big data streaming platform, migrating data from our legacy platform to a modern SaaS multi-tenant platform following microservices architecture hosted on Azure Kubernetes Service (AKS) using an event-driven approach. The primary focus of this migration is to enhance agility, scalability, and maintainability, with <code>Azure Event Hubs</code> identified as the big data streaming platform migrating from the legacy to the new platform.</p> <ul> <li> <p>Background: Our organization has relied on a legacy platform for an extended period sincde year, and our architecture board has recently finalized the design for a digital transformation initiative. To achieve this transformation, we have embraced a containerized microservices architecture deployed on Azure Kubernetes Service (AKS) along with Azure event hub for big data streaming platform. The adoption of this modern architecture aims to bring about increased agility, scalability, and maintainability.</p> </li> <li> <p>Legacy System: The legacy system currently holds a substantial volume of data that requires migration to the new microservices-based platform.</p> </li> <li> <p>Microservices Architecture: Our new platform is composed of multiple microservices, each dedicated to specific business functionalities. These microservices are meticulously designed to be loosely coupled and independently deployable, aligning with best practices in modern software architecture.</p> </li> <li> <p>Azure Event Hubs Integration: To enable real-time data streaming from the legacy system to the new containerized microservices, we have strategically chosen azure event hubs as our central event streaming platform.</p> </li> <li> <p>Publisher Applications (Legacy System): Publisher applications play a pivotal role in this migration by handling the responsibility of publishing data events to dedicated event hubs within azure event hubs. Each publisher application corresponds to a specific data domain or entity type, ensuring a granular and organized approach to data migration.</p> </li> <li> <p>Event Serialization and Schema: For efficient data serialization, producer applications utilize frameworks such as Avro to serialize data into a common schema. This schema is registered in Azure Schema Registry, guaranteeing consistency in data representation and format across the migration process.</p> </li> <li> <p>Consumer Applications (New Microservices): On the new microservices platform, consumer applications are designed to seamlessly consume events related to their specific business domains. These microservices subscribe to the event hub, allowing them to receive real-time updates as data is streamed from the legacy system.</p> </li> </ul>"},{"location":"azure/16-event-hubs-part-2/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare azure event hubs variables</li> <li>Task-2: Create storage account resources using terraform</li> <li>Task-3: Create kafka azure event hubs namespace</li> <li>Task-4: Create diagnostic settings for event hub namespace</li> <li>Task-5: Shared access policies for event hub namespace Level</li> <li>Task-5.1: Create shared access policy rule for listen</li> <li>Task-5.2: Create shared access policy rule for send</li> <li>Task-5.3: Create shared access policy rule for manage</li> <li>Task-6: Restrict access using private endpoint &amp; virtual network</li> <li>Task-6.1: Configure the private DNS zone</li> <li>Task-6.2: Create a virtual network link association</li> <li>Task-6.3: Create private endpoints for azure event hubs</li> <li>Task-6.4: Validate private link connection using nslookup or dig</li> <li>Task-7: Create azure event hubs or kafka topics</li> </ul>"},{"location":"azure/16-event-hubs-part-2/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the key components of azure event hubs architecture</p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ol> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet - for configuring a private endpoint.</li> <li>Basic knowledge of terraform and azure concepts.</li> </ol>"},{"location":"azure/16-event-hubs-part-2/#implementation-details","title":"Implementation details","text":"<p>Here's a step-by-step guide on how to create an azure event hub namespace and azure event hubs using Terraform</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/16-event-hubs-part-2/#task-1-define-and-declare-azure-event-hubs-variables","title":"Task-1: Define and declare azure event hubs variables","text":"<p>In this task, we will define and declare the necessary variables for creating the azure event hub namespace and azure event hub resources. </p> <p>This table summarizes the key information about each variable, including its name, description, and default value.</p> Variable Name Description Default Value kafka_eh_prefix Prefix of the Azure Event Hub (Kafka) name that's combined with the name of the event hub namespace. \"ehns\" kafka_eh_namespace_name (Required) Specifies the resource group name of the Event Hub namespace name that's combined with the name of the event hub namespace. \"eventhub1\" kafka_eh_resource_group_name (Required) Specifies the resource group name of the Event Hub. \"replace me\" kafka_eh_location (Required) Specifies the location where the Event Hub will be deployed. \"replace me\" kafka_eh_sku (Required) Defines which tier to use. Valid options are Basic, Standard, and Premium. Please note that setting this field to Premium will force the creation of a new resource. \"Standard\" kafka_eh_capacity (Optional) Specifies the Capacity / Throughput Units for a Standard SKU namespace. Default capacity has a maximum of 2 but can be increased in blocks of 2 on a committed purchase basis. 2 kafka_eh_partition_count (Optional) Specifies the number of partitions for a Kafka topic. 10 kafka_eh_message_retention (Optional) Specifies the number of message retention. 1 kafka_eh_topics (Optional) An array of strings that indicates values of Kafka topics. [\"eventhub-1\", \"eventhub-2\", \"eventhub-3\", \"eventhub-4\", \"eventhub-5\"] kafka_eh_tags (Optional) Specifies the tags of the Kafka event hub. {} <p>Variable declaration:</p> variables.tf<pre><code>// ==========================  azure event hubs variables ==========================\nvariable \"kafka_eh_prefix\" {\n  type        = string\n  default     = \"ehns\"\n  description = \"Prefix of the Azure Event Hub (Kafka) name that's combined with name of the event hub namespace.\"\n}\nvariable \"kafka_eh_namespace_name\" {\n  type        = string\n  default     = \"eventhub1\"\n  description = \"(Required) Specifies the resource group name of the Event Hub namespace name that's combined with name of the event hub namespace.\"\n}\nvariable \"kafka_eh_resource_group_name\" {\n  description = \"(Required) Specifies the resource group name of the Event Hub.\"\n  type        = string\n  default     = \"replace me\" \n}\nvariable \"kafka_eh_location\" {\n  description = \"(Required) Specifies the location where the Event Hub will be deployed.\"\n  type        = string\n  default     = \"replace me\" \n}\nvariable \"kafka_eh_sku\" {\n  description = \"(Required) Defines which tier to use. Valid options are Basic, Standard, and Premium. Please note that setting this field to Premium will force the creation of a new resource.\"\n  type        = string\n  default     = \"Standard\"\n  validation {\n    condition     = contains([\"Standard\", \"Premium\"], var.kafka_eh_sku)\n    error_message = \"The sku of the event hub is invalid.\"\n  }\n}\nvariable \"kafka_eh_capacity\" {\n  description = \"(Optional) Specifies the Capacity / Throughput Units for a Standard SKU namespace. Default capacity has a maximum of 2, but can be increased in blocks of 2 on a committed purchase basis.\"\n  type        = number\n  default     = 2\n}\nvariable \"kafka_eh_partition_count\" {\n  description = \"(Optional) Specifies the  number of partitions for a Kafka topic.\"\n  type        = number\n  default     = 10\n}\nvariable \"kafka_eh_message_retention\" {\n  description = \"(Optional) Specifies the  number of message_retention \"\n  type        = number\n  default     = 1\n}\n\nvariable \"kafka_eh_topics\" {\n  description = \"(Optional) An array of strings that indicates values of kafka topics.\"\n  type        = list(string)\n  default = [\n    \"eventhub-1\",\n    \"eventhub-2\",\n    \"eventhub-3\",\n    \"eventhub-4\",\n    \"eventhub-5\",\n  ]\n}\nvariable \"kafka_eh_tags\" {\n  description = \"(Optional) Specifies the tags of the Kafka event hub\"\n  type        = map(any)\n  default     = {}\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># kafka event hub\nkafka_eh_namespace_name             = \"eventhub1\"\nkafka_eh_sku                        = \"Standard\"\nkafka_eh_capacity                   = 1\nkafka_eh_partition_count            = 10\nkafka_eh_message_retention          = 1\n</code></pre>"},{"location":"azure/16-event-hubs-part-2/#task-2-create-storage-account-resources-using-terraform","title":"Task-2: Create storage account resources using terraform","text":"<p>In this task, the objective is to establish an Azure Storage Account specifically designed for capturing events from the Kafka system. As part of the 'Create Storage Account using Terraform' lab, we have already generated resources related to the storage account. For more detailed information, refer to the documentation provided in the 'Create Storage Account using Terraform' lab.</p> <p>Create Storage Account using Terraform</p>"},{"location":"azure/16-event-hubs-part-2/#task-3-create-kafka-azure-event-hubs-namespace","title":"Task-3: Create kafka azure event hubs namespace","text":"<p>Establishing an azure event hubs Namespace dedicated to a business unit ensures a centralized, scalable, and reliable platform for handling real-time event streaming. It acts as the core component for event ingestion and distribution.</p> eventhub.tf<pre><code># Create azure event hub namespace using terraform\nresource \"azurerm_eventhub_namespace\" \"kafka_eh\" {\n  name                = lower(\"${var.kafka_eh_prefix}-${var.kafka_eh_namespace_name}-${local.environment}\")\n  resource_group_name = azurerm_resource_group.rg.name\n  location            = azurerm_resource_group.rg.location\n  sku                 = var.kafka_eh_sku\n  capacity            = var.kafka_eh_capacity\n  # auto_inflate_enabled     = true\n  # maximum_throughput_units = 20\n  network_rulesets {\n    default_action                 = \"Deny\" //\"Allow\"\n    trusted_service_access_enabled = true\n    virtual_network_rule = [\n      {\n        subnet_id                                       = azurerm_subnet.aks.id\n        ignore_missing_virtual_network_service_endpoint = false\n    }]\n  }\n\n  tags = merge(local.default_tags, var.kafka_eh_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-4-create-diagnostic-settings-for-event-hub-namespace","title":"Task-4: Create diagnostic settings for event hub namespace","text":"<p>This task configures diagnostic settings but at the event hub namespace level. It captures logs specific to Kafka-related activities, providing visibility into the performance and health of the event hub.</p> eventhub.tf<pre><code># Create azure event hub namespace diagnostic settings using terraform\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_kafka_eh\" {\n  name                       = lower(\"${var.diag_prefix}-${azurerm_eventhub_namespace.kafka_eh.name}\")\n  target_resource_id         = azurerm_eventhub_namespace.kafka_eh.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n  enabled_log {\n    category_group = \"allLogs\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n  enabled_log {\n    category_group = \"audit\"\n\n    retention_policy {\n      days    = 0\n      enabled = false\n    }\n  }\n\n  metric {\n    category = \"AllMetrics\"\n    enabled  = true\n\n    # retention_policy {\n    #   enabled = true\n    #   days    = var.kafka_eh_log_analytics_retention_days\n    # }\n  }\n  lifecycle {\n    ignore_changes = [\n      # enabled_log\n    ]\n  }\n\n  depends_on = [\n    azurerm_eventhub_namespace.kafka_eh,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p> <p>Diagnostic settings details:</p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-5-shared-access-policies-for-event-hub-namespace-level","title":"Task-5: Shared access policies for event hub namespace Level","text":""},{"location":"azure/16-event-hubs-part-2/#task-51-create-shared-access-policy-rule-for-listen","title":"Task-5.1: Create shared access policy rule for listen","text":"<p>This task creates a shared access policy rule with listening permissions. It enables entities to consume events from the event hub namespace, supporting secure and controlled access to the streaming data.</p> <p>eventhub.tf<pre><code># Create shared access policy rule for listen\nresource \"azurerm_eventhub_namespace_authorization_rule\" \"ns_sap_listen\" {\n  name                = \"ns_sap_listen-${local.environment}\"\n  namespace_name      = azurerm_eventhub_namespace.kafka_eh.name\n  resource_group_name = azurerm_eventhub_namespace.kafka_eh.resource_group_name\n  listen              = true // Grants listen access to this this Authorization Rule.\n  send                = false\n  manage              = false\n}\n</code></pre> Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-52-create-shared-access-policy-rule-for-send","title":"Task-5.2: Create shared access policy rule for send","text":"<p>Establishing a shared access policy rule with sending permissions allows entities to publish events to the event hub namespace. It ensures controlled data ingestion, preventing unauthorized entities from sending data.</p> eventhub.tf<pre><code># Create shared access policy rule for send\nresource \"azurerm_eventhub_namespace_authorization_rule\" \"ns_sap_send\" {\n  name                = \"ns_sap_send-${local.environment}\"\n  namespace_name      = azurerm_eventhub_namespace.kafka_eh.name\n  resource_group_name = azurerm_eventhub_namespace.kafka_eh.resource_group_name\n  listen              = true // Grants listen access to this this Authorization Rule.\n  send                = true // Grants send access to this this Authorization Rule\n  manage              = false\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-53-create-shared-access-policy-rule-for-manage","title":"Task-5.3: Create shared access policy rule for manage","text":"<p>Creating a shared access policy rule with management permissions provides entities the capability to manage and administer the event hub namespace. It's crucial for maintaining the security and configuration of the event hub.</p> eventhub.tf<pre><code># Create shared access policy rule for manage\nresource \"azurerm_eventhub_namespace_authorization_rule\" \"ns_sap_manage\" {\n  name                = \"ns_sap_manage-${local.environment}\"\n  namespace_name      = azurerm_eventhub_namespace.kafka_eh.name\n  resource_group_name = azurerm_eventhub_namespace.kafka_eh.resource_group_name\n  listen              = true // Grants listen access to this this Authorization Rule.\n  send                = true // Grants send access to this this Authorization Rule.\n  manage              = true // Grants manage access to this this Authorization Rule.\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-6-restrict-access-using-private-endpoint-virtual-network","title":"Task-6: Restrict access using private endpoint &amp; virtual network","text":"<p>To enhance security and limit access to an Azure Event Hubs namespace, you can utilize private endpoints and Azure Private Link. This approach assigns virtual network private IP addresses to the Azure Event Hubs namespace endpoints, ensuring that network traffic between clients on the virtual network and the Azure Event Hubs namespace's private endpoints traverses a secure path on the Microsoft backbone network, eliminating exposure from the public internet.</p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-61-configure-the-private-dns-zone","title":"Task-6.1: Configure the private DNS zone","text":"<p>Creating a private DNS zone enhances security by allowing resolution of azure event hubs privately. It's a prerequisite for establishing a private link between the virtual network and the azure event hubs.</p> eventhub.tf<pre><code># Create private DNS zone for azure event hub namespace\nresource \"azurerm_private_dns_zone\" \"pdz_ehns\" {\n  name                = \"privatelink.servicebus.windows.net\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-62-create-a-virtual-network-link-association","title":"Task-6.2: Create a virtual network link association","text":"<p>This task associates the virtual network with the private DNS zone, enabling DNS resolution of Azure azure event hubs services within the virtual network. It's a key step for establishing a private link.</p> eventhub.tf<pre><code># Create private virtual network link to virtual network\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"ehns_pdz_vnet_link\" {\n  name                  = \"privatelink_to_${azurerm_virtual_network.vnet.name}\"\n  resource_group_name   = azurerm_resource_group.vnet.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n  private_dns_zone_name = azurerm_private_dns_zone.pdz_ehns.name\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.vnet,\n    azurerm_virtual_network.vnet,\n    azurerm_private_dns_zone.pdz_ehns\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-63-create-private-endpoints-for-azure-event-hubs","title":"Task-6.3: Create private endpoints for azure event hubs","text":"<p>Creating private endpoints for Azure azure event hubs ensures that data traffic between the virtual network and azure event hubs remains within the Microsoft Azure network. It enhances security by avoiding exposure to the public internet.</p> eventhub.tf<pre><code># Create private endpoint for Event Hubs Namespace\nresource \"azurerm_private_endpoint\" \"pe_ehns\" {\n  name                = lower(\"${var.private_endpoint_prefix}-${azurerm_eventhub_namespace.kafka_eh.name}\")\n  location            = azurerm_eventhub_namespace.kafka_eh.location\n  resource_group_name = azurerm_eventhub_namespace.kafka_eh.resource_group_name\n  subnet_id           = azurerm_subnet.jumpbox.id\n  tags                = merge(local.default_tags, var.kafka_eh_tags)\n\n  private_service_connection {\n    name                           = \"pe-${azurerm_eventhub_namespace.kafka_eh.name}\"\n    private_connection_resource_id = azurerm_eventhub_namespace.kafka_eh.id\n    is_manual_connection           = false\n    subresource_names              = [\"namespace\"]\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = \"default\"\n    private_dns_zone_ids = [azurerm_private_dns_zone.pdz_ehns.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n  depends_on = [\n    azurerm_eventhub_namespace.kafka_eh,\n    azurerm_private_dns_zone.pdz_ehns\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p> <p></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#task-64-validate-private-link-connection-using-nslookup-or-dig","title":"Task-6.4: Validate private link connection using nslookup or dig","text":"<p>Manually validating the private link connection ensures that the private endpoints are properly configured and functioning. using nslookup or dig confirms the successful resolution of the private endpoint's DNS name within the virtual network.</p> <p>This process ensures that the private link connection is successfully established and allows expected private IP address associated with our resource in the private virtual network.</p>"},{"location":"azure/16-event-hubs-part-2/#task-7-create-azure-event-hubs-or-kafka-topics","title":"Task-7: Create azure event hubs or kafka topics","text":"<p>This task involves the creation of Azure Event Hubs or Kafka Topics, providing a crucial foundation for applications to efficiently consume events from these messaging platforms. The objective is to demonstrate the capability of creating multiple topics systematically. In the course of this task, we will create five topics, showcasing a approach to establishing and managing multiple topics within a loop.</p> eventhub.tf<pre><code># Create azure event hubs or Kafka Topics\nresource \"azurerm_eventhub\" \"eventhubs\" {\n  for_each = toset([\n    \"eventhub-1\",\n    \"eventhub-2\",\n    \"eventhub-3\",\n    \"eventhub-4\",\n    \"eventhub-5\",\n  ])\n  name                = each.key\n  namespace_name      = azurerm_eventhub_namespace.kafka_eh.name\n  resource_group_name = azurerm_eventhub_namespace.kafka_eh.resource_group_name\n  partition_count     = var.kafka_eh_partition_count\n  message_retention   = var.kafka_eh_message_retention\n  capture_description {\n    enabled  = true\n    encoding = \"Avro\"\n    destination {\n      archive_name_format = \"{Namespace}/{EventHub}/{PartitionId}/{Year}_{Month}_{Day}/{Hour}_{Minute}_{Second}\"\n      name                = \"EventHubArchive.AzureBlockBlob\"\n      blob_container_name = azurerm_storage_container.st_container_eh.name\n      storage_account_id  = azurerm_storage_account.st.id\n    }\n  }\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n  depends_on = [\n    azurerm_eventhub_namespace.kafka_eh,\n    azurerm_storage_account.st,\n    azurerm_storage_container.st_container_eh\n  ]\n}\n</code></pre> <p>Run terraform validate &amp; format: <pre><code>terraform validate\nterraform fmt\n</code></pre></p> <p>Run terraform plan &amp; apply: <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre></p> <p></p>"},{"location":"azure/16-event-hubs-part-2/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Azure Event Hubs documentation</li> <li>Microsoft MSDN - Azure Blob Storage documentation</li> <li>Microsoft MSDN - Create a storage account</li> <li>Microsoft MSDN - Storage account overview</li> <li>Microsoft MSDN - Create a container</li> <li>Terraform Registry - azurerm_storage_account</li> <li>Terraform Registry - azurerm_storage_container</li> <li>Terraform Registry - azurerm_storage_share</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_eventhub_namespace</li> <li>Terraform Registry - azurerm_eventhub_namespace_authorization_rule</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - azurerm_private_endpoint</li> </ul>"},{"location":"azure/17-cdn-frontdoor/","title":"Create Front Door and CDN profile using Terraform","text":""},{"location":"azure/17-cdn-frontdoor/#introduction","title":"Introduction","text":"<p>Azure Front Door and Azure CDN (Content Delivery Network) services are provided by Microsoft Azure</p> <p>Azure Front Door</p> <p><code>Azure Front Door</code>is a global, scalable entry-point service that optimizes and secures web applications. It acts as a load balancer with routing, providing global load balancing, SSL termination, and application acceleration. Azure Front Door improves the performance, availability, and security of web applications.</p> <p>Key Features:</p> <ol> <li> <p>Global Load Balancing: Distributes user traffic across multiple endpoints globally, ensuring users are directed to the closest and healthiest server.</p> </li> <li> <p>Web Application Firewall (WAF): Provides protection against common web vulnerabilities and exploits through customizable security policies.</p> </li> <li> <p>SSL/TLS Termination: Terminates SSL/TLS at the edge to reduce the load on backend servers and enhance performance.</p> </li> <li> <p>Acceleration of Dynamic and Static Content: Optimizes content delivery by caching static content at edge locations and accelerating dynamic content.</p> </li> <li> <p>Session Affinity: Supports sticky sessions to direct user requests to the same backend server, ensuring a consistent user experience.</p> </li> <li> <p>Custom Domains: Allows the use of custom domain names and certificates for secure communication.</p> </li> <li> <p>Health Probing: Monitors the health of backend servers and automatically routes traffic away from unhealthy servers.</p> </li> <li> <p>Routing Rules: Enables flexible routing based on criteria such as path, host, query string, and client IP address.</p> </li> <li> <p>Backend Pools: Groups endpoints (backend servers) into pools for better organization and management.</p> </li> <li> <p>Analytics and Monitoring: Provides insights into user traffic, performance, and security through analytics and monitoring tools.</p> </li> </ol> <p>Azure CDN Profile:</p> <p>Azure CDN is a distributed network of servers that delivers web content (such as images, videos, and scripts) to users based on their geographical location. It helps reduce latency and improves content delivery by caching and serving content from <code>edge locations</code> that are closer to the end-users.</p> <p>Key Features:</p> <ol> <li> <p>Content Caching: Distributes and caches content at edge locations to reduce latency and improve performance.</p> </li> <li> <p>Dynamic Site Acceleration: Optimizes delivery of dynamic content by using advanced caching techniques.</p> </li> <li> <p>HTTPS Support: Provides secure content delivery over HTTPS.</p> </li> <li> <p>Origin Fetch Optimization: Minimizes the need to fetch content from the origin server by serving it directly from the edge.</p> </li> <li> <p>Compression: Compresses content to reduce file sizes and improve delivery speed.</p> </li> <li> <p>Content Purging: Allows manual purging of cached content to ensure updated content is delivered.</p> </li> <li> <p>Rules Engine: Enables customization of content delivery rules based on various criteria.</p> </li> <li> <p>Analytics and Monitoring: Provides insights into content delivery performance and user behavior.</p> </li> </ol> <p><code>Azure Front Door</code> is primarily focused on application optimization, global load balancing, and security, while <code>Azure CDN</code> is focused on efficiently delivering and caching content to improve overall performance. </p> <p>In this lab, I will walk through the steps to create an Azure Front Door &amp; CDN profile using Terraform. We'll also configure frontend URL of Azure Front Door and backend origin group and origin with route and custom domain.  I'll also configure diagnostic settings to monitor its performance effectively. Finally, we'll validate these resources within the Azure portal to confirm that everything is functioning as expected.</p>"},{"location":"azure/17-cdn-frontdoor/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Architect</code>, your mission is to architect a solution for optimizing the caching of static and dynamic web content globally, elevating user experiences, and ensuring the creation of a high-performance, globally accessible, and secure website. Leveraging the capabilities of Azure Front Door and CDN profile is key to achieving these goals efficiently.</p>"},{"location":"azure/17-cdn-frontdoor/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and Declare Azure Front Door &amp; CDN variables.</li> <li>Task-2: Create a Front Door CDN profile using Terraform.</li> <li>Task-3: Create a Front Door Endpoint using using Terraform - (frontend).</li> <li>Task-4: Create a Front Door Origin Group using Terraform - (backend).</li> <li>Task-5: Create a Front Door Origin using Terraform - (backend).</li> <li>Task-6: Create Custom Domains for the Front Door using Terraform.</li> <li>Task-7: Create a Front Door Route using Terraform.</li> <li>Task-8: Create a DNS TXT (temporary) record in DNS Zone</li> <li>Task-9: Create DNS CNAME records in DNS Zone</li> <li>Task-10: Configure diagnostic settings for CDN profile</li> <li>Task-10: Apply lock on Front Door Profile</li> </ul>"},{"location":"azure/17-cdn-frontdoor/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the high level architecture of Azure Front Door &amp; CDN:</p> <p></p>"},{"location":"azure/17-cdn-frontdoor/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this lab, make sure you have the following prerequisites in place:</p> <ul> <li>Download and Install Terraform.</li> <li>Download and Install Azure CLI.</li> <li>Azure subscription.</li> <li>Visual Studio Code.</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Azure Public DNS zone - for custom domain</li> <li>Basic knowledge of Terraform and Azure concepts.</li> </ul>"},{"location":"azure/17-cdn-frontdoor/#implementation-details","title":"Implementation details","text":"<p>Step-by-step implementation details will be covered here.</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-1-define-and-declare-azure-front-door-cdn-variables","title":"Task-1: Define and Declare Azure Front Door &amp; CDN variables.","text":"<p>In this task, we will define and declare the necessary variables for creating the Azure Front Door &amp; CDN resources. These variables will be used to specify the Azure Front Door &amp; CDN resources settings and customize the values according to our requirements in each environment.</p> <p>This table presents the variables along with their descriptions, data types, and default values:</p> Variable Name Description Type Default Value <code>cdn_frontdoor_prefix</code> Prefix of the Front Door &amp; CDN name. <code>string</code> <code>\"afd\"</code> <code>cdn_frontdoor_tags</code> Specifies a mapping of tags for the Front Door Endpoint. <code>map(any)</code> <code>{}</code> <code>cdn_frontdoor_profile_name</code> Specifies the name of the Front Door Profile. <code>string</code> <code>\"afd-cdn_frontdoor1-dev\"</code> <code>cdn_frontdoor_profile_sku_name</code> The SKU for the Front Door profile. Possible values include: <code>Standard_AzureFrontDoor</code>, <code>Premium_AzureFrontDoor</code>. <code>string</code> <code>\"Standard_AzureFrontDoor\"</code> <code>cdn_frontdoor_endpoint_name</code> The name for the Front Door Origin. <code>string</code> <code>\"endpoint1\"</code> <code>cdn_frontdoor_endpoint_enabled</code> Specifies if this Front Door Endpoint is enabled. Defaults to <code>true</code>. <code>bool</code> <code>true</code> <code>cdn_frontdoor_origin_group_name</code> The name for the Front Door Origin Group. <code>string</code> <code>\"origingroup1\"</code> <code>session_affinity_enabled</code> Specifies whether session affinity should be enabled on this host. Defaults to <code>true</code>. <code>bool</code> <code>true</code> <code>restore_traffic_time_to_healed_or_new_endpoint_in_minutes</code> Time before shifting traffic to another endpoint when a healthy endpoint becomes unhealthy or a new endpoint is added. <code>number</code> <code>10</code> <code>cdn_frontdoor_origin_name</code> The name for the Front Door Origin. <code>string</code> <code>\"origin1\"</code> <code>cdn_frontdoor_origin_host_ip</code> The IPv4 address, IPv6 address, or Domain name of the Origin. <code>string</code> <code>\"20.125.213.106\"</code> <code>cdn_frontdoor_origin_host_name</code> The host name of the domain. The <code>host_name</code> field must be the FQDN of your domain. <code>string</code> <code>\"sitename.mydomain.com\"</code> <code>cdn_frontdoor_origin_host_header</code> The host header value which is sent to the origin with each request. <code>string</code> <code>\"sitename.mydomain.com\"</code> <code>cdn_frontdoor_route_name</code> The name for the Front Door Route. Valid values must begin with a letter or number and may only contain letters, numbers, and hyphens with a maximum length of 90 characters. <code>string</code> <code>\"route1\"</code> <code>content_types_to_compress</code> (Optional) An array of strings that indicates a content types on which compression will be applied. The value for the elements should be MIME types. <code>list(string)</code> See default values in the code <code>public_dns_zone_name</code> The name of the DNS Zone. Must be a valid domain name. <code>string</code> <code>\"mydomain.com\"</code> <code>public_dns_zone_rg_name</code> Specifies the resource group where the resource exists. <code>string</code> <code>\"rg-mydomains-dev\"</code> <code>cdn_frontdoor_custom_domain_name</code> The name for the Front Door Custom Domain. Must be between 2 and 260 characters in length. <code>string</code> <code>\"sitename\"</code> <code>dns_txt_record</code> The name of the DNS TXT Record. <code>string</code> <code>\"sitename\"</code> <code>dns_cname_record</code> The name of the DNS CNAME Record. <code>string</code> <code>\"sitename\"</code> <p>Variable declaration:</p> variables.tf<pre><code>// ========================== Azure Front Door &amp; CDN profile ==========================\n# Task-1: Define and Declare Azure Front Door &amp; CDN variables.\n# cdn_profile\nvariable \"cdn_frontdoor_prefix\" {\n  type        = string\n  default     = \"afd\"\n  description = \"Prefix of the Front Door &amp; CDN name.\"\n}\nvariable \"cdn_frontdoor_tags\" {\n  description = \"(Optional) Specifies a mapping of tags which should be assigned to the Front Door Endpoint.\"\n  type        = map(any)\n  default     = {}\n}\nvariable \"cdn_frontdoor_profile_name\" {\n  type        = string\n  default     = \"afd-cdn_frontdoor1-dev\"\n  description = \"(Required) Specifies the name of the Front Door Profile.\"\n}\nvariable \"cdn_frontdoor_profile_sku_name\" {\n  type        = string\n  description = \"The SKU for the Front Door profile. Possible values include: Standard_AzureFrontDoor, Premium_AzureFrontDoor\"\n  default     = \"Standard_AzureFrontDoor\"\n  validation {\n    condition     = contains([\"Standard_AzureFrontDoor\", \"Premium_AzureFrontDoor\"], var.cdn_frontdoor_profile_sku_name)\n    error_message = \"The SKU value must be one of the following: Standard_AzureFrontDoor, Premium_AzureFrontDoor.\"\n  }\n}\n# endpoint\nvariable \"cdn_frontdoor_endpoint_name\" {\n  type        = string\n  default     = \"endpoint1\"\n  description = \"(Required) The name which should be used for this Front Door Origin. Changing this forces a new Front Door Origin to be created.\"\n}\nvariable \"cdn_frontdoor_endpoint_enabled\" {\n  description = \" (Optional) Specifies if this Front Door Endpoint is enabled? Defaults to true.\"\n  default     = true\n  type        = bool\n}\n# origin_group\nvariable \"cdn_frontdoor_origin_group_name\" {\n  type        = string\n  default     = \"origingroup1\"\n  description = \"(Required) The name which should be used for this Front Door Origin Group.\"\n}\nvariable \"session_affinity_enabled\" {\n  description = \"(Optional) Specifies whether session affinity should be enabled on this host. Defaults to true.\"\n  default     = true\n  type        = bool\n}\nvariable \"restore_traffic_time_to_healed_or_new_endpoint_in_minutes\" {\n  description = \"(Optional) Specifies the amount of time which should elapse before shifting traffic to another endpoint when a healthy endpoint becomes unhealthy or a new endpoint is added. Possible values are between 0 and 50 minutes (inclusive). Default is 10 minutes.\"\n  default     = 10\n  type        = number\n}\n# origin\nvariable \"cdn_frontdoor_origin_name\" {\n  type        = string\n  default     = \"origin1\"\n  description = \"(Required) The name which should be used for this Front Door Origin.\"\n}\nvariable \"cdn_frontdoor_origin_host_ip\" {\n  type        = string\n  default     = \"20.125.213.106\"\n  description = \"(Required) The IPv4 address, IPv6 address or Domain name of the Origin.\"\n}\nvariable \"cdn_frontdoor_origin_host_name\" {\n  type        = string\n  default     = \"sitename.mydomain.com\"\n  description = \"(Required) The host name of the domain. The host_name field must be the FQDN of your domain(e.g. contoso.fabrikam.com).\"\n}\nvariable \"cdn_frontdoor_origin_host_header\" {\n  type        = string\n  default     = \"sitename.mydomain.com\"\n  description = \"(Optional) The host header value (an IPv4 address, IPv6 address or Domain name) which is sent to the origin with each request.\"\n}\n# route\nvariable \"cdn_frontdoor_route_name\" {\n  type        = string\n  default     = \"route1\"\n  description = \"(Required) The name which should be used for this Front Door Route. Valid values must begin with a letter or number, end with a letter or number and may only contain letters, numbers and hyphens with a maximum length of 90 characters.\"\n}\nvariable \"content_types_to_compress\" {\n  description = \"Specifies the address space of the hub virtual virtual network\"\n  type        = list(string)\n  default = [\n    \"application/eot\",\n    \"application/font\",\n    \"application/font-sfnt\",\n    \"application/javascript\",\n    \"application/json\",\n    \"application/opentype\",\n    \"application/otf\",\n    \"application/pkcs7-mime\",\n    \"application/truetype\",\n    \"application/ttf\",\n    \"application/vnd.ms-fontobject\",\n    \"application/x-font-opentype\",\n    \"application/x-font-truetype\",\n    \"application/x-font-ttf\",\n    \"application/x-httpd-cgi\",\n    \"application/x-javascript\",\n    \"application/x-mpegurl\",\n    \"application/x-opentype\",\n    \"application/x-otf\",\n    \"application/x-perl\",\n    \"application/x-ttf\",\n    \"application/xhtml+xml\",\n    \"application/xml\",\n    \"application/xml+rss\",\n    \"font/eot\",\n    \"font/opentype\",\n    \"font/otf\",\n    \"font/ttf\",\n    \"image/svg+xml\",\n    \"text/css\",\n    \"text/csv\",\n    \"text/html\",\n    \"text/javascript\",\n    \"text/js\",\n    \"text/plain\",\n    \"text/richtext\",\n    \"text/tab-separated-values\",\n    \"text/x-component\",\n    \"text/x-java-source\",\n    \"text/x-script\",\n    \"text/xml\",\n  ]\n}\n# custom_domain\nvariable \"public_dns_zone_name\" {\n  type        = string\n  default     = \"mydomain.com\"\n  description = \"(Required) The name of the DNS Zone. Must be a valid domain name. \"\n}\nvariable \"public_dns_zone_rg_name\" {\n  type        = string\n  default     = \"rg-mydomains-dev\"\n  description = \"(Required) Specifies the resource group where the resource exists.\"\n}\nvariable \"cdn_frontdoor_custom_domain_name\" {\n  type        = string\n  default     = \"sitename\"\n  description = \"(Required) The name which should be used for this Front Door Custom Domain. Possible values must be between 2 and 260 characters in length, must begin with a letter or number, end with a letter or number and contain only letters, numbers and hyphens.\"\n}\nvariable \"dns_txt_record\" {\n  type        = string\n  default     = \"sitename\"\n  description = \"(Required) The name of the DNS TXT Record.\"\n}\nvariable \"dns_cname_record\" {\n  type        = string\n  default     = \"sitename\"\n  description = \" (Required) The name of the DNS CNAME Record.\"\n}\n</code></pre> <p>Variable Definition:</p> dev-variables.tfvars<pre><code># Azure Front Door &amp; CDN profile\ncdn_frontdoor_profile_name          = \"frontdoor1\"\ncdn_frontdoor_profile_sku_name      = \"Standard_AzureFrontDoor\"\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-2-create-a-front-door-cdn-profile-using-terraform","title":"Task-2: Create a Front Door CDN profile using Terraform.","text":"<p>In this task, you will use Terraform to create an Azure CDN profile. The provided Terraform code demonstrates how to configure the CDN profile, including its name, resource group, SKU, and tags. This profile is the foundation of your content delivery network (CDN).</p> <p>CDN Profile which contains a collection of endpoints and origin groups which we will discuss in next steps.</p> cdn-frontdoor.tf<pre><code># Task-2: Create a Front Door CDN profile using Terraform.\nresource \"azurerm_cdn_frontdoor_profile\" \"cdn_frontdoor_profile\" {\n  name                = lower(\"${var.cdn_frontdoor_prefix}-${var.cdn_frontdoor_profile_name}-${local.environment}\")\n  resource_group_name = azurerm_resource_group.rg.name\n  sku_name            = var.cdn_frontdoor_profile_sku_name\n  tags                = merge(local.default_tags)\n  depends_on = [\n    azurerm_resource_group.rg,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> Azure Front Door and CDN profile - Overview blade </p> <p></p>"},{"location":"azure/17-cdn-frontdoor/#task-3-create-a-front-door-endpoint-using-terraform","title":"Task-3: Create a Front Door Endpoint using Terraform.","text":"<p>In Azure Front Door, an \"Endpoint\" represents a specific destination to which incoming requests are routed. Endpoints are critical components of the routing and load balancing capabilities provided by Azure Front Door. Each endpoint points to a backend resource, such as an Azure Web App, an Azure API Management instance, a custom domain, or any other service that you want to expose through Azure Front Door.</p> <p>For example, suppose you have created an endpoint named <code>myendpoint</code>. The endpoint domain name might be <code>myendpoint-mdjf2jfgjf82mnzx.z01.azurefd.net</code>.</p> <p>for more information - https://learn.microsoft.com/en-us/azure/frontdoor/endpoint?tabs=azurecli </p> cdn-frontdoor.tf<pre><code># Task-3: Create a Front Door endpoint using Terraform. - (frontend) \nresource \"azurerm_cdn_frontdoor_endpoint\" \"cdn_frontdoor_endpoint\" {\n  name                     = var.cdn_frontdoor_endpoint_name\n  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile.id\n  enabled                  = var.cdn_frontdoor_endpoint_enabled\n  tags                     = merge(local.default_tags, var.cdn_frontdoor_tags)\n  depends_on = [\n    azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/17-cdn-frontdoor/#task-4-create-a-front-door-origin-group-using-terraform","title":"Task-4: Create a Front Door Origin Group using Terraform.","text":"<p>An Origin Group, also known as a Backend Pool, is a logical grouping of multiple backend resources (usually web servers or application instances) that serve the same content or application but may be distributed across different geographic locations or data centers. The primary purpose of an Origin Group is to ensure high availability and load balancing.</p> <p>more information - https://learn.microsoft.com/en-us/azure/frontdoor/origin?pivots=front-door-standard-premium</p> cdn-frontdoor.tf<pre><code># Task-4: Create a Front Door origin group using Terraform. (backend)\nresource \"azurerm_cdn_frontdoor_origin_group\" \"cdn_frontdoor_origin_group\" {\n  name                                                      = var.cdn_frontdoor_origin_group_name\n  cdn_frontdoor_profile_id                                  = azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile.id\n  session_affinity_enabled                                  = var.session_affinity_enabled\n  restore_traffic_time_to_healed_or_new_endpoint_in_minutes = var.restore_traffic_time_to_healed_or_new_endpoint_in_minutes\n\n  load_balancing {\n    additional_latency_in_milliseconds = 50\n    sample_size                        = 4\n    successful_samples_required        = 3\n  }\n\n  # health_probe {\n  #   path                = \"/\"\n  #   request_type        = \"HEAD\"\n  #   protocol            = \"Http\"\n  #   interval_in_seconds = 100\n  # }\n  depends_on = [\n    azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/17-cdn-frontdoor/#task-5-create-a-front-door-origin-using-terraform","title":"Task-5: Create a Front Door Origin using Terraform.","text":"<p>Origins are the destinations where Front Door forwards user traffic</p> <p>An origin can be any service or resource that you want to expose through Azure Front Door. This includes Azure Web Apps, API Management instances, custom domains, or other backend services.</p> <p>Origins are part of Origin Groups in Azure Front Door.</p> <p>for more information - https://learn.microsoft.com/en-us/azure/frontdoor/origin?pivots=front-door-standard-premium</p> cdn-frontdoor.tf<pre><code># Task-5: Create a Front Door origin using Terraform. (backend)\nresource \"azurerm_cdn_frontdoor_origin\" \"cdn_frontdoor_origin\" {\n  name                          = var.cdn_frontdoor_origin_name\n  cdn_frontdoor_origin_group_id = azurerm_cdn_frontdoor_origin_group.cdn_frontdoor_origin_group.id\n\n  enabled                        = true\n  host_name                      = \"20.125.213.106\" # azurerm_public_ip.appgtw_pip.ip_address\n  http_port                      = 80\n  https_port                     = 443\n  origin_host_header             = \"sitename.mydomain.com\" # azurerm_public_ip.appgtw_pip.ip_address\n  priority                       = 1\n  weight                         = 1000\n  certificate_name_check_enabled = false\n\n  depends_on = [\n    azurerm_cdn_frontdoor_origin_group.cdn_frontdoor_origin_group,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> <p>Here's a bit more detail:</p> <ul> <li> <p>Origin Group: An Origin Group is a logical grouping of multiple backend Origins. It allows you to specify a collection of backend endpoints, often geographically distributed, that host the same content or service. These endpoints are used to serve traffic for your application. An Origin Group can be used for redundancy, load balancing, or traffic distribution based on your configuration.</p> </li> <li> <p>Origin: An Origin, within an Origin Group, is a specific backend endpoint or server that hosts the content you want to serve. You might have multiple Origins within an Origin Group to provide high availability and fault tolerance. Azure Front Door automatically balances and routes traffic to these Origins.</p> </li> </ul> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/17-cdn-frontdoor/#task-6-create-custom-domains-for-the-front-door-using-terraform","title":"Task-6: Create custom domains for the Front Door using Terraform.","text":"<p>This Terraform configuration is used to define and create a custom domain for an Azure Front Door profile, associating it with the provided Azure CDN Front Door profile and specifying details related to TLS for secure communication. The custom domain allows you to map your own domain name to the Front Door, enabling users to access your services via a user-friendly domain, such as www.example.com, while benefiting from the performance and security features of Azure Front Door.</p> <p>cdn-frontdoor.tf<pre><code>data \"azurerm_dns_zone\" \"dns_zone\" {\n  name                = var.public_dns_zone_name\n  resource_group_name = var.public_dns_zone_rg_name\n}\n\n# Task-6: Create custom domains for the Front Door\nresource \"azurerm_cdn_frontdoor_custom_domain\" \"cdn_frontdoor_custom_domain\" {\n  name                     = var.cdn_frontdoor_custom_domain_name\n  cdn_frontdoor_profile_id = azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile.id\n  dns_zone_id              = data.azurerm_dns_zone.dns_zone.id\n  host_name                = var.cdn_frontdoor_origin_host_name\n  tls {\n    certificate_type    = \"ManagedCertificate\"\n    minimum_tls_version = \"TLS12\"\n  }\n  depends_on = [\n    azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile,\n    data.azurerm_dns_zone.dns_zone,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> for more information - https://learn.microsoft.com/en-us/azure/frontdoor/domain</p> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-7-create-a-front-door-route-using-terraform","title":"Task-7: Create a Front Door Route using Terraform","text":"<p>In the context of Azure Front Door and similar content delivery or load balancing services, a <code>Route</code> typically refers to the configuration or rule that defines how incoming requests are directed to specific backend Origins or Origin Groups.</p> <p>cdn-frontdoor.tf<pre><code># Task-7: Create Route in Origin of the Origin Group\nresource \"azurerm_cdn_frontdoor_route\" \"cdn_frontdoor_route\" {\n  name                            = var.cdn_frontdoor_route_name\n  cdn_frontdoor_endpoint_id       = azurerm_cdn_frontdoor_endpoint.cdn_frontdoor_endpoint.id\n  cdn_frontdoor_origin_group_id   = azurerm_cdn_frontdoor_origin_group.cdn_frontdoor_origin_group.id\n  cdn_frontdoor_origin_ids        = [azurerm_cdn_frontdoor_origin.cdn_frontdoor_origin.id]\n  cdn_frontdoor_custom_domain_ids = [azurerm_cdn_frontdoor_custom_domain.cdn_frontdoor_custom_domain.id]\n\n  supported_protocols = [\"Http\", \"Https\"]\n  patterns_to_match   = [\"/*\", \"/\"]\n  forwarding_protocol    = \"MatchRequest\"\n  link_to_default_domain = true\n  https_redirect_enabled = true\n  cache {\n    query_string_caching_behavior = \"UseQueryString\"\n    # Content won't be compressed when the requested content is smaller than 1 KB or larger than 8 MB(inclusive).\n    compression_enabled       = true\n    content_types_to_compress = var.content_types_to_compress\n  }\n  depends_on = [\n    azurerm_cdn_frontdoor_endpoint.cdn_frontdoor_endpoint,\n    azurerm_cdn_frontdoor_origin_group.cdn_frontdoor_origin_group,\n    azurerm_cdn_frontdoor_origin.cdn_frontdoor_origin,\n    azurerm_cdn_frontdoor_custom_domain.cdn_frontdoor_custom_domain,\n  ]\n  lifecycle {\n    ignore_changes = [\n      # tags\n    ]\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-8-create-a-dns-txt-temporary-record-in-dns-zone","title":"Task-8: Create a DNS TXT (temporary) record in DNS Zone","text":"<p>The provided Terraform configuration is used to create a DNS TXT record in an Azure DNS zone. The primary purpose of this DNS TXT record is to validate ownership and control of a custom domain for Azure Front Door. </p> cdn-frontdoor.tf<pre><code># Task-8: Create a DNS TXT (temporary) record in DNS Zone\nresource \"azurerm_dns_txt_record\" \"dns_txt_record_validation\" {\n  name                = join(\".\", [\"_dnsauth\", var.dns_txt_record])\n  zone_name           = data.azurerm_dns_zone.dns_zone.name\n  resource_group_name = data.azurerm_dns_zone.dns_zone.resource_group_name\n  ttl                 = 3600\n\n  record {\n    value = azurerm_cdn_frontdoor_custom_domain.cdn_frontdoor_custom_domain.validation_token\n  }\n  depends_on = [\n    azurerm_cdn_frontdoor_custom_domain.cdn_frontdoor_custom_domain,\n    data.azurerm_dns_zone.dns_zone\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>This Terraform configuration creates a DNS TXT record in an Azure DNS zone with specific content, and it associates it with Azure Front Door for domain ownership validation. It's an essential step when setting up a custom domain to point to Azure Front Door, as it proves that you have control over the domain you're configuring.</p>"},{"location":"azure/17-cdn-frontdoor/#task-9-create-dns-cname-records-in-dns-zone","title":"Task-9: Create DNS CNAME records in DNS Zone","text":"<p>This Terraform task facilitates the automated setup of DNS CNAME records, linking your custom domain or subdomain to an Azure CDN Front Door endpoint for streamlined access to your application.</p> cdn-frontdoor.tf<pre><code># Task-9: Create DNS CNAME records in DNS Zone\nresource \"azurerm_dns_cname_record\" \"dns_cname_record\" {\n  name                = var.dns_cname_record\n  zone_name           = data.azurerm_dns_zone.dns_zone.name\n  resource_group_name = data.azurerm_dns_zone.dns_zone.resource_group_name\n  ttl                 = 3600\n  record              = azurerm_cdn_frontdoor_endpoint.cdn_frontdoor_endpoint.host_name\n\n  depends_on = [\n    data.azurerm_dns_zone.dns_zone,\n    azurerm_cdn_frontdoor_endpoint.cdn_frontdoor_endpoint,\n    # azurerm_cdn_frontdoor_security_policy.example\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-10-configure-diagnostic-settings-for-cdn-profile","title":"Task-10: Configure diagnostic settings for CDN profile","text":"<p>By configuring diagnostic settings, we can monitor and analyze the performance and behavior of the Azure Front Door &amp; CDN profile instance.</p> redis_cache.tf<pre><code># Task-10: Create Diagnostic Settings for Azure Front Door\nresource \"azurerm_monitor_diagnostic_setting\" \"cdn_frontdoor_diag\" {  \n  name                       = \"DiagnosticsSettings\"\n  target_resource_id         = azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  log {\n    category_group = \"allLogs\"\n  }\n\n  log {\n    category_group = \"audit\"\n  }\n\n  metric {\n    category = \"AllMetrics\"\n  }\n  lifecycle {\n    ignore_changes = [\n      log_analytics_destination_type,\n    ]\n  }\n  depends_on = [\n    azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#task-11-apply-lock-on-front-door-profile","title":"Task-11: Apply lock on Front Door Profile","text":"<p>The purpose of applying a lock to an Azure resource, in this case, the Azure Front Door profile, is to prevent accidental deletion. By setting a \"CanNotDelete\" lock, you are ensuring that the resource remains in place and operational, which is particularly useful for critical resources in production environments to avoid data loss or service disruption caused by accidental deletion. It provides an additional layer of protection for important resources.</p> cdn-frontdoor.tf<pre><code># Task-10: Apply lock on Front Door Profile\nresource \"azurerm_management_lock\" \"cdn_frontdoor_lock\" {\n  name       = \"afd-profile\"\n  scope      = azurerm_cdn_frontdoor_profile.cdn_frontdoor_profile.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_cdn_frontdoor_endpoint.cdn_frontdoor_endpoint,\n    azurerm_cdn_frontdoor_origin_group.cdn_frontdoor_origin_group,\n    azurerm_cdn_frontdoor_origin.cdn_frontdoor_origin,\n    azurerm_cdn_frontdoor_custom_domain.cdn_frontdoor_custom_domain,\n    azurerm_cdn_frontdoor_route.cdn_frontdoor_route\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre>"},{"location":"azure/17-cdn-frontdoor/#reference","title":"Reference","text":"<p>Here are some references related to Azure Front Door and CDN:</p> <ul> <li>Microsoft MSDN - Azure Front Door and CDN documentation</li> <li>Microsoft MSDN - What is Azure Front Door?</li> <li>Microsoft MSDN - Create an Azure Front Door Standard/Premium profile using Terraform</li> <li>Microsoft MSDN - Decision tree for load balancing in Azure</li> <li>Terraform Registry - azurerm_cdn_frontdoor_profile</li> <li>Terraform Registry - azurerm_cdn_frontdoor_endpoint</li> <li>Terraform Registry - azurerm_cdn_frontdoor_origin_group</li> <li>Terraform Registry - azurerm_cdn_frontdoor_origin</li> <li>Terraform Registry - azurerm_cdn_frontdoor_route</li> <li>Terraform Registry - azurerm_cdn_frontdoor_custom_domain</li> <li>Terraform Registry - azurerm_dns_txt_record</li> <li>Terraform Registry - azurerm_dns_cname_record</li> <li>Azure Terraform Quickstart/101-front-door-standard-premium</li> </ul>"},{"location":"azure/2-naming-conventions/","title":"2 naming conventions","text":""},{"location":"azure/2-naming-conventions/#naming-conventions","title":"Naming conventions","text":"<p>Azure naming conventions are guidelines and best practices for naming resources in Azure. These conventions help us to ensure that resources are easily identifiable, consistent, and organized within your Azure environment. Some of the key components of Azure naming conventions include:</p> <ul> <li> <p>Consistency: All resources should be named in a consistent manner, using the same naming convention across all resource types. This makes it easier to identify and locate resources in the Azure portal and other Azure tools.</p> </li> <li> <p>Meaningful names: Resources should be named in a way that makes their purpose and context clear. This includes using prefixes or suffixes to indicate the resource type, environment, or other relevant information.</p> </li> <li> <p>Length and character limitations: Azure has a maximum length limit and character limitations on the name of a resource, it's important to be aware of these limitations when creating a name.</p> </li> <li> <p>Uniqueness: Resources must have unique names within the subscription and resource group they are located in.</p> </li> <li> <p>Avoid using special characters and spaces: Azure resource names cannot contain special characters, such as \u201c!\u201d, \u201c@\u201d, \u201c#\u201d, and \u201c$\u201d, and spaces.</p> </li> </ul> <p>Using a consistent naming convention for resources can help us to improve the organization and management of your Azure environment, making it easier to find, understand and troubleshoot resources.</p> <p>Examples of naming conventions for Azure resources: <pre><code>- Virtual Machine: &lt;prefix&gt;-vm-&lt;environment&gt;-&lt;number&gt;\n- Virtual Network: &lt;prefix&gt;-vnet-&lt;environment&gt;-&lt;number&gt;\n- Storage Account: &lt;prefix&gt;-storage-&lt;environment&gt;-&lt;number&gt;\n- SQL Server: &lt;prefix&gt;-sql-&lt;environment&gt;-&lt;number&gt;\n</code></pre></p>"},{"location":"azure/2-naming-conventions/#tagging-conventions","title":"Tagging conventions","text":"<p>Tagging is useful for tracking and managing resources, as well as for cost allocation and reporting. Azure tagging conventions are guidelines and best practices for tagging resources in Azure, similar to naming conventions. These conventions help to ensure that resources are easily identifiable, consistent, and organized within your Azure environment.</p> <p>Examples of tagging conventions for Azure resources:</p> <pre><code>Environment: Key: \"Environment\" Value: \"Production\"\nCost Center: Key: \"Cost Center\" Value: \"CC100\"\nApplication: Key: \"Application\" Value: \"Project1\"\n</code></pre> <p>It is always recommended to follow the industry standards and best practices rather than reinventing the weel. read the following links and understand the pattern and create your own standards if any deviations from these naming conventions and standards.</p> <p>naming and tagging conventions template is available here for download and modify as per your company needs.</p> <p>https://raw.githubusercontent.com/microsoft/CloudAdoptionFramework/master/ready/naming-and-tagging-conventions-tracking-template.xlsx</p> <p>best way to force these naming conventions for entire organization level is to keep them in the terraform configuration file so that it can used or re-used while creating all the services. you will see these in actions during azure resource creation using terraform configuration.</p>"},{"location":"azure/2-naming-conventions/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Develop your naming and tagging strategy for Azure resources</li> <li>Microsoft MSDN - Define your naming convention</li> <li>Microsoft MSDN - Abbreviation examples for Azure resources</li> </ul>"},{"location":"azure/3-azure-account-subscription/","title":"Azure Account & Subscription Management","text":"<p>In this lab you will learn how to create new azure account &amp; new azure subscription so that you can accomplish rest of the labs.</p> <p>Azure Account:    - An Azure account is essentially a user account that is associated with Microsoft Azure. This account allows individuals or organizations to access Azure services and resources.    - An Azure account can be created using an email address provided by Microsoft , or it can be associated with an organization's domain if they are using Azure Active Directory (Azure AD).</p> <p>Azure Subscription:    - An Azure subscription is a logical container that holds the Azure resources and services used by an organization or an individual.    - It is tied to the billing and payment of Azure services. Azure resources are billed based on usage, and all usage is linked to a specific subscription.   </p> <p>Azure Active Directory (Azure AD):    - Azure Active Directory is Microsoft's cloud-based identity and access management service. It is often referred to as Azure AD.    - It serves as a directory service, storing information about users, groups, and applications, including their access permissions.</p> <p>Azure Subscription Management Hierarchy</p> <p></p>"},{"location":"azure/3-azure-account-subscription/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need a Credit Card</li> <li>Valid email address</li> <li>Working phone number</li> </ul>"},{"location":"azure/3-azure-account-subscription/#create-new-azure-account","title":"Create new azure account","text":"<p>Follow these steps to create new free azure account:</p> <p>Step-1: Go to the following URL - Azure free account</p> <p>Step-2: Click on start Free.</p> <p>Step-3: If you already have Microsoft Account, then Sign-in using existing email address and password. else Sign-up for a Microsoft account using your personal email address.</p> <p>Step-4: Enter your Country/Region and Date of Birth and click next.</p> <p>Step-5: Enter the verification code received on the email address and click next.</p> <p>Step-6: Type the captcha you see on your screen and click on next.</p> <p>Step-7: Enter details in your profile screen </p> <p>Step-7: Enter payment details</p> <p>Step-8: Check the Terms and Conditions and click Sign-up.</p> <p>Once your account is created, you will be taken to the Azure portal where you can start creating and managing resources.</p>"},{"location":"azure/3-azure-account-subscription/#create-new-subscription","title":"Create new Subscription","text":"<p>Prerequisites </p> <ul> <li>Azure Account or Tenant</li> <li>Azure subscription creator role</li> </ul> <p>There are different subscription types available in Azure, creating new subscription steps are completely different depending on type of the subscription you want to create.</p> <p>read the information from following Microsoft documentation before creating any new subscription to make sure that you select the right one.</p> <p>Create a subscription Create an Enterprise Agreement subscription Create a subscription for a partner's customer</p> <p>To create a new Azure subscription, you can follow these steps:</p> <p>Step-1: Login into Azure portal - Azure portal login</p> <p>Step-2: Click on Subscriptions in the left nav and click Add button</p> <p>Step-3: On the Subscriptions page, click on the \"+New Subscription\" button.</p> <p>Step-4: On the \"Create a subscription\" page, select the type of subscription you want to create. This can be a \"Pay-As-You-Go\" subscription, a \"Free Trial\" subscription, or an \"Enterprise Agreement\" subscription.</p> <p>Step-5: Enter all the required details in the Basic and Advance tabs</p> <p>Step-6: Review all the details and finally click on Review + Create button</p> <p>After a few minutes, your new subscription will be created and will be listed on the Subscriptions page in the Azure portal.</p>"},{"location":"azure/3-azure-account-subscription/#reference","title":"Reference","text":"<ul> <li>YouTube - Chapter-1.1: Create New Azure Portal Account</li> <li>Microsoft- Azure free account</li> <li>Microsoft- Create a subscription</li> </ul>"},{"location":"azure/4-azure-subscription/","title":"4 azure subscription","text":"<p>In this lab you will learn how to create a new azure subscription so that you can accomplish rest of the labs. if you already have Free Azure account then you can skip this lab.</p> <p>Prerequisites </p> <ul> <li>Azure Account or Tenant</li> <li>Azure subscription creator role</li> </ul> <p>There are different subscription types available in Azure, creating new subscription steps are completely different depending on type of the subscription you want to create.</p> <p>read the information from following Microsoft documentation before creating any new subscription to make sure that you select the right one.</p> <ul> <li>https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/create-subscription#create-a-subscription</li> <li>https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/create-enterprise-subscription</li> <li>https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/create-customer-subscription</li> </ul> <p>To create a new Azure subscription, you can follow these steps:</p> <p>Step-1: Login into Azure portal - https://portal.azure.com/</p> <p>Step-2: Click on Subscriptions in the left nav and click Add button</p> <p>Step-3: On the Subscriptions page, click on the \"+New Subscription\" button.</p> <p>Step-4: On the \"Create a subscription\" page, select the type of subscription you want to create. This can be a \"Pay-As-You-Go\" subscription, a \"Free Trial\" subscription, or an \"Enterprise Agreement\" subscription.</p> <p>Step-5: Enter all the required details in the Basic and Advance tabs</p> <p>Step-6: Review all the details and finally click on Review + Create button</p> <p>After a few minutes, your new subscription will be created and will be listed on the Subscriptions page in the Azure portal.</p>"},{"location":"azure/4-azure-subscription/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/cost-management-billing/manage/direct-ea-administration?WT.mc_id=Portal-Microsoft_Azure_Ea#create-a-subscription</li> </ul>"},{"location":"azure/5-resource-providers/","title":"5 resource providers","text":""},{"location":"azure/5-resource-providers/#introduction","title":"Introduction","text":"<p>A resource provider in Azure is a service that supplies a specific type of resource, such as virtual machines, storage accounts, or virtual networks. When you create a new Azure subscription, certain resource providers are automatically registered, but others may need to be registered manually.</p> <p>azure resource providers registration can be done instantly before creating any the azure resource if it is missing but if you want to avoid the errors during Terraform plan and apply register upfront would help. sometime you may have to spend more time if the errors are misleading.  </p> <p>In this lab we will learn how to register resource providers in azure subscription manually and using PowerShell:</p>"},{"location":"azure/5-resource-providers/#register-manually","title":"Register manually","text":"<p>To view the registered resource providers for an Azure subscription, you can follow these steps:</p> <ol> <li> <p>Sign in to the Azure portal (https://portal.azure.com/) with your Azure account.</p> </li> <li> <p>In the top-left corner of the Azure portal, click on the \"Subscriptions\" link.</p> </li> <li> <p>Select the subscription that you want to view the registered resource providers for.</p> </li> <li> <p>On the subscription page, click on the \"Resource providers\" link on the left-hand menu.</p> </li> <li> <p>The \"Resource providers\" page will show a list of all the resource providers that are currently registered for the selected subscription, as well as their registration status.</p> </li> <li> <p>You can filter the list by typing the name of the resource provider in the search bar.</p> </li> </ol> <p>If you want to register a new resource provider for the subscription, click on the \"+ Register\" button on the top of the page, and then select the resource provider you want to register.</p> <p>This manual register takes lot of time especially if you've to do the same for multiple subscriptions for supporting different environment.</p>"},{"location":"azure/5-resource-providers/#register-using-az-cli-command","title":"Register using az CLI command","text":"<p>In this lab you will see how to register resource providers in azure subscription using az cli.</p> <p>log in to Azure account &amp; select the subscription</p> <pre><code>az login\n\naz account list\nor\naz account list --output table\n\naz account set -s \"anji.keesari\"\n\naz account show\nor\naz account show --output table\n</code></pre> <p>List of all resource providers</p> <p>Use this command to see all resource providers in Azure, and the registration status for your subscription.</p> <pre><code>az provider list --query \"[].{Provider:namespace, Status:registrationState}\" --out table\n</code></pre> <p>Sample Output</p> <pre><code>Provider                                   Status\n-----------------------------------------  -------------\nMicrosoft.AlertsManagement                 Registered\nMicrosoft.Cache                            Registered\nMicrosoft.Web                              Registered\nMicrosoft.ApiManagement                    Registered\nMicrosoft.Network                          Registered\nmicrosoft.insights                         Registered\nMicrosoft.ResourceHealth                   Registered\nMicrosoft.ContainerRegistry                Registered\nDynatrace.Observability                    NotRegistered\nMicrosoft.AAD                              NotRegistered\nmicrosoft.aadiam                           NotRegistered\nMicrosoft.Addons                           NotRegistered\nMicrosoft.ADHybridHealthService            Registered\nMicrosoft.AgFoodPlatform                   NotRegistered\nMicrosoft.AnalysisServices                 NotRegistered\nMicrosoft.AnyBuild                         NotRegistered\nMicrosoft.ApiSecurity                      NotRegistered\n</code></pre> <p>registered resource providers only</p> <p>Use this command to see registered resource providers only in Azure.</p> <pre><code>az provider list --query \"sort_by([?registrationState=='Registered'].{Provider:namespace, Status:registrationState}, &amp;Provider)\" --out table\n</code></pre> <p>Sample Output</p> <pre><code>Microsoft.ADHybridHealthService     Registered\nMicrosoft.AVS                       Registered\nMicrosoft.Advisor                   Registered\nMicrosoft.AlertsManagement          Registered\nMicrosoft.ApiManagement             Registered\n</code></pre>"},{"location":"azure/5-resource-providers/#register-single-provider-example","title":"Register single provider example","text":"<p>Get Resource Provider</p> <p><pre><code>Connect-AzAccount\nGet-AzResourceProvider -ProviderNamespace Microsoft.Addons\n</code></pre> Output</p> <pre><code>ProviderNamespace : Microsoft.Addons\nRegistrationState : NotRegistered\nResourceTypes     : {supportProviders}\nLocations         : {West Central US, South Central US, East US, West Europe}\n\nProviderNamespace : Microsoft.Addons\nRegistrationState : NotRegistered\nResourceTypes     : {operations}\nLocations         : {West Central US, South Central US, East US, West Europe}\n\nProviderNamespace : Microsoft.Addons\nRegistrationState : NotRegistered\nResourceTypes     : {operationResults}\nLocations         : {West Central US, South Central US, East US, West Europe}\n</code></pre> <p>Register Resource Provider</p> <p><pre><code>Register-AzResourceProvider -ProviderNamespace Microsoft.Addons\n</code></pre> Output <pre><code>ProviderNamespace : Microsoft.Addons\nRegistrationState : Registering\nResourceTypes     : {supportProviders, operations, operationResults}\nLocations         : {West Central US, South Central US, East US, West Europe}\n</code></pre> Status After register</p> <pre><code># command\nGet-AzResourceProvider -ProviderNamespace Microsoft.Addons\n\n# output\nProviderNamespace : Microsoft.Addons\nRegistrationState : Registered\nResourceTypes     : {supportProviders}\nLocations         : {West Central US, South Central US, East US, West Europe}\n\nProviderNamespace : Microsoft.Addons\nRegistrationState : Registered\nResourceTypes     : {operations}\nLocations         : {West Central US, South Central US, East US, West Europe}\n\nProviderNamespace : Microsoft.Addons\nRegistrationState : Registered\nResourceTypes     : {operationResults}\nLocations         : {West Central US, South Central US, East US, West Europe}\n</code></pre>"},{"location":"azure/5-resource-providers/#register-using-powershell","title":"Register using PowerShell","text":"<p>This PowerShell script helps you obtain a list of \u201cResource Providers\u201d from an existing subscription, and registers them to the new subscription.</p> <p>follow these instruction before running this script:</p> <ol> <li>Update source and target subscriptionId </li> <li>Uncomment #-WhatIf:$WhatIf for testing before actually registering.</li> <li>Once everything is fine then comment and run the script to register providers</li> </ol> resource-providers.ps1<pre><code># Register missing resource providers in a new subscription based on a list from an old subscription\n\nSet-AzContext -SubscriptionId '33dffc83-ffec-4773-b939-fc5be0d00a558' #old-subscription\n$ExistingResourceProvidersInOldSubscription = Get-AzResourceProvider\n\n\nSet-AzContext -SubscriptionId 'sfafed0f-0e40-43c6-8ccb-f5a5807211222' #new-subscription\n$ExistingResourceProvidersInNewSubscription = Get-AzResourceProvider\n\n\nforeach($ExistingResourceProviderInOldSubscription in $ExistingResourceProvidersInOldSubscription) {\n    if($ExistingResourceProviderInOldSubscription.RegistrationState -eq 'Registered'){\n        $providerfound = $false;\n        foreach($ExistingResourceProviderInNewSubscription in $ExistingResourceProvidersInNewSubscription){\n                if($ExistingResourceProviderInNewSubscription.ProviderNamespace -eq $ExistingResourceProviderInOldSubscription.ProviderNamespace){\n                    $providerfound = $true;\n                    if($ExistingResourceProviderInNewSubscription.RegistrationState -eq 'Registered'){\n                        # Write-Host '------------------------------'\n                        Write-Host $ExistingResourceProviderInOldSubscription.ProviderNamespace 'PRESENT, REGISTERED'\n                        # Write-Host '------------------------------'\n                    }\n                    else{\n                        Write-Host $ExistingResourceProviderInOldSubscription.ProviderNamespace 'PRESENT, UNREGISTERED, Registering'\n                        Register-AzResourceProvider -ProviderNamespace $ExistingResourceProviderInOldSubscription.ProviderNamespace #-WhatIf:$WhatIf\n                    }                        \n                }\n            }\n        if(-Not($providerfound)){\n            Write-Host $ExistingResourceProviderInOldSubscription.ProviderNamespace 'ABSENT, Registering'\n            Register-AzResourceProvider -ProviderNamespace $ExistingResourceProviderInOldSubscription.ProviderNamespace #-WhatIf:$WhatIf\n        }\n    }\n}\n</code></pre> <p>Output</p> <pre><code>PS C:\\Source\\Repos\\infrastructure-as-code&gt; .\\resource-providers.ps1\n.\n.\n.\n.\nMicrosoft.Diagnostics ABSENT, Registering\nWhat if: Performing the operation \"Registering provider ...\" on target \"Microsoft.Diagnostics\".\nMicrosoft.Advisor ABSENT, Registering\nWhat if: Performing the operation \"Registering provider ...\" on target \"Microsoft.Advisor\".\nMicrosoft.AAD ABSENT, Registering\nWhat if: Performing the operation \"Registering provider ...\" on target \"Microsoft.AAD\".\nMicrosoft.DevOps ABSENT, Registering\n</code></pre>"},{"location":"azure/6-tf-foundation-1/","title":"Setup Terraform Foundation Part-1","text":""},{"location":"azure/6-tf-foundation-1/#introduction","title":"Introduction","text":"<p>Welcome to Part 1 of the Terraform Foundation lab. In this lab, we will set up the <code>Terraform Management</code> environment, enabling you to create Azure resources using Terraform. As part of this setup, we will create a Azure Blob storage account for storing the Terraform state and an Azure Key Vault for securing the Terraform Secrets.</p> <p>The Terraform <code>Terraform Management</code> environment setup is a one-time activity that lays the foundation for automating infrastructure provisioning using Infrastructure as Code (IaC) principles. Once this setup is completed, you will be ready to create various Azure resources using Terraform in an automated and consistent manner.</p> <p>Let's get started with setting up the Terraform Management environment!</p>"},{"location":"azure/6-tf-foundation-1/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you play a critical role in managing cloud infrastructure efficiently and effectively. As part of your responsibilities, you have been tasked with setting up the Terraform Management environment, which will empower your team to provision and manage cloud resources using Terraform.</p> <p>By setting up the Terraform Management environment, you will provide your team with a centralized platform to automate the creation and management of cloud resources. This setup will streamline the deployment process, enhance collaboration, and improve the overall efficiency of your cloud infrastructure management.</p>"},{"location":"azure/6-tf-foundation-1/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Register resource providers in the subscription</li> <li>Visual studio code</li> <li>Download &amp; Install Git tools</li> <li>Basic knowledge on Terraform</li> </ul>"},{"location":"azure/6-tf-foundation-1/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish the following tasks and gain a deeper understanding of the Terraform Management setup:</p> <p>Azure DevOps setup</p> <ul> <li>Task-1: Create a new project in azure DevOps</li> <li>Task-2: Create a new Azure DevOps Repo for terraform</li> <li>Task-3: Clone a new git repo</li> <li>Task-4: Add .gitignore file in the git repo for terraform</li> </ul> <p>Terraform Management setup</p> <ul> <li>Task-5: Create terraform Service Principle </li> <li>Task-6: Create new azure resource group</li> <li>Task-7: Create new azure storage account &amp; container</li> <li>Task-8: Create new azure Key vault </li> <li>Task-9: Create secrets in azure  Key Vault </li> <li>Task-10: Setup Access Policy in Key Vault </li> <li>Task-11: Configure Service Principal Role Assignment </li> <li>Task-12: Create terraform management using PowerShell Script.</li> </ul>"},{"location":"azure/6-tf-foundation-1/#high-level-components","title":"High Level components","text":"<p>The following diagram illustrates the essential components used in the Terraform management setup in this lab.</p> <p></p>"},{"location":"azure/6-tf-foundation-1/#implementation-details","title":"Implementation details","text":"<p>In this lab, we will perform various tasks both in the Azure DevOps portal and the Azure portal to set up the Terraform Management. The following are the implementation details for each task:</p>"},{"location":"azure/6-tf-foundation-1/#task-1-create-a-new-project-in-azure-devops","title":"Task-1: Create a new project in azure DevOps","text":"<p>We will create a new project in Azure DevOps to organize our Terraform-related activities and workflows. This project will serve as a central hub for managing our infrastructure as code.</p> <p>It is always recommended to have separate project in azure DevOps for Infrastructure as Code maintenance. Here we are going to create new project called <code>IaC</code> and under this project we can create a new repo called <code>terraform</code> </p> <p>Here are the details of the new project:</p> <p>Project Name - <code>IaC</code></p> <p>Project Description - <code>This project contains the source code related to Infrastructure as code (IaC), IaC is a method of provisioning and managing infrastructure using code and automation rather than manual configuration.</code></p> <p>Follow these steps for creating a new project in azure DevOps.</p> <ol> <li>Sign in to your organization in Azure DevOps</li> <li>Select New project.</li> <li>Enter project name &amp; description</li> </ol> <p>Once the project is created then you can add more repos as needed.</p> <p>For example:</p> <ul> <li><code>terraform</code> - you can use this repo for terraform configuration</li> <li><code>scripts</code> - you can use this repo for managing all kinds of scripts like PowerShell, Bash and CLI etc..</li> <li><code>bicep</code> - you can use this repo for Microsoft Bicep scripts etc..</li> </ul> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-2-create-a-new-azure-devops-repo-for-terraform","title":"Task-2: Create a new Azure DevOps Repo for terraform","text":"<p>We will create a dedicated Git repository within Azure DevOps to store our Terraform configurations and scripts. This repository will enable version control and collaboration among team members.</p> <p>Here are the steps to create a new Git repository in Azure DevOps:</p> <ul> <li>Login into azure DevOps -  azure DevOps</li> <li>Select the project where we want to create the repo</li> <li>Click on <code>Repos</code> left nav link</li> <li>From the repo drop-down, select <code>New repository</code></li> <li>In the <code>Create a new repository</code> dialog, verify that Git is the repository type and enter a name for the new repository. </li> <li> <p>You can also add a README and create a <code>.gitignore</code> for the type of code you plan to manage in the repo.</p> </li> <li> <p>use lower case for the repos (best practice)</p> </li> <li>repo name - terraform </li> </ul> <p>for example: </p> <p></p> <ul> <li> <p>Initialize the main branch with a README or gitignore</p> </li> <li> <p>Create new feature branch (develop) from main</p> </li> </ul> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-3-clone-the-git-repo","title":"Task-3: Clone the git repo","text":"<p>We will clone the newly created Git repository to our local machine. This will allow us to work with the Terraform configurations locally and push changes to the remote repository.</p> <p>Here we will use the Git CMD tool, open the git cmd tool in admin mode and follow these commands to clone the code locally.</p> <pre><code>C:\\Users\\anji.keesari&gt;cd c:\\Source\\Repos\\IaC\n\nc:\\Source\\Repos\\IaC&gt;git clone https://keesari.visualstudio.com/IaC/_git/terraform\nCloning into 'terraform'...\nremote: Azure Repos\nremote: Found 4 objects to send. (26 ms)\nUnpacking objects: 100% (4/4), 1.22 KiB | 62.00 KiB/s, done.\n\nc:\\Source\\Repos\\IaC&gt;cd terraform\n\n# open the folder in VS code\nc:\\Source\\Repos\\IaC\\terraform&gt;code .\n\nc:\\Source\\Repos\\IaC\\terraform&gt;\n</code></pre>"},{"location":"azure/6-tf-foundation-1/#task-4-add-gitignore-file-in-the-git-repo-for-terraform","title":"Task-4: Add .gitignore file in the git repo for terraform","text":"<p>We will add a .gitignore file to the Git repository specifically tailored for Terraform. This file will exclude unnecessary files and directories from being tracked by Git, ensuring a cleaner and more manageable repository.</p> <p>This is the first file you need to add in the source code before commit any files.</p> <p>This .gitignore file will prevent Terraform state files, override files, local tfvars files, and CLI configuration files from being tracked by Git. These files contain sensitive information and can cause issues with consistency and conflicts if multiple users are working on the same Terraform project. It's important to not commit these files to your source code repository to maintain the integrity and security of your infrastructure code.</p> <p>Here is the sample file:</p> .gitignore<pre><code># Local .terraform directories\n**/.terraform/*\n\n# .tfstate files\n*.tfstate\n*.tfstate.*\n\n# Crash log files\ncrash.log\n\n# Exclude all .tfvars files, which are likely to contain sentitive data, such as\n# password, private keys, and other secrets. These should not be part of version \n# control as they are data points which are potentially sensitive and subject \n# to change depending on the environment.\n#\n*.tfvars\n\n# Ignore override files as they are usually used to override resources locally and so\n# are not checked in\noverride.tf\noverride.tf.json\n*_override.tf\n*_override.tf.json\n\n# Include override files you do wish to add to version control using negated pattern\n#\n# !example_override.tf\n\n# Include tfplan files to ignore the plan output of command: terraform plan -out=tfplan\n# example: *tfplan*\n\n# Ignore CLI configuration files\n.terraformrc\nterraform.rc\n</code></pre>"},{"location":"azure/6-tf-foundation-1/#terraform-management-setup","title":"Terraform Management Setup","text":"<p>The azure resources which are managing and securing the terraform state and securing the service principle are called <code>Terraform management resources</code></p> <p>By following these steps, you can set up a Terraform management environment for creating Azure resources. It's important to keep your configuration files and state file in a secure location, and to follow best practices for managing infrastructure as code.</p> <p>Here we are going to use Azure Storage Account for storing the terraform state files and Azure Key vault for securing the secrets used for running the terraform configuration in azure.</p> <p>Since these steps are part of terraform setup itself,  we can't create following resources using terraform, here we can use either az cli or PowerShell script to create following resources. I'll be showing the az cli and PowerShell script for creating these resources.</p> <p>We are going to create separate azure resource group for managing terraform management specific azure resource like azure storage account and azure key vault.</p> <p>We will start with service principe before creating azure storage account and azure key vault.</p>"},{"location":"azure/6-tf-foundation-1/#task-5-create-terraform-service-principle-credentials","title":"Task-5: Create Terraform Service Principle Credentials","text":"<p>A Service Principal is an identity that Terraform can use to authenticate and manage Azure resources.  Terraform uses a Service Principal to authenticate with Azure and manage Azure resources, such as virtual machines, storage accounts, and databases.</p> <p>Here's how you can create a Service Principal in Azure for use with Terraform:</p> <ul> <li>Log in to the Azure portal and go to the Azure Active Directory (AD) section.</li> <li>Click on \"App registrations\" and then click \"New registration\".</li> <li>Provide a name for the application and select \"Accounts in this organizational directory only\" as the supported account type.</li> <li>For the \"Redirect URI\" field, select \"Web\" and enter a valid URI. This URI can be any valid URI, but it must be accessible to the application.</li> <li>Click on \"Register\" to create the application registration. Once the application is created, note down the \"Application ID\" and \"Directory (tenant) ID\" values. These will be used in the Terraform configuration.</li> <li>Create a client secret by clicking on \"Certificates &amp; secrets\" and then \"New client secret\". Note down the client secret value that is generated. This value will be used in the Terraform configuration.</li> </ul> <p>Now that you have created a Service Principal in Azure, you can use it in Terraform to authenticate with Azure and manage Azure resources. In your Terraform configuration file, you can add the following code to authenticate with Azure using the Service Principal:</p> provider.tf<pre><code>provider \"azurerm\" {\n  subscription_id = \"SUBSCRIPTION_ID\"\n  client_id       = \"CLIENT_ID\"\n  client_secret   = \"CLIENT_SECRET\"\n  tenant_id       = \"TENANT_ID\"\n}\n</code></pre> <p>Replace SUBSCRIPTION_ID, CLIENT_ID, CLIENT_SECRET, and TENANT_ID with the values you obtained when creating the Service Principal in Azure. </p> <p>additional az cli commands may be helpful.</p> <pre><code>az login\naz ad app create --display-name &lt;APP_NAME&gt;\naz ad app create --display-name \"sp-tfmgmt-dev\"\naz ad app show --id &lt;APP_ID&gt; --query \"appId\"\naz account show --query \"tenantId\"\n</code></pre> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-6-create-new-resource-group","title":"Task-6: Create new resource group","text":"<p>To maintain the Terraform management resources in Azure, we will create a separate resource group specifically for this purpose.</p> <p>Resource Group Name - <code>rg-tfmgmt-dev</code> </p> <p>You can use the following Azure CLI command to create the resource group:</p> <pre><code>az group create --name &lt;GROUP_NAME&gt; --location &lt;LOCATION&gt;\n</code></pre> <p>For example:</p> <pre><code>az group create -n \"rg-tfmgmt-dev\"-l \"east us\"\n</code></pre> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-7-create-new-storage-account-container","title":"Task-7: Create new storage account &amp; container","text":"<p>To store the Terraform remote state, we will create an Azure Storage Account. This will allow us to store the state file remotely and access it from anywhere, while also providing additional features such as versioning, locking, and auditing.</p> <p>Terraform remote state is a way to store and manage Terraform's state files remotely. By default, Terraform stores the state file on the local file system, but this can cause problems when working in a team or when scaling up infrastructure. Remote state storage provides a centralized location for storing state files, which makes it easier to manage state across teams and across environments.</p> <p>Here is the command to create a new Azure Storage Account using the Azure CLI:</p> <pre><code>az storage account create --name &lt;ACCOUNT_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --location &lt;LOCATION&gt; --sku &lt;SKU&gt;\n</code></pre> <p>For example:</p> <p><pre><code>az storage account create -n \"tfmgmtstates\" -g \"rg-tfmgmt-dev\" -l \"east us\" --sku \"Standard_LRS\"\n</code></pre> </p> <p>We also need blob container for storing terraform state files, Let's create new storage account container using az cli here</p> <pre><code>az storage container create --name &lt;CONTAINER_NAME&gt; --account-name &lt;ACCOUNT_NAME&gt; --account-key &lt;ACCOUNT_KEY&gt;\naz storage container create -n \"terraformstates\" --account-name \"tfmgmtstates\" --account-key \"koB5PQEGX5pEHVAWsyM0efP3aeFsuNhw8dzRXvqrLXXcD12VEIC4HkhNnwDAGWUJcZWb8Q3C8yxZ+AStXGHDGQ==\"\n</code></pre> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-8-create-new-key-vault","title":"Task-8: Create new Key vault","text":"<p>This Key Vault will be used for storing all kind of secrets related terraform management. it is very critical securing secrets like terraform service principle because these are actually used to authenticate Terraform to Azure and create azure resources in the Azure Portal.</p> <p>create new azure key vault using az cli</p> <pre><code>az keyvault create --name &lt;VAULT_NAME&gt; --resource-group &lt;RESOURCE_GROUP_NAME&gt; --location &lt;LOCATION&gt;\naz keyvault create -n \"kv-tfstates-dev\" -g \"rg-tfmgmt-dev\" -l \"east us\"\n</code></pre> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-9-create-secrets-in-key-vault","title":"Task-9: Create secrets in Key Vault","text":"<p>Terraform management secrets will be protected by storing in the azure key vault.</p> <p>Here are the commands to create and store secrets in the Azure Key Vault using the Azure CLI:</p> <pre><code>az keyvault secret set --vault-name &lt;VAULT_NAME&gt; --name &lt;SECRET_NAME&gt; --value &lt;SECRET_VALUE&gt;\n</code></pre> <p>For example:</p> <pre><code>az keyvault secret set --vault-name \"kv-tfstates-dev\" --name \"tf-subscription-id\" --value \"1115d52c-5170-4366-b262-cc12cba2d222\"\naz keyvault secret set --vault-name \"kv-tfstates-dev\" --name \"tf-client-id\" --value \"11183cf3-7184-457a-a71b-eb5fb7e02222\"\naz keyvault secret set --vault-name \"kv-tfstates-dev\" --name \"tf-client-secret\" --value \"1118Q~eQTMTWEwpvK~CHeTIrU7l7xnhw9wE1z222\"\naz keyvault secret set --vault-name \"kv-tfstates-dev\" --name \"tf-tenant-id\" --value \"1113c4a0-f87d-46ad-b4be-3ee05cefe222\"\naz keyvault secret set --vault-name \"kv-tfstates-dev\" --name \"tf-access-key\" --value \"1115PQEGX5pEHVAWsyM0efP3aeFsuNhw8dzRXvqrLXXcD12VEIC4HkhNnwDAGWUJcZWb8Q3C8yxZ+AStXGHDG222\"\n</code></pre> <p></p> <p>Make sure to store sensitive information, such as passwords or access keys, as secrets in the Azure Key Vault to maintain security and compliance.</p>"},{"location":"azure/6-tf-foundation-1/#task-10-setup-access-policy-in-key-vault","title":"Task-10: Setup Access Policy in Key Vault","text":"<p>To grant access to the Terraform service principal to retrieve the secrets stored in the Azure Key Vault, we need to set up an access policy in the Key Vault.</p> <p>Use the following command to set up the access policy using the Azure CLI:</p> <pre><code>az keyvault set-policy --name &lt;VAULT_NAME&gt; --object-id &lt;OBJECT_ID&gt; --secret-permissions &lt;SECRET_PERMISSIONS&gt; --key-permissions &lt;KEY_PERMISSIONS&gt;\n</code></pre> <p>For example:</p> <pre><code>az keyvault set-policy --name \"kv-tfstates-dev\" --object-id \"a68e4529-b584-43c6-9ffd-4ca681da9efc\" --secret-permissions get list --key-permissions get list\n</code></pre> <p></p>"},{"location":"azure/6-tf-foundation-1/#task-11-configure-service-principal-role-assignment","title":"Task-11: Configure Service Principal Role Assignment","text":"<p>The new Service principle needs at least contributor access at subscription level where azure resources will be created,  if you want avoid unnecessary issues during resources creation or azure DevOps terraform automation you can even provide owner role at subscription level so that we don't run any issues while creating azure resource from terraform configuration.</p> <p>Use the following command to assign the role to the service principal using the Azure CLI:</p> <pre><code>az role assignment create --assignee-object-id &lt;SERVICE_PRINCIPAL_OBJECT_ID&gt; --role Owner --scope &lt;SCOPE&gt;\n</code></pre> <p>For example:</p> <pre><code>az role assignment create --assignee-object-id \"a68e4529-b584-43c6-9ffd-4ca681da9efc\" --role Owner --scope \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be\" \n</code></pre> <p>Note:- make sure that you will get the service principle object id here instead of app registration object id service principle object id will be found in <code>Enterprise Application</code></p> <p></p> <p>That it! now you've setup terraform management for running your terraform configuration.</p>"},{"location":"azure/6-tf-foundation-1/#create-terraform-management-using-powershell-script","title":"Create terraform management using PowerShell Script.","text":"<p>Alternately you can also use the re-usable PowerShell script to do above steps which is more efficient way:</p> <p>Here is the re-usable and re-runnable PowerShell function to create terraform management resources; this function will allow you to run in multiple environments.</p> <p>Let's look at the complete PowerShell script file together.</p> tf-mgmt.ps1<pre><code>&lt;#\n.SYNOPSIS\n    Configures Azure for secure Terraform access.\n.DESCRIPTION\n    Configures Azure for secure Terraform access using Azure Key Vault.\n    The following steps are automated:\n    - Creates an Azure Service Principle for Terraform.\n    - Creates a new Resource Group.\n    - Creates a new Storage Account.\n    - Creates a new Storage Container.\n    - Creates a new Key Vault.\n    - Configures Key Vault Access Policies.\n    - Creates Key Vault Secrets for these sensitive Terraform login details:\n       - 'tf-subscription-id'\n       - 'tf-client-id'\n       - 'tf-client-secret'\n       - 'tf-tenant-id'   \n       - 'tf-access-key' \n.EXAMPLE\n    Connect-AzAccount -UseDeviceAuthentication\n    .\\scripts\\TerraformManagement.ps1 -adminUserDisplayName 'AnjKeesari@gmail.com'\n    Displays device login link, then configures secure Terraform access for admin user 'AnjKeesari@gmail.com'\n.NOTES\n    Assumptions:\n    - Azure PowerShell module is installed: https://docs.microsoft.com/en-us/powershell/azure/install-az-ps\n    - You are already logged into Azure before running this script (eg. Connect-AzAccount)\n    - Use \"Connect-AzAccount -UseDeviceAuthentication\" if browser prompts don't work.\n    - select-AzSubscription -SubscriptionName 'Dev'\n#&gt;\n\n[CmdletBinding()]\nparam (    \n    $adminUserDisplayName = \"anjkeesari@gmail.com\",# This is used to assign yourself access to KeyVault\n    $servicePrincipalName = \"sp-tfmgmt-dev\",\n    $resourceGroupName = \"rg-tfmgmt-dev\",\n    $location = \"East US\",\n    $storageAccountSku = \"Standard_LRS\",\n    $storageContainerName = \"tfmgmttates\",\n    $vaultName = \"kv-tfstates-dev\",\n    $storageAccountName = \"sttfstatesdev\",\n    $subscriptionID = \"111111-ffec-4773-b939-fc5be0d00222\"\n)\n\n# Azure login\nWrite-Host \"Checking for an active Azure login...\"\n\n$azContext = Get-AzContext\n\nif (-not $azContext) {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw \"There is no active login for Azure. Please login first using 'Connect-AzAccount'\"\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\n\n# Service Principle\nWrite-Host \"Checking for an active Service Principle: [$servicePrincipalName]...\"\n\n# Get current context\n$terraformSP = Get-AzADServicePrincipal -DisplayName $servicePrincipalName\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\nif (-not $terraformSP) {\n    Write-Host \"Creating a Terraform Service Principle: [$servicePrincipalName] ...\"\n    try {\n        $terraformSP = New-AzADServicePrincipal -DisplayName $servicePrincipalName -Role 'Contributor' -ErrorAction 'Stop'\n        $servicePrinciplePassword = $newSpCredential.secretText\n    } catch {\n        Write-Host \"ERROR!\" -ForegroundColor 'Red'\n        throw $_\n    }\n    Write-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\n} else {\n    # Service Principle exists so renew password (as cannot retrieve current one-off password)\n    $newSpCredential = $terraformSP | New-AzADSpCredential\n    $servicePrinciplePassword = $newSpCredential.secretText\n}\n\n# Get Subscription\nWrite-Host \"`nFinding Subscription and Tenant details...\"\ntry {\n    $subscription = Get-AzSubscription -SubscriptionID $subscriptionID -ErrorAction 'Stop'\n} catch {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw $_\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\n# New Resource Group\nif(Get-AzResourceGroup -Name $resourceGroupName -ErrorAction SilentlyContinue)\n{  \n    Write-Host -ForegroundColor Magenta $resourceGroupName \"- Terraform Management Resource Group already exists.\"  \n}  \nelse  \n{  \n    Write-Host \"`nCreating Terraform Management Resource Group: [$resourceGroupName]...\"\n    try {\n        $azResourceGroupParams = @{\n            Name        = $resourceGroupName\n            Location    = $location\n            Tag         = @{ keep = \"true\" }\n            Force       = $true\n            ErrorAction = 'Stop'\n            Verbose     = $VerbosePreference\n        }\n        New-AzResourceGroup @azResourceGroupParams | Out-String | Write-Verbose\n    } catch {\n        Write-Host \"ERROR!\" -ForegroundColor 'Red'\n        throw $_\n    }\n    Write-Host \"SUCCESS!\" -ForegroundColor 'Green'\n}\n\n\n# New storage account\n if(Get-AzStorageAccount -ResourceGroupName $resourceGroupName -Name $storageAccountName -ErrorAction SilentlyContinue)  \n {  \n      Write-Host -ForegroundColor Magenta $storageAccountName \"- Storage Account for terraform states already exists.\"     \n }  \n else  \n {  \n    Write-Host \"`nCreating Storage Account for terraform states: [$storageAccountName]...\"\n    try {\n        $azStorageAccountParams = @{\n            ResourceGroupName = $resourceGroupName\n            Location          = $location\n            Name              = $storageAccountName\n            SkuName           = $storageAccountSku\n            Kind              = 'StorageV2'\n            ErrorAction       = 'Stop'\n            Verbose           = $VerbosePreference\n        }\n        New-AzStorageAccount @azStorageAccountParams | Out-String | Write-Verbose\n    } catch {\n        Write-Host \"ERROR!\" -ForegroundColor 'Red'\n        throw $_\n    }\n    Write-Host \"SUCCESS!\" -ForegroundColor 'Green'    \n}\n\n# Select Storage Container\nWrite-Host \"`nSelecting Default Storage Account...\"\ntry {\n    $azCurrentStorageAccountParams = @{\n        ResourceGroupName = $resourceGroupName\n        AccountName       = $storageAccountName\n        ErrorAction       = 'Stop'\n        Verbose           = $VerbosePreference\n    }\n    Set-AzCurrentStorageAccount @azCurrentStorageAccountParams | Out-String | Write-Verbose\n} catch {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw $_\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\n\n# # New Storage Container\n# if(Get-AzStorageContainer -Name $storageContainerName -ErrorAction SilentlyContinue)  \n# {  \n#     Write-Host -ForegroundColor Magenta $storageContainerName \"- Storage Container already exists.\"  \n# }  \n# else  \n# {\n#     Write-Host \"`nCreating Storage Container: [$storageContainerName]...\"\n#     try {\n#         $azStorageContainerParams = @{\n#             Name        = $storageContainerName\n#             Permission  = 'Off'\n#             # ErrorAction = 'Stop'\n#             Verbose     = $VerbosePreference\n#         }\n\n#         $storageAccountKey = (Get-AzStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName)[0].Value\n#         $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey\n\n#         New-AzStorageContainer @azStorageContainerParams -Context $storageContext | Out-String | Write-Verbose\n#     } catch {\n#         Write-Host \"ERROR!\" -ForegroundColor 'Red'\n#         throw $_\n#     }\n#     Write-Host \"SUCCESS!\" -ForegroundColor 'Green'\n# }\n\n# New KeyVault\n\nWrite-Host \"`nCreating Key Vault for terraform secrets: [$vaultName]...\"\nif(Get-AzKeyVault -Name $vaultName -ErrorAction SilentlyContinue)  \n{  \n     Write-Host -ForegroundColor Magenta $vaultName \"- Key Vault already exists.\"  \n}  \nelse  \n{\n    try {\n\n        Register-AzResourceProvider -ProviderNamespace \"Microsoft.KeyVault\"\n        $azKeyVaultParams = @{\n            VaultName         = $vaultName\n            ResourceGroupName = $resourceGroupName\n            Location          = $location\n            ErrorAction       = 'Stop'\n            Verbose           = $VerbosePreference\n        }\n        New-AzKeyVault @azKeyVaultParams | Out-String | Write-Verbose\n    } catch {\n        Write-Host \"ERROR!\" -ForegroundColor 'Red'\n        throw $_\n    }\n    Write-Host \"SUCCESS!\" -ForegroundColor 'Green'\n}\n\n# Set KeyVault Access Policy\nWrite-Host \"`nSetting KeyVault Access Policy for Admin User: [$adminUserDisplayName]...\"\n$adminADUser = Get-AzADUser -DisplayName $adminUserDisplayName\nWrite-Host \"adminADUser = ${adminADUser}\" -ForegroundColor 'Green'\ntry {\n    $azKeyVaultAccessPolicyParams = @{\n        VaultName                 = $vaultName\n        ResourceGroupName         = $resourceGroupName\n        UserPrincipalName         = $adminUserDisplayName\n        PermissionsToKeys         = @('Get', 'List')\n        PermissionsToSecrets      = @('Get', 'List', 'Set')\n        PermissionsToCertificates = @('Get', 'List')\n        ErrorAction               = 'Stop'\n        Verbose                   = $VerbosePreference\n    }\n    Set-AzKeyVaultAccessPolicy @azKeyVaultAccessPolicyParams -PassThru | Out-String | Write-Verbose\n} catch {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw $_\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\nWrite-Host \"`nSetting KeyVault Access Policy for Terraform SP: [$servicePrincipalName]...\"\ntry {\n    $azKeyVaultAccessPolicyParams = @{\n        VaultName                 = $vaultName\n        ResourceGroupName         = $resourceGroupName\n        ObjectId                  = $terraformSP.Id\n        PermissionsToKeys         = @('Get', 'List')\n        PermissionsToSecrets      = @('Get', 'List', 'Set')\n        PermissionsToCertificates = @('Get', 'List')\n        ErrorAction               = 'Stop'\n        Verbose                   = $VerbosePreference\n    }\n    Set-AzKeyVaultAccessPolicy @azKeyVaultAccessPolicyParams | Out-String | Write-Verbose\n} catch {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw $_\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n\n\n# Terraform login variables\n# Get Storage Access Key\n$storageAccessKeys = Get-AzStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName\n$storageAccessKey = $storageAccessKeys[0].Value # only need one of the keys\n\n$terraformLoginVars = @{\n    'tf-subscription-id' = $subscription.Id\n    'tf-client-id'       = $terraformSP.appId\n    'tf-client-secret'   = $servicePrinciplePassword\n    'tf-tenant-id'       = $subscription.TenantId\n    'tf-access-key'      = $storageAccessKey\n}\nWrite-Host \"`nTerraform login details:\"\n$terraformLoginVars | Out-String | Write-Host\n\n# Create KeyVault Secrets\nWrite-Host \"`nCreating KeyVault Secrets for Terraform...\"\ntry {\n    foreach ($terraformLoginVar in $terraformLoginVars.GetEnumerator()) {\n        $AzKeyVaultSecretParams = @{\n            VaultName   = $vaultName\n            Name        = $terraformLoginVar.Key\n            SecretValue = (ConvertTo-SecureString -String $terraformLoginVar.Value -AsPlainText -Force)\n            ErrorAction = 'Stop'\n            Verbose     = $VerbosePreference\n        }\n        Set-AzKeyVaultSecret @AzKeyVaultSecretParams | Out-String | Write-Verbose\n    }\n} catch {\n    Write-Host \"ERROR!\" -ForegroundColor 'Red'\n    throw $_\n}\nWrite-Host \"SUCCESS!\" -ForegroundColor 'Green'\n</code></pre>"},{"location":"azure/6-tf-foundation-1/#verify-azure-resources","title":"Verify Azure resources","text":"<p>The final step in terraform management is to make sure that all the azure resources are created as expected, let's quickly review them manually. </p> <p></p>"},{"location":"azure/6-tf-foundation-1/#references","title":"References","text":"<ul> <li>Microsoft MSDN - Create a project in Azure DevOps</li> <li>Microsoft MSDN - Create a new Git repo in your project</li> <li>Microsoft MSDN - Tutorial: Register an app with Microsoft Entra ID</li> </ul>"},{"location":"azure/6-tf-foundation-2/","title":"Setup Terraform Foundation Part-2","text":""},{"location":"azure/6-tf-foundation-2/#introduction","title":"Introduction","text":"<p>This is part-2 of the terraform foundation, in this module we will setup the terraform project and folder structure for running any kind of terraform configuration. this folder structure and terraform source code created as part of this lab is the minimum requirements for creating any kind of azure resources using terraform.</p>"},{"location":"azure/6-tf-foundation-2/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you have been asked to start working on infrastructure as code (IaC) setup using terraform for your organization so that you can create any kind of azure cloud resources using the source code written in terraform language. As part of the terraform project structure we will be creating our first resource group so that we can test the basic terraform setup or files created in this module.</p> <p>we will start with bare minimum terraform configuration in this module and update the same files while creating more and more resources in the future modules.</p>"},{"location":"azure/6-tf-foundation-2/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Visual studio code</li> <li>Azure DevOps project &amp; repo</li> <li>Terraform Management setup (part-1)</li> </ul>"},{"location":"azure/6-tf-foundation-2/#objective","title":"Objective","text":"<p>The objective of this lab is to accomplish the following tasks in order to set up the Terraform environment and successfully provision Azure resources:</p> <ul> <li>Task-1: Create terraform environment variables</li> <li>Task-2: Create terraform providers</li> <li>Task-3: Configure terraform backend state</li> <li>Task-4: Create terraform variables</li> <li>Task-5: Create locals file</li> <li>Task-6: Create azure resource group</li> <li>Task-7: Store Terraform commands</li> <li>Task-8: Initialize Terraform</li> <li>Task-9: Setup Terraform workspace</li> <li>Task-10: Create a Terraform execution plan</li> <li>Task-11: Apply a Terraform execution plan</li> <li>Task-12: Verify the results</li> <li>Task-12: Verify terraform statefile</li> </ul>"},{"location":"azure/6-tf-foundation-2/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram illustrates the essential components used to run Terraform configuration setup in this lab.</p> <p></p>"},{"location":"azure/6-tf-foundation-2/#implementation-details","title":"Implementation details","text":"<p>Before proceeding with the tasks, make sure to clone the Terraform Git repository from the Azure DevOps portal. This repository should have been created in the previous module. Once the repository is cloned, open the VS Code text editor and navigate to the Terraform folder to access the Terraform configuration files.</p> <p>Here are the step-by-step instructions to perform each task:</p>"},{"location":"azure/6-tf-foundation-2/#task-1-create-terraform-environment-variables","title":"Task-1: Create terraform environment variables","text":"<p>Terraform allows you to use environment variables to define variables that are used in your Terraform configuration. Environment variables are a convenient way to pass values to Terraform, without having to hard-code them in your configuration files. </p> <p>By using environment variables in Terraform, you can more easily customize your Terraform configuration based on your environment, without having to modify your configuration files directly. This can be useful when working with multiple environments, such as development, testing, and production.</p> <p>Let's create new folder called <code>environments</code> and add following files in it. here we are creating *.tfvar files for each environment, these *.tfvar files will contains environment specific values in it.</p> <p>We are going to use these files for controlling your environment specific setting in future labs;</p> <p>let's start with adding service principle credentials in environment variables and later use these files for adding more environment specific variable as and when needed while creating new azure resources.</p> <p>environments folder structure will look like below: <pre><code>environments\n  |_ dev-variables.tfvar \n  |_ test-variables.tfvar \n  |_ prod-variables.tfvar \n</code></pre></p> <p>Important</p> <p>Replace these values with your environment specific values</p> dev-variables.tf<pre><code>sp-subscription-id = \"test-6c89-4044-8a23-test\"\nsp-client-id = \"test-645b-48f5-b586-test\"\nsp-client-secret = \"test~oFskcGH6bno8kS~tet\"\nsp-tenant-id = \"test-f87d-46ad-b4be-test\"\n</code></pre> test-variables.tf<pre><code>sp-subscription-id = \"value will be replaced by key vault value\"\nsp-client-id = \"value will be replaced by key vault value\"\nsp-client-secret = \"value will be replaced by key vault value\"\nsp-tenant-id = \"value will be replaced by key vault value\"\n</code></pre> prod-variables.tf<pre><code>sp-subscription-id = \"value will be replaced by key vault value\"\nsp-client-id = \"value will be replaced by key vault value\"\nsp-client-secret = \"value will be replaced by key vault value\"\nsp-tenant-id = \"value will be replaced by key vault value\"\n</code></pre> <p>Best-practice</p> <p>We are going to use lowercase for all the files and folders in the terraform. </p>"},{"location":"azure/6-tf-foundation-2/#task-2-create-terraform-providers","title":"Task-2: Create terraform providers","text":"<p>Terraform providers are plugins that allow Terraform to interact with different resources in different infrastructure providers (Azure, AWS, GCP) Here are the steps to create a provider in Terraform:</p> <p>The <code>azurerm</code> Terraform Provider allows managing resources within Azure Resource Manager. </p> <p>Here we will create a new file called <code>provider.tf</code> and register all the required providers for terraform to create required resources.</p> provider.tf<pre><code>terraform {\n\n  required_version = \"&gt;=0.12\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;3.31.0\" //\"~&gt;2.0\"\n    }\n\n    azuread = {\n      version = \"&gt;= 2.26.0\" // https://github.com/terraform-providers/terraform-provider-azuread/releases\n    }\n  }\n}\n\nprovider \"random\" {}\n\nprovider \"azurerm\" {\n  features {}\n  skip_provider_registration = true\n  subscription_id            = var.sp-subscription-id\n  client_id                  = var.sp-client-id\n  client_secret              = var.sp-client-secret\n  tenant_id                  = var.sp-tenant-id\n}\n</code></pre> <p>Note</p> <p>It's important to keep your provider versions up to date and ensure compatibility with your other Terraform modules and configurations.</p>"},{"location":"azure/6-tf-foundation-2/#task-3-configure-terraform-backend-state","title":"Task-3: Configure terraform backend state","text":"<p>By default, Terraform state is stored locally, which isn't ideal, rather we should be storing using remote state which is in azure storage account so that multiple team members can work on the terraform project. here we will configure the terraform backend for remote state.</p> <p>use the azure storage account created in part-1 of the terraform foundation and configure terraform backed state.</p> <p>To configure the backend state, you need the following information:</p> <ul> <li><code>resource_group_name</code>: The name of the azure resource group where Storage account is created.</li> <li><code>storage_account_name</code>: The name of the Azure Storage account.</li> <li><code>container_name</code>: The name of the blob container.</li> <li><code>key</code>: The name of the state store file to be created.</li> </ul> <p>Create a new file called <code>backend.tf</code> in your Terraform project directory.</p> <p>In the <code>backend.tf</code> file, add the following configuration to define the Azure Storage Account as the backend:</p> backend.tf<pre><code>terraform {\n  backend \"azurerm\" {\n    resource_group_name  = \"rg-tf-mgmt-project1\"\n    storage_account_name = \"sttfstatesdev\"\n    container_name       = \"terraformstates\"\n    key                  = \"project1-state-\"\n  }\n}\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-4-create-terraform-variables","title":"Task-4: Create terraform variables","text":"<p>Terraform variables allow you to define reusable values in your Terraform configuration, which can be used to parameterize your Terraform code.</p> <p>In terraform, variables are divided into inputs and outputs. let's create separate file for each one of them here.</p> <p>input variables</p> <p><code>variables.tf</code> - use this file for input variables</p> variables.tf<pre><code># Service Principal\nvariable \"sp-subscription-id\" {\n  description = \"Id of the azure subscription where all resources will be created\"\n  type        = string\n}\nvariable \"sp-client-id\" {\n  description = \"Client Id of A Service Principal or Azure Active Directory application registration used for provisioning azure resources.\"\n  type        = string\n}\nvariable \"sp-client-secret\" {\n  description = \"Secret of A Service Principal or Azure Active Directory application registration used for provisioning azure resources.\"\n  type        = string\n}\nvariable \"sp-tenant-id\" {\n  description = \"Tenant Id of the azure account.\"\n  type        = string\n}\n# Azure resources\nvariable \"rg_name\" {\n  description = \"Name of the main resource rroup name for project-1\"\n  type        = string\n}\n\nvariable \"location\" {\n  description = \"Specifies the location for the resource group and all the resources\"\n  type        = string\n  default     = \"East US\"\n}\n\nvariable \"default_tags\" {\n  type = map(any)\n  default = {\n    \"Project\"   = \"Project-1\"\n    \"Owner\"     = \"Anji.Keesari\"\n    \"CreatedBy\" = \"Anji.Keesari\"\n  }\n}\n</code></pre> <p>output variables </p> <p><code>output.tf</code> - use this files for output variables</p> output.tf<pre><code>output \"resource_group_name\" {\n  value = azurerm_resource_group.rg.name\n}\n</code></pre> <p>variables prefix</p> <p>I am also going to create one more file for azure resource names naming conventions or standards across my organization. This file contains prefix of all the frequently used azure resource names as per the standards.</p> <p>Let's begin with resource group prefix and keep updating this files in the future labs as per the new resource added.</p> <p><code>variables_prefix.tf</code></p> variables_prefix.tf<pre><code>variable \"rg_prefix\" {\n  type        = string\n  default     = \"rg\"\n  description = \"Prefix of the resource group name that's combined with name of the resource group.\"\n}\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-5-create-locals-file","title":"Task-5: Create locals file","text":"<p>In Terraform, <code>locals</code> is a block used to define values that are derived from other values in your configuration. These values are typically used for convenience or to improve readability of your configuration.</p> <p>This is optional but it is recommended to keep all the locals in single files, we are going to use this file in the future labs.</p> <p>Use this file for storing all the locals in one place together for entire terraform project.</p> locals.tf<pre><code>locals {\n  default_tags = merge(var.default_tags, { \"Environment\" = \"${terraform.workspace}\" })\n  environment  = terraform.workspace != \"default\" ? terraform.workspace : \"\"\n}\n</code></pre> <p>Note: - don't worry about <code>terraform.workspace</code> will talk more on this in the future labs.</p>"},{"location":"azure/6-tf-foundation-2/#task-6-create-azure-resource-group","title":"Task-6: Create azure resource group","text":"<p>Finally let's create one sample azure resource group to make sure that all the terraform configuration we created above is actually working as expected.</p> resource_group.tf<pre><code># Create the resource group\nresource \"azurerm_resource_group\" \"rg\" {\n  name     = lower(\"${var.rg_prefix}-${var.rg_name}-${local.environment}\")\n  location = var.location\n  tags = merge(local.default_tags,\n    {\n      \"CreatedBy\" = \"Anji.Keesari\"\n  })\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-7-store-terraform-commands","title":"Task-7: Store terraform commands","text":"<p>Create a separate file for storing all terraform commands here, so that anyone working on this project can use them for running commands quickly.</p> <p>This task is optional, but it will be really helpful especially new team members working on the same project.</p> tf_commands.ps1<pre><code># azure login related\naz login\n\naz account list --output table\n\naz account set -s \"anji.keesari\"\n\naz account show --output table\n\n# terraform related\nterraform init\n# terraform init -reconfigure\n\nterraform validate\nterraform fmt\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n\nterraform state list\n\n# workspaces related\nterraform workspace list\n\nterraform workspace new dev\n\nterraform workspace select dev\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-8-initialize-terraform","title":"Task-8: Initialize Terraform","text":"<p>Run <code>terraform init</code> to initialize the Terraform deployment. This command downloads the Azure modules required to manage your Azure resources.</p> <p>terraform init</p> <pre><code>Initializing the backend...\n\nInitializing provider plugins...\n- Finding latest version of hashicorp/azurerm...\n- Installing hashicorp/azurerm v3.40.0...\n- Installed hashicorp/azurerm v3.40.0 (signed by HashiCorp)\n\nTerraform has created a lock file .terraform.lock.hcl to record the provider\nselections it made above. Include this file in your version control repository\nany changes that are required for your infrastructure. All Terraform commands\nshould now work.\n\nIf you ever set or change modules or backend configuration for Terraform,\nrerun this command to reinitialize your working directory. If you forget, other\ncommands will detect it and remind you to do so if necessary.\n</code></pre> <p>You will notice this new folder created when you run <code>terraform init</code> command executed</p> <p></p> <p>Also see the folder structure of the terraform project.</p> <p>Important</p> <p>Verify to make sure that new state file got created in azure storage account blob container </p> <p>terraform validate</p> <p>validate terraform code for syntax</p> <pre><code>Success! The configuration is valid.\n</code></pre> <p>terraform fmt</p> <p>Formats terraform source code as per HCL canonical standard</p>"},{"location":"azure/6-tf-foundation-2/#task-9-setup-terraform-workspace","title":"Task-9: Setup Terraform workspace","text":"<p>Terraform workspace is a feature in Terraform that allows you to manage multiple instances of the same Terraform configuration in a single Terraform state. Each workspace is effectively a separate and isolated environment, with its own state and variables. Here are the steps to create and manage workspaces in Terraform:</p> <p>default workspace</p> <p><pre><code>Terraform workspace\n</code></pre> output</p> <p><pre><code>* default\n</code></pre> Create a new workspace</p> <pre><code>terraform workspace new dev\n</code></pre> <p><pre><code>Created and switched to workspace \"dev\"!\n\nYou're now on a new, empty workspace. Workspaces isolate their state,\nso if you run \"terraform plan\" Terraform will not see any existing state\nfor this configuration.\n</code></pre> <pre><code>terraform workspace list   \n</code></pre> output</p> <p><pre><code>  default\n* dev\n</code></pre> Use this to switch between workspaces</p> <pre><code>terraform workspace select dev\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-10-create-a-terraform-execution-plan","title":"Task-10: Create a Terraform execution plan","text":"<p>Run <code>terraform plan</code> to create an execution plan.</p> <p>Here are the key points to keep in mind when running terraform plan:</p> <ul> <li>The terraform plan command creates an execution plan by evaluating your Terraform configuration files. It determines what actions are necessary to achieve the desired state specified in your configuration.</li> <li>The plan provides a detailed summary of the changes that Terraform will make to your infrastructure. It shows additions, modifications, and deletions of resources, as well as any dependencies or potential issues.</li> <li>Running terraform plan allows you to review and verify the execution plan before applying any changes to the actual resources. This step is crucial for understanding the impact of the changes and ensuring they align with your expectations.</li> <li>By default, the execution plan is displayed in the terminal. However, you can also save the plan to a file using the optional -out parameter. This ensures that the plan you reviewed is exactly what will be applied when you proceed with the terraform apply command.</li> </ul> <p>Terraform plan</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output</p> <pre><code>Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # azurerm_resource_group.rg will be created\n  + resource \"azurerm_resource_group\" \"rg\" {\n      + id       = (known after apply)\n      + location = \"northcentralus\"\n      + name     = \"rg-resourcegroup1-dev\"\n      + tags     = {\n          + \"CreatedBy\"   = \"Anji.Keesari\"\n          + \"Environment\" = \"dev\"\n          + \"Owner\"       = \"Anji.Keesari\"\n          + \"Project\"     = \"Project-1\"\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + resource_group_name = \"rg-resourcegroup1-dev\"\n\nSaved the plan to: dev-plan\n\nTo perform exactly these actions, run the following command to apply:\n    terraform apply \"dev-plan\"\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-11-apply-a-terraform-execution-plan","title":"Task-11: Apply a Terraform execution plan","text":"<p>Run <code>terraform apply</code> to apply the execution plan to your cloud infrastructure.</p> <p>Terraform apply</p> <pre><code>terraform apply dev-plan\n</code></pre> <p>output <pre><code>azurerm_resource_group.rg: Creating...\nazurerm_resource_group.rg: Creation complete after 2s [id=/subscriptions/test-5170-4366-b262-test/resourceGroups/rg-resourcegroup1-dev]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nresource_group_name = \"rg-resourcegroup1-dev\"\n</code></pre></p>"},{"location":"azure/6-tf-foundation-2/#task-12-verify-the-results","title":"Task-12: Verify the results","text":"<p>Login into azure portal and validate azure resources created from terraform configuration.</p> <p>Finally commit terraform source code to git repo before start next module.</p> <p></p> <pre><code>git add .\ngit commit -am \"initial setup\"\ngit push --set-upstream origin develop\n</code></pre>"},{"location":"azure/6-tf-foundation-2/#task-13-verify-terraform-state-file","title":"Task-13: Verify terraform state file.","text":"<p>Azure storage account container should have two files created and stored the terraform state file in JSON format.</p> <p></p> <p>We are fully ready with terraform development for creating any azure resources using this foundation, we are going to learn more about terraform configuration while creating new azure resources in the future labs.</p>"},{"location":"azure/6-tf-foundation-2/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/developer/terraform/create-resource-group?source=recommendations&amp;tabs=azure-cli</li> <li>https://learn.microsoft.com/en-us/azure/developer/terraform/store-state-in-azure-storage?tabs=azure-cli</li> </ul>"},{"location":"azure/7-log-analytics-workspace/","title":"Create Log Analytics Workspace using terraform","text":""},{"location":"azure/7-log-analytics-workspace/#introduction","title":"Introduction","text":"<p>A Log Analytics workspace allows you to log data from Azure Monitor and other Azure services.</p> <p>In this lab, I will guide you through the process of creating a Log Analytics workspace using Terraform and demonstrate how to verify its successful creation in the Azure portal.</p>"},{"location":"azure/7-log-analytics-workspace/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you have been asked to collect all the monitoring data, azure resources logs from azure services for your organization so that you can use the log queries to retrieve and analyze data from a Log Analytics workspace.</p>"},{"location":"azure/7-log-analytics-workspace/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Visual studio code</li> <li>Azure DevOps Project &amp; repo</li> <li>Terraform Foundation Setup</li> </ul>"},{"location":"azure/7-log-analytics-workspace/#architecture-diagram","title":"Architecture diagram","text":"<p>The following diagram shows Log Analytics Workspace high level components used in this lab.</p> <p></p>"},{"location":"azure/7-log-analytics-workspace/#implementation-details","title":"Implementation Details","text":"<p>In this exercise we will accomplish &amp; learn following:</p> <ul> <li>Task-1: Configure terraform variables for Log Analytics workspace </li> <li>Task-2: Create new resource group for Log Analytics workspace</li> <li>Task-3: Create Log Analytics workspace using terraform</li> <li>Task-4: Validate Log Analytics workspace in the portal</li> <li>Task-5: Lock the Log Analytics workspace resource group</li> </ul> <p>Through these tasks, you will gain practical experience on azure Log Analytics workspace.</p> <p>This is our first azure resource that will be created using Terraform configuration, open the terraform folder in VS core and start creating new files or update existing files for Log Analytics specific resources to provision in azure cloud.</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/7-log-analytics-workspace/#task-1-configure-terraform-variables-for-log-analytics-workspace","title":"Task-1: Configure terraform variables for Log Analytics workspace","text":"<p>Here's the table with the variable names, their descriptions, variable type and their default values:</p> Variable Name Description Type Default Value log_analytics_workspace_rg_name (Required) Specifies the resource group name of the log analytics workspace string \"rg-workspace-dev\" log_analytics_workspace_name (Required) Specifies the name of the log analytics workspace string \"workspace-workspace1-dev\" log_analytics_workspace_location (Required) Specifies the location of the log analytics workspace string \"East US\" log_analytics_workspace_sku (Optional) Specifies the sku of the log analytics workspace string \"PerGB2018\" solution_plan_map (Required) Specifies solutions to deploy to log analytics workspace map(any) See default value log_analytics_retention_days (Optional) Specifies the workspace data retention in days. number 30 log_analytics_tags (Optional) Specifies the tags of the log analytics map(any) {}"},{"location":"azure/7-log-analytics-workspace/#define-variables","title":"define variables","text":"<p>Here is the list of variable used in log analytics workspace creation; we are going to update existing <code>variable.tf</code> file with following variables. read the <code>description</code> provided in this source code to understand the purpose of each variable. also look into the variable <code>type</code> and <code>default</code> value of each variable.</p> variable.tf<pre><code>variable \"log_analytics_workspace_rg_name\" {\n  description = \"(Required) Specifies the resource group name of the log analytics workspace\"\n  type        = string\n  default     = \"rg-workspace-dev\"\n}\n\nvariable \"log_analytics_workspace_name\" {\n  description = \"(Required) Specifies the name of the log analytics workspace\"\n  type        = string\n  default     = \"workspace-workspace1-dev\"\n}\n\nvariable \"log_analytics_workspace_location\" {\n  description = \"(Required) Specifies the location of the log analytics workspace\"\n  type        = string\n  default     = \"East US\"\n}\n\nvariable \"log_analytics_workspace_sku\" {\n  description = \"(Optional) Specifies the sku of the log analytics workspace\"\n  type        = string\n  default     = \"PerGB2018\"\n\n  validation {\n    condition     = contains([\"Free\", \"Standalone\", \"PerNode\", \"PerGB2018\"], var.log_analytics_workspace_sku)\n    error_message = \"The log analytics sku is incorrect.\"\n  }\n}\n\nvariable \"solution_plan_map\" {\n  description = \"(Required) Specifies solutions to deploy to log analytics workspace\"\n  type        = map(any)\n  default = {\n    ContainerInsight   product   = \"OMSGallery/ContainerInsights\"\n      publisher = \"Microsoft\"\n    }\n  }\n}\n\nvariable \"log_analytics_retention_days\" {\n  description = \" (Optional) Specifies the workspace data retention in days. Possible values are either 7 (Free Tier only) or range between 30 and 730.\"\n  type        = number\n  default     = 30\n}\n\nvariable \"log_analytics_tags\" {\n  description = \"(Optional) Specifies the tags of the log analytics\"\n  type        = map(any)\n  default     = {}\n}\n</code></pre>"},{"location":"azure/7-log-analytics-workspace/#declare-variables","title":"declare variables","text":"<p>Here I am going to update existing <code>dev-variable.tfvar</code> file for the list of variable different for each environment.</p> dev-variable.tfvar<pre><code>log_analytics_workspace_rg_name     = \"workspace\"\nlog_analytics_workspace_name        = \"workspace1\"\n</code></pre> <p>Let's create a new file <code>log_analytics.tf</code> for log analytics workspace specific azure resources. </p>"},{"location":"azure/7-log-analytics-workspace/#task-2-create-new-resource-group-for-log-analytics-workspace","title":"Task-2: Create new resource group for Log Analytics workspace","text":"<p>We will be maintaining separate resource group for Log analytics workspace related resources.</p> <p>log_analytics.tf<pre><code># Create the resource group\nresource \"azurerm_resource_group\" \"workspace\" {\n  name     = lower(\"${var.rg_prefix}-${var.log_analytics_workspace_rg_name}-${local.environment}\")\n  location = var.log_analytics_workspace_location\n  tags     = merge(local.default_tags, var.log_analytics_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre> <code>Notes:</code></p> <ul> <li> <p><code>var.rg_prefix</code> - this variable is coming from <code>naming_conventions.tf</code> file. all the azure resource prefix naming conventions are listed in this file, keep this in mind for rest of the labs too.</p> </li> <li> <p><code>local.environment</code> - this is coming from local.tf file, terraform workspace value is nothing but environment value like dev, test, prod.</p> </li> </ul> locals.tf<pre><code>locals {\n  environment   = terraform.workspace != \"default\" ? terraform.workspace : \"\"\n}\n</code></pre> <p>run terraform plan &amp; apply and create new resource group.</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> </p>"},{"location":"azure/7-log-analytics-workspace/#task-3-create-log-analytics-workspace-using-terraform","title":"Task-3: Create Log Analytics workspace using terraform","text":"<p>Use the above resource group and create a new log analytics workspace using following terraform configuration. </p> variables_prefix.tf<pre><code>variable \"log_analytics_workspace_prefix\" {\n  type        = string\n  default     = \"workspace\"\n  description = \"Prefix of the log analytics workspace prefix resource.\"\n}\n</code></pre> log_analytics.tf<pre><code># Create Log Analytics Workspace \nresource \"azurerm_log_analytics_workspace\" \"workspace\" {\n  name                = lower(\"${var.log_analytics_workspace_prefix}-${var.log_analytics_workspace_name}-${local.environment}\")\n  resource_group_name = azurerm_resource_group.workspace.name\n  location            = var.log_analytics_workspace_location\n  sku                 = var.log_analytics_workspace_sku\n  retention_in_days   = var.log_analytics_retention_days != \"\" ? var.log_analytics_retention_days : null\n  tags                = merge(local.default_tags, var.log_analytics_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.workspace,\n  ]\n}\n\n# Create log analytics workspace solution\nresource \"azurerm_log_analytics_solution\" \"workspace_solution\" {\n  for_each              = var.solution_plan_map\n  solution_name         = each.key\n  resource_group_name   = azurerm_resource_group.workspace.name\n  location              = var.log_analytics_workspace_location\n  workspace_resource_id = azurerm_log_analytics_workspace.workspace.id\n  workspace_name        = azurerm_log_analytics_workspace.workspace.name\n  plan {\n    product   = each.value.product\n    publisher = each.value.publisher\n  }\n  tags = merge(local.default_tags, var.log_analytics_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_log_analytics_workspace.workspace,\n  ]\n}\n</code></pre> output.tf<pre><code>output \"log_analytics_workspace_id\" {\n  value       = azurerm_log_analytics_workspace.workspace.id\n  description = \"Specifies the resource id of the log analytics workspace\"\n}\n\noutput \"log_analytics_workspace_location\" {\n  value       = azurerm_log_analytics_workspace.workspace.location\n  description = \"Specifies the location of the log analytics workspace\"\n}\n\noutput \"log_analytics_workspace_name\" {\n  value       = azurerm_log_analytics_workspace.workspace.name\n  description = \"Specifies the name of the log analytics workspace\"\n}\n\noutput \"log_analytics_workspace_resource_group_name\" {\n  value       = azurerm_log_analytics_workspace.workspace.resource_group_name\n  description = \"Specifies the name of the resource group that contains the log analytics workspace\"\n}\n\noutput \"log_analytics_workspace_workspace_id\" {\n  value       = azurerm_log_analytics_workspace.workspace.workspace_id\n  description = \"Specifies the workspace id of the log analytics workspace\"\n}\n\noutput \"log_analytics_workspace_primary_shared_key\" {\n  value       = azurerm_log_analytics_workspace.workspace.primary_shared_key\n  description = \"Specifies the workspace key of the log analytics workspace\"\n  sensitive   = true\n}\n</code></pre>"},{"location":"azure/7-log-analytics-workspace/#terraform-validate","title":"Terraform validate","text":"<p><pre><code>terraform validate\n</code></pre> output</p> <pre><code>Success! The configuration is valid.\n</code></pre> <p>run terraform plan &amp; apply again here.</p>"},{"location":"azure/7-log-analytics-workspace/#terraform-plan","title":"Terraform plan","text":"<pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> <pre><code>azurerm_resource_group.rg: Refreshing state... [id=/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-resourcegroup1-dev]\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # azurerm_log_analytics_solution.workspace_solution[\"ContainerInsights\"] will be created\n  + resource \"azurerm_log_analytics_solution\" \"workspace_solution\" {\n      + id                    = (known after apply)\n      + location              = \"eastus\"\n      + resource_group_name   = \"rg-workspace-dev\"\n      + solution_name         = \"ContainerInsights\"\n      + tags                  = {\n          + \"CreatedBy\"   = \"Anji.Keesari\"\n          + \"Environment\" = \"dev\"\n          + \"Owner\"       = \"Anji.Keesari\"\n          + \"Project\"     = \"Project-1\"\n        }\n      + workspace_name        = \"workspace-workspace1-dev\"\n      + workspace_resource_id = (known after apply)\n\n      + plan {\n          + name      = (known after apply)\n          + product   = \"OMSGallery/ContainerInsights\"\n          + publisher = \"Microsoft\"\n        }\n    }\n\n  # azurerm_log_analytics_workspace.workspace will be created\n  + resource \"azurerm_log_analytics_workspace\" \"workspace\" {\n      + daily_quota_gb                     = -1\n      + id                                 = (known after apply)\n      + internet_ingestion_enabled         = true\n      + internet_query_enabled             = true\n      + location                           = \"eastus\"\n      + name                               = \"workspace-workspace1-dev\"\n      + primary_shared_key                 = (sensitive value)\n      + reservation_capacity_in_gb_per_day = (known after apply)\n      + resource_group_name                = \"rg-workspace-dev\"\n      + retention_in_days                  = 30\n      + secondary_shared_key               = (sensitive value)\n      + sku                                = \"PerGB2018\"\n      + tags                               = {\n          + \"CreatedBy\"   = \"Anji.Keesari\"\n          + \"Environment\" = \"dev\"\n          + \"Owner\"       = \"Anji.Keesari\"\n          + \"Project\"     = \"Project-1\"\n        }\n      + workspace_id                       = (known after apply)\n    }\n\n  # azurerm_resource_group.workspace will be created\n  + resource \"azurerm_resource_group\" \"workspace\" {\n      + id       = (known after apply)\n      + location = \"eastus\"\n      + name     = \"rg-workspace-dev\"\n      + tags     = {\n          + \"CreatedBy\"   = \"Anji.Keesari\"\n          + \"Environment\" = \"dev\"\n          + \"Owner\"       = \"Anji.Keesari\"\n          + \"Project\"     = \"Project-1\"\n        }\n    }\n\nPlan: 3 to add, 0 to change, 0 to destroy.\n\nChanges to Outputs:\n  + log_analytics_workspace_id                  = (known after apply)\n  + log_analytics_workspace_location            = \"eastus\"\n  + log_analytics_workspace_name                = \"workspace-workspace1-dev\"\n  + log_analytics_workspace_primary_shared_key  = (sensitive value)\n  + log_analytics_workspace_resource_group_name = \"rg-workspace-dev\"\n  + log_analytics_workspace_workspace_id        = (known after apply)\n</code></pre>"},{"location":"azure/7-log-analytics-workspace/#terraform-apply","title":"terraform apply","text":"<pre><code>terraform apply dev-plan\n</code></pre> <p>output</p> <pre><code>azurerm_resource_group.workspace: Creating...\nazurerm_resource_group.workspace: Creation complete after 1s [id=/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-workspace-dev]\nazurerm_log_analytics_workspace.workspace: Creating...\nazurerm_log_analytics_workspace.workspace: Still creating... [10s elapsed]\nazurerm_log_analytics_workspace.workspace: Still creating... [20s elapsed]\nazurerm_log_analytics_workspace.workspace: Still creating... [30s elapsed]\nazurerm_log_analytics_workspace.workspace: Creation complete after 37s [id=/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-workspace-dev/providers/Microsoft.OperationalInsights/workspaces/workspace-workspace1-dev]\nazurerm_log_analytics_solution.workspace_solution[\"ContainerInsights\"]: Creating...\nazurerm_log_analytics_solution.workspace_solution[\"ContainerInsights\"]: Creation complete after 4s [id=/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-workspace-dev/providers/Microsoft.OperationsManagement/solutions/ContainerInsights(workspace-workspace1-dev)]\n\nApply complete! Resources: 3 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nlog_analytics_workspace_id = \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-workspace-dev/providers/Microsoft.OperationalInsights/workspaces/workspace-workspace1-dev\"\nlog_analytics_workspace_location = \"eastus\"\nlog_analytics_workspace_name = \"workspace-workspace1-dev\"\nlog_analytics_workspace_primary_shared_key = &lt;sensitive&gt;\nlog_analytics_workspace_resource_group_name = \"rg-workspace-dev\"\nlog_analytics_workspace_workspace_id = \"d66de214-064b-4745-abcf-e8a8060fce1f\"\nresource_group_name = \"rg-resourcegroup1-dev\"\n</code></pre> <p></p>"},{"location":"azure/7-log-analytics-workspace/#task-4-validate-log-analytics-workspace-in-the-azure-portal","title":"Task-4: Validate Log Analytics workspace in the Azure portal","text":"<p>Once azure resources are created, login into azure portal and validate new those new resources.</p> <p></p>"},{"location":"azure/7-log-analytics-workspace/#task-5-lock-the-resource-group","title":"Task-5: Lock the resource group","text":"<p>Locking the resource group is the final steps once all the resources are created. this will prevent unexpected resource deletions by any kind of manual or automation script.</p> log_analytics.tf<pre><code># Lock the resource group\nresource \"azurerm_management_lock\" \"rg_workspace_lock\" {\n  name       = \"CanNotDelete\"\n  scope      = azurerm_resource_group.workspace.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource group can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_resource_group.workspace,\n    azurerm_log_analytics_workspace.workspace,\n    azurerm_log_analytics_solution.workspace_solution,\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <p><pre><code>terraform validate\nterraform fmt\n</code></pre> Log Analytics run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p> <p>That's it, now we've fully working azure Log analytics workspace which we are going to use for future labs.</p>"},{"location":"azure/7-log-analytics-workspace/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Log Analytics workspace overview</li> <li>Terraform Registry - azurerm_resource_group</li> <li>Terraform Registry - azurerm_log_analytics_solution</li> <li>Terraform Registry - azurerm_log_analytics_workspace</li> <li>Terraform Registry - azurerm_management_lock</li> <li>Azure Terraform Quickstart/201-aks-log-analytics</li> </ul>"},{"location":"azure/8-vnet/","title":"Create Virtual Network using terraform","text":""},{"location":"azure/8-vnet/#introduction","title":"Introduction","text":"<p>Azure Virtual Network also known as vnet is the fundamental building block for private network for securing the infrastructure, azure virtual network will be used for securely communicate with each other in the azure infrastructure and securely connecting from outside publicly, azure vnet will brings with it additional benefits of Azure's infrastructure such as scale, availability, and isolation.</p> <p>In this lab, I will walk you through the steps to create an Azure Virtual Network for a hub-and-spoke model with subnets using Terraform. Additionally, I will show you how to confirm its successful deployment through the Azure portal. Through these tasks, you will gain practical experience on Azure Virtual Network.</p>"},{"location":"azure/8-vnet/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you have been asked to deploy azure resources with fully secured and isolated networking capabilities, we will use the azure virtual network capabilities to protect azure virtual machines, AKS clusters, azure key vault services, azure storage account services using azure virtual network.</p>"},{"location":"azure/8-vnet/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Define and declare virtual network variables</li> <li>Task-2: Create a resource group for virtual network</li> <li>Task-3: Create hub virtual network using terraform</li> <li>Task-4: Review hub virtual network resources</li> <li>Task-5: Create spoke virtual network using terraform   </li> <li>Task-6: Review spoke virtual network resources</li> </ul> <ul> <li>Task-7: Create vnet Peering from hub to spoke</li> <li>Task-8: Create vnet Peering from spoke to hub   </li> <li>Task-9: Lock virtual network resource group</li> </ul>"},{"location":"azure/8-vnet/#architecture-diagram","title":"Architecture diagram","text":"<p>Here is the reference architecture diagram used for creating virtual network with hub &amp; spoke model.</p> <p></p>"},{"location":"azure/8-vnet/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Visual studio code</li> <li>Azure DevOps Project &amp; repo</li> <li>Terraform Foundation</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Basic knowledge of terraform and azure concepts.</li> </ul>"},{"location":"azure/8-vnet/#implementation-details","title":"Implementation details","text":"<p>In this lab we are going to create Hub &amp; Spoke virtual network topology. Let's try to understand little bit about this here before jumping into the lab:</p> <p>A hub and spoke virtual network architecture is a networking topology commonly used in cloud computing, and particularly in Microsoft Azure. In this architecture, a central \"hub\" virtual network is connected to one or more \"spoke\" virtual networks, forming a hub-and-spoke topology.</p> <p>There are several benefits to using a hub and spoke virtual network architecture in Azure:</p> <ul> <li>Scalability: The hub and spoke architecture is highly scalable and can be easily extended as the needs of the organization change.</li> <li>Security: The centralized hub network can act as a secure boundary for the entire network, providing a single point of entry for traffic.</li> <li>Simplified management: The hub and spoke architecture simplifies network management, making it easier to monitor and manage traffic between the various spoke networks.</li> <li>Cost-effective: The hub and spoke architecture can help reduce costs by reducing the need for redundant networking infrastructure.</li> </ul> <p>In Azure the hub network is created first, followed by the spoke networks which are then connected to the hub network.</p> <p>Now let's start working on the lab.</p> <p>Open the terraform project folder in Visual Studio code and creating new file named <code>virtual_network.tf</code> for Virtual Network specific azure resources;</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/8-vnet/#task-1-define-and-declare-virtual-network-variables","title":"Task-1: Define and declare virtual network variables","text":"<p>This section covers list of variables used to create virtual network with detailed description and purpose of each variable with default values.</p> Name Description Values vnet_rg_name Name of the resource group name for virtual network rg-vnet1-dev vnet_location Location in which to deploy the virtual network East US hub_vnet_name Specifies the name of the hub virtual virtual network vnet-hub-dev hub_vnet_address_space Specifies the address space of the hub virtual virtual network [\"10.63.0.0/20\"] hub_gateway_subnet_name Specifies the Name of the hub gateway subnet gateway hub_gateway_subnet_address_space Specifies the address space of the hub gateway subnet [\"10.63.0.0/25\"] hub_appgtw_subnet_name Specifies the Name of the hub application gateway subnet snet-appgtw1 hub_appgtw_subnet_address_space Specifies the address space of the hub application-gateway subnet [\"10.63.1.0/28\"] hub_bastion_subnet_name Specifies the Name of the hub bastion host subnet AzureBastionSubnet hub_bastion_subnet_address_space Specifies the address space of the hub bastion host subnet [\"10.63.2.0/28\"] spoke_vnet_name Specifies the name of the spoke virtual virtual network vnet-spoke-dev spoke_vnet_address_space Specifies the address space of the spoke virtual virtual network [\"10.64.0.0/16\"] spoke_gateway_subnet_name Specifies the Name of the spoke gateway subnet gateway spoke_gateway_subnet_address_space Specifies the address space of the gateway subnet [\"10.64.0.0/25\"] spoke_aks_subnet_name Specifies the name of the aks cluster -1 name snet-aks1 spoke_aks_subnet_address_space Specifies the address space of the gateway subnet [\"10.64.1.0/22\"] spoke_postgresql_subnet_name Specifies the name of the postgresql name snet-postgresql1 spoke_postgresql_subnet_address_space Specifies the address space of the postgresql subnet [\"10.64.2.0/26\"] <p>Variables Prefixed</p> <p>Here is the list of new prefixes used in this lab</p> variables_prefix.tf<pre><code>variable \"vnet_prefix\" {\n  type        = string\n  default     = \"vnet\"\n  description = \"Prefix of the vnet name.\"\n}\nvariable \"subnet_prefix\" {\n  type        = string\n  default     = \"snet\"\n  description = \"Prefix of the Subnet name.\"\n}\n</code></pre> <p>Declare Variables</p> <p>Here is the list of new variables used in this lab</p> <p>variables.tf<pre><code>// ========================== virtual netowrking ==========================\n\nvariable \"vnet_rg_name\" {\n  description = \"Name of the resource group name for virtual network\"\n  type        = string\n  default     = \"rg-vnet1-dev\"\n}\n\nvariable \"vnet_location\" {\n  description = \"Location in which to deploy the virtual network\"\n  type        = string\n  default     = \"East US\"\n}\n\nvariable \"hub_vnet_name\" {\n  description = \"Specifies the name of the hub virtual virtual network\"\n  default     = \"vnet-hub-dev\"\n  type        = string\n}\n\nvariable \"hub_vnet_address_space\" {\n  description = \"Specifies the address space of the hub virtual virtual network\"\n  type        = list(string)\n  default     = [\"10.1.0.0/16\"]\n}\n\n\nvariable \"hub_gateway_subnet_name\" {\n  description = \"Specifies the name of the gateway subnet\"\n  default     = \"gateway\"\n  type        = string\n}\n\nvariable \"hub_gateway_subnet_address_prefixes\" {\n  description = \"Specifies the address prefix of the hub gateway subnet\"\n  type        = list(string)\n}\n\nvariable \"hub_bastion_subnet_name\" {\n  description = \"Specifies the name of the hub vnet AzureBastion subnet\"\n  default     = \"AzureBastionSubnet\"\n  type        = string\n}\nvariable \"hub_bastion_subnet_address_prefixes\" {\n  description = \"Specifies the address prefix of the hub bastion host subnet\"\n  type        = list(string)\n}\nvariable \"hub_firewall_subnet_name\" {\n  description = \"Specifies the name of the azure firewall subnet\"\n  type        = string\n  default     = \"AzureFirewallSubnet\"\n}\nvariable \"hub_firewall_subnet_address_prefixes\" {\n  description = \"Specifies the address prefix of the azure firewall subnet\"\n  type        = list(string)\n}\n\nvariable \"spoke_vnet_name\" {\n  description = \"Specifies the name of the spoke virtual virtual network\"\n  type        = string\n  default     = \"vnet-spoke-dev\"\n}\n\nvariable \"spoke_vnet_address_space\" {\n  description = \"Specifies the address space of the spoke virtual virtual network\"\n  type        = list(string)\n  default     = [\"10.0.0.0/16\"]\n}\n\nvariable \"jumpbox_subnet_name\" {\n  description = \"Specifies the name of the jumpbox subnet\"\n  default     = \"snet-jumpbox\"\n  type        = string\n}\n\nvariable \"jumpbox_subnet_address_prefix\" {\n  description = \"Specifies the address prefix of the jumbox subnet\"\n  type        = list(string)\n}\n\n\nvariable \"aks_subnet_name\" {}\nvariable \"aks_address_prefixes\" {}\nvariable \"appgtw_subnet_name\" {}\nvariable \"appgtw_address_prefixes\" {}\nvariable \"psql_subnet_name\" {}\nvariable \"psql_address_prefixes\" {}\n\nvariable \"vnet_log_analytics_retention_days\" {\n  description = \"Specifies the number of days of the retention policy\"\n  type        = number\n  default     = 7\n}\n\nvariable \"vnet_tags\" {\n  description = \"(Optional) Specifies the tags of the virtual network\"\n  type        = map(any)\n  default     = {}\n}\nvariable \"gateway_address_prefixes\" {\n  type    = string\n  default = \"10.64.0.0/26\"\n}\nvariable \"gatewaysubnet_address_prefixes\" {\n  type    = string\n  default = \"10.64.0.128/28\"\n}\n</code></pre> Define variables</p> <p>Here is the list of new variables used in this lab</p> <p><code>dev-variables.tfvar</code> - update this existing file for virtual network values for development environment. dev-variables.tfvar<pre><code># virtual network\nvnet_rg_name                        = \"vnet1\"\nvnet_location                       = \"East US\"\nhub_vnet_name                       = \"hub\"\nhub_vnet_address_space              = [\"10.63.0.0/20\"]\nhub_gateway_subnet_name             = \"gateway\"\nhub_gateway_subnet_address_prefixes = [\"10.63.0.0/25\"] // HostMin:   10.63.0.1 , HostMax:   10.63.0.126  \nhub_bastion_subnet_name             = \"AzureBastionSubnet\"\nhub_bastion_subnet_address_prefixes = [\"10.63.0.128/28\"] //HostMin:   10.63.0.129,HostMax:   10.63.0.142\nappgtw_subnet_name                  = \"appgtw\"\nappgtw_address_prefixes             = \"10.63.1.0/28\"\nhub_firewall_subnet_name            = \"AzureFirewallSubnet\"\nhub_firewall_subnet_address_prefixes= [\"10.63.2.0/24\"]\nspoke_vnet_name                     = \"spoke\" //spoke_vnet_name\nspoke_vnet_address_space            = [\"10.64.0.0/16\"]\naks_subnet_name                     = \"aks1\"\naks_address_prefixes                = \"10.64.4.0/22\"\npsql_subnet_name                    = \"psql1\"\npsql_address_prefixes               = \"10.64.2.0/26\"\njumpbox_subnet_name                 = \"jumpbox\"\njumpbox_subnet_address_prefix       = [\"10.64.3.0/28\"]\n</code></pre></p> <p>output variables</p> <p>Here is the list of output variables used in this lab </p> output.tf<pre><code>// ========================== virtual netowrking ==========================\noutput \"vnet_name\" {\n  description = \"Specifies the name of the virtual network\"\n  value       = azurerm_virtual_network.vnet.name\n}\n\noutput \"vnet_id\" {\n  description = \"Specifies the resource id of the virtual network\"\n  value       = azurerm_virtual_network.vnet.id\n}\n\noutput \"subnet_gateway_id\" {\n  description = \"Specifies the resource id of the gateway subnets\"\n  value       = azurerm_subnet.gateway.id\n}\noutput \"subnet_appgtw_id\" {\n  description = \"Specifies the resource id of the appgtw subnets\"\n  value       = azurerm_subnet.appgtw.id\n}\noutput \"subnet_psql_id\" {\n  description = \"Specifies the resource id of the psql subnets\"\n  value       = azurerm_subnet.psql.id\n}\noutput \"subnet_aks_id\" {\n  description = \"Specifies the resource id of the tenantmgmt subnets\"\n  value       = azurerm_subnet.aks.id\n}\n</code></pre>"},{"location":"azure/8-vnet/#task-2-create-a-resource-group-for-virtual-network","title":"Task-2: Create a resource group for virtual network","text":"<p>We will create separate resource group for the all the networking related resources.</p> <p>In this task, we will create Azure resource group by using the terraform </p> <p>network.tf<pre><code># Create the resource group\nresource \"azurerm_resource_group\" \"vnet\" {\n  name     = lower(\"${var.rg_prefix}-${var.vnet_rg_name}-${local.environment}\")\n  location = var.vnet_location\n  tags     = merge(local.default_tags, var.vnet_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> </p>"},{"location":"azure/8-vnet/#task-3-create-hub-virtual-network-using-terraform","title":"Task-3: Create Hub virtual network using terraform","text":"<p>Once the resource group is ready, we will start creating the hub virtual network in that existing resource group.</p> <p>Calculate address ranges</p> <p>you may want to use this link to calculate the address rages required for your virtual networks.</p> <p>https://jodies.de/ipcalc</p> <p>here is the terraform configuration for Hub virtual network and subnets inside the hub vnet.</p> <p>List of subnets created:</p> <ul> <li>gateway subnet - default</li> <li>subnet for application gateway</li> <li>subnet for bastion subnet</li> <li>subnet for azure firewall subnet</li> </ul> network.tf<pre><code># Create hub virtual network (Management vnet)\nresource \"azurerm_virtual_network\" \"hub_vnet\" {\n  name                = lower(\"${var.vnet_prefix}-${var.hub_vnet_name}-${local.environment}\")\n  address_space       = var.hub_vnet_address_space\n  resource_group_name = azurerm_resource_group.vnet.name\n  location            = azurerm_resource_group.vnet.location\n  depends_on = [\n    azurerm_resource_group.vnet,\n  ]\n}\n\n//Create hub vnet gateway subnet\nresource \"azurerm_subnet\" \"hub_gateway\" {\n  name                 = var.hub_gateway_subnet_name\n  resource_group_name  = azurerm_virtual_network.hub_vnet.resource_group_name\n  virtual_network_name = azurerm_virtual_network.hub_vnet.name\n  address_prefixes     = var.hub_gateway_subnet_address_prefixes\n  depends_on = [\n    azurerm_virtual_network.hub_vnet\n  ]\n}\n\n// Create hub bastion host subnet\nresource \"azurerm_subnet\" \"hub_bastion\" {\n  name                                          = var.hub_bastion_subnet_name\n  resource_group_name                           = azurerm_virtual_network.hub_vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.hub_vnet.name\n  address_prefixes                              = var.hub_bastion_subnet_address_prefixes\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  depends_on = [\n    azurerm_virtual_network.hub_vnet\n  ]\n}\n\n// Create hub application gateway subnet\nresource \"azurerm_subnet\" \"appgtw\" {\n  name                                          = lower(\"${var.subnet_prefix}-${var.appgtw_subnet_name}\")\n  resource_group_name                           = azurerm_virtual_network.hub_vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.hub_vnet.name\n  address_prefixes                              = [var.appgtw_address_prefixes]\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  depends_on = [\n    azurerm_virtual_network.hub_vnet\n  ]\n}\n\n// Create hub azure firewall subnet\nresource \"azurerm_subnet\" \"firewall\" {\n  name                                          = var.hub_firewall_subnet_name\n  resource_group_name                           = azurerm_virtual_network.hub_vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.hub_vnet.name\n  address_prefixes                              = var.hub_firewall_subnet_address_prefixes\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  depends_on = [\n    azurerm_virtual_network.hub_vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> Address space</p> <p></p> <p>subnets</p> <p></p>"},{"location":"azure/8-vnet/#task-4-create-spoke-virtual-network-using-terraform","title":"Task-4: Create Spoke virtual network using terraform","text":"<p>Once the hub vnet creation has been completed then we will start creating the spoke virtual network in that existing resource group.</p> <p>again you can use the below link for calculating address spaces for spoke virtual network before creating it.</p> <p>https://jodies.de/ipcalc</p> <p>In this task, we will create a spoke virtual network and subnets by using the terraform</p> <p>Here is the list of subnets created in spoke vnet:</p> <ul> <li>gateway subnet - default</li> <li>VPN gateway subnet</li> <li>subnet for PostgreSQL</li> <li>subnet for AKS cluster</li> <li>jumpm VM server subnet</li> </ul> network.tf<pre><code># Create spoke virtual network\nresource \"azurerm_virtual_network\" \"vnet\" {\n  name                = lower(\"${var.vnet_prefix}-${var.spoke_vnet_name}-${local.environment}\")\n  address_space       = var.spoke_vnet_address_space\n  resource_group_name = azurerm_resource_group.vnet.name\n  location            = azurerm_resource_group.vnet.location\n  depends_on = [\n    azurerm_resource_group.vnet,\n  ]\n}\n\n// gateway subnet\nresource \"azurerm_subnet\" \"gateway\" {\n  name                 = \"gateway\"\n  resource_group_name  = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name = azurerm_virtual_network.vnet.name\n  address_prefixes     = [var.gateway_address_prefixes]\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n\n// VPN gateway subnet\nresource \"azurerm_subnet\" \"vpn_gateway\" {\n  name                 = \"GatewaySubnet\"\n  resource_group_name  = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name = azurerm_virtual_network.vnet.name\n  address_prefixes     = [var.gatewaysubnet_address_prefixes]\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n\n// postgreSQL subnet\nresource \"azurerm_subnet\" \"psql\" {\n  name                                          = lower(\"${var.subnet_prefix}-${var.psql_subnet_name}\")\n  resource_group_name                           = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.vnet.name\n  address_prefixes                              = [var.psql_address_prefixes]\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  service_endpoints                             = [\"Microsoft.Storage\"]\n  delegation {\n    name = \"fs\"\n    service_delegation {\n      name = \"Microsoft.DBforPostgreSQL/flexibleServers\"\n      actions = [\n        \"Microsoft.Network/virtualNetworks/subnets/join/action\",\n      ]\n    }\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n\n// aks subnet\nresource \"azurerm_subnet\" \"aks\" {\n  name                                          = lower(\"${var.subnet_prefix}-${var.aks_subnet_name}\")\n  resource_group_name                           = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.vnet.name\n  address_prefixes                              = [var.aks_address_prefixes]\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n\n// jumpm VM server subnet\nresource \"azurerm_subnet\" \"jumpbox\" {\n  name                                          = lower(\"${var.subnet_prefix}-${var.jumpbox_subnet_name}\")\n  resource_group_name                           = azurerm_virtual_network.vnet.resource_group_name\n  virtual_network_name                          = azurerm_virtual_network.vnet.name\n  address_prefixes                              = var.jumpbox_subnet_address_prefix\n  private_endpoint_network_policies_enabled     = false\n  private_link_service_network_policies_enabled = false\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Address space</p> <p></p> <p>subnets</p> <p></p>"},{"location":"azure/8-vnet/#task-5-create-diagnostics-settings-for-networking","title":"Task-5: Create Diagnostics Settings for Networking","text":"<p>We are going to create diagnostics setting for all the resources created as part of our labs. Once the diagnostics setting is created, Azure will begin collecting diagnostic logs for the selected categories and sending them to the designated destination. You can use these logs to monitor and troubleshoot your network resources, as well as to create custom alerts and dashboards for monitoring purposes.</p> <p>network.tf<pre><code># Create Diagnostics Settings for Networking\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_vnet\" {\n  name                       = \"DiagnosticsSettings\"\n  target_resource_id         = azurerm_virtual_network.vnet.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  log {\n    category = \"VMProtectionAlerts\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.vnet_log_analytics_retention_days\n    }\n  }\n\n  metric {\n    category = \"AllMetrics\"\n\n    retention_policy {\n      enabled = true\n      days    = var.log_analytics_retention_days\n    }\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet,\n    azurerm_log_analytics_workspace.workspace,\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/8-vnet/#task-6-lock-the-resource-group","title":"Task-6: Lock the resource group","text":"<p>Finally, it is time to lock the resource group created part of this exercise, so that we can avoid the accidental deletion of the azure resources created here.</p> <p>network.tf<pre><code># Lock the resource group\nresource \"azurerm_management_lock\" \"vnet\" {\n  name       = \"CanNotDelete\"\n  scope      = azurerm_resource_group.vnet.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource group can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_resource_group.vnet,\n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> list of resources in the vnet resource group</p> <p></p> <p>resource group lock.</p> <p></p> <p>Now our Virtual Network is created and ready to use. You can now deploy AKS, ACR and other resources within your Virtual Network, and configure their network settings as needed.</p>"},{"location":"azure/8-vnet/#references","title":"References","text":"<ul> <li>Microsoft MSDN - Virtual Network documentation</li> <li>Microsoft MSDN - Virtual network peering</li> <li>Microsoft MSDN - Network concepts for applications in Azure Kubernetes Service (AKS)</li> <li>Microsoft MSDN - Hub-spoke network topology in Azure</li> <li>Microsoft MSDN - Baseline architecture for an Azure Kubernetes Service (AKS) cluster</li> <li>Terraform Registry - azurerm_resource_group</li> <li>Terraform Registry - azurerm_virtual_network</li> <li>Terraform Registry - azurerm_subnet</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_management_lock</li> <li>IP Calculator</li> <li>Azure Terraform Quickstart/301-hub-spoke</li> <li>Azure Terraform Quickstart/301-aks-enterprise/networking</li> </ul>"},{"location":"azure/9-acr/","title":"Create Azure Container Registry (ACR) using terraform","text":""},{"location":"azure/9-acr/#introduction","title":"Introduction","text":"<p>Azure Container Registry (ACR) is a managed Docker registry service provided by Microsoft Azure. It allows you to store and manage container images for your applications in a secure and private environment.</p> <p>In this lab, I will guide you through the process of creating an Azure Container Registry using Terraform. Furthermore, I will demonstrate how to verify its successful deployment within the Azure portal and provide insights on how to utilize it effectively post-creation.</p> <p>ACR provides a number of benefits, including:</p> <ul> <li> <p>Private repository: ACR provides a private Docker registry, which means that you can store your Docker images securely and privately, and only authorized users or services can access them.</p> </li> <li> <p>High availability: ACR is built on Azure, so it benefits from Azure's global network and high availability features. This means that your container images are always available, and you can easily replicate them across regions for disaster recovery.</p> </li> <li> <p>Integration with Azure services: ACR integrates seamlessly with other Azure services, such as Azure Kubernetes Service (AKS), Azure Web Apps, and Azure DevOps, making it easy to incorporate ACR into your existing workflows.</p> </li> <li> <p>Security and compliance: ACR provides built-in security features, such as role-based access control, network security, and encryption at rest, to help you meet your security and compliance requirements.</p> </li> <li> <p>Geo-replication: ACR allows you to replicate your container images across multiple regions for improved performance and disaster recovery.</p> </li> </ul> <p>To get started with ACR, we are going to use terraform to create a new Azure Container Registry. Once you have a registry, you can push your Docker images to it using the Docker CLI, Azure CLI, or other tools, and manage your images using the Azure portal or a variety of third-party tools.</p>"},{"location":"azure/9-acr/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, you have been asked to store and manage organization application development private container images and helm charts in secure way in cloud so that these are not directly accessible outside of the company network. also, make sure that azure Container Registry should provide organization users with direct control of their container content, with integrated authentication.</p>"},{"location":"azure/9-acr/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Create ACR resource group </li> <li>Task-2: Configure variables for ACR</li> <li>Task-3: Create ACR user assigned identity</li> <li>Task-4: Create Azure Container Registry (ACR) using terraform</li> <li>Task-5: Create Diagnostics Settings for ACR</li> <li>Task-6: Lock ACR resource group</li> <li>Task-7: Validate ACR resource<ul> <li>Task-7.1: Log in to registry</li> <li>Task-7.2: Push image to registry</li> <li>Task-7.3: Pull image from registry</li> <li>Task-7.4: List container images</li> </ul> </li> <li>Task-8: Restrict Access Using Private Endpoint<ul> <li>Task-8.1: Configure the Private DNS Zone</li> <li>Task-8.2: Create a Virtual Network Link Association</li> <li>Task-8.3: Create a Private Endpoint Using Terraform</li> <li>Task-8.4: Validate private link connection using <code>nslookup</code> or <code>dig</code> </li> </ul> </li> </ul> <p>Through these tasks, you will gain practical experience on Azure Container Registry.</p>"},{"location":"azure/9-acr/#architecture-diagram","title":"Architecture diagram","text":"<p>Here is the reference architecture diagram of Azure container registry.</p> <p></p>"},{"location":"azure/9-acr/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Terraform</li> <li>Download &amp; Install Azure CLI</li> <li>Azure subscription</li> <li>Visual studio code</li> <li>Azure DevOps Project &amp; repo</li> <li>Terraform Foundation</li> <li>Log Analytics workspace - for configuring diagnostic settings.</li> <li>Virtual Network with subnet - for configuring a private endpoint.</li> <li>Basic knowledge of terraform and azure concepts.</li> </ul>"},{"location":"azure/9-acr/#implementation-details","title":"Implementation details","text":"<p>Open the terraform project folder in Visual Studio code and creating new file named <code>acr.tf</code> for Azure container registry specific azure resources;</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"azure/9-acr/#task-1-define-and-declare-acr-variables","title":"Task-1: Define and declare ACR variables","text":"<p>This section covers list of variables used to create  Azure container registry with detailed description and purpose of each variable with default values.</p> Variable Name Description Type Default Value acr_name (Required) Specifies the name of the Container Registry. Changing this forces a new resource to be created. string acr1dev acr_rg_name (Required) The name of the resource group in which to create the Container Registry. Changing this forces a new resource to be created. string acr1 acr_location Location in which to deploy the Container Registry string \"East US\" acr_admin_enabled (Optional) Specifies whether the admin user is enabled. Defaults to false. bool false acr_sku (Optional) The SKU name of the container registry. Possible values are Basic, Standard, and Premium. Defaults to Basic string \"Basic\" acr_georeplication_locations (Optional) A list of Azure locations where the container registry should be geo-replicated. list(string) [\"Central US\", \"East US\"] acr_log_analytics_retention_days Specifies the number of days of the retention policy number 7 acr_tags (Optional) Specifies the tags of the ACR map(any) {} data_endpoint_enabled (Optional) Whether to enable dedicated data endpoints for this Container Registry? Defaults to false. This is only supported on resources with the Premium SKU. bool true <p>Variables Prefixed</p> <p>Here is the list of new prefixes used in this lab</p> variables_prefix.tf<pre><code>variable \"acr_prefix\" {\n  type        = string\n  default     = \"acr\"\n  description = \"Prefix of the Azure Container Registry (ACR) name that's combined with name of the ACR\"\n}\n</code></pre> <p>Declare Variables</p> <p>Here is the list of new variables used in this lab</p> <p>variables.tf<pre><code>// ========================== Azure Container Registry (ACR) ==========================\n\nvariable \"acr_name\" {\n  description = \"(Required) Specifies the name of the Container Registry. Changing this forces a new resource to be created.\"\n  type        = string\n}\n\nvariable \"acr_rg_name\" {\n  description = \"(Required) The name of the resource group in which to create the Container Registry. Changing this forces a new resource to be created.\"\n  type        = string\n}\n\nvariable \"acr_location\" {\n  description = \"Location in which to deploy the Container Registry\"\n  type        = string\n  default     = \"East US\"\n}\n\nvariable \"acr_admin_enabled\" {\n  description = \"(Optional) Specifies whether the admin user is enabled. Defaults to false.\"\n  type        = string\n  default     = false\n}\n\nvariable \"acr_sku\" {\n  description = \"(Optional) The SKU name of the container registry. Possible values are Basic, Standard and Premium. Defaults to Basic\"\n  type        = string\n  default     = \"Basic\"\n\n  validation {\n    condition     = contains([\"Basic\", \"Standard\", \"Premium\"], var.acr_sku)\n    error_message = \"The container registry sku is invalid.\"\n  }\n}\nvariable \"acr_georeplication_locations\" {\n  description = \"(Optional) A list of Azure locations where the container registry should be geo-replicated.\"\n  type        = list(string)\n  default     = [\"Central US\", \"East US\"]\n}\n\nvariable \"acr_log_analytics_retention_days\" {\n  description = \"Specifies the number of days of the retention policy\"\n  type        = number\n  default     = 7\n}\nvariable \"acr_tags\" {\n  description = \"(Optional) Specifies the tags of the ACR\"\n  type        = map(any)\n  default     = {}\n}\nvariable \"data_endpoint_enabled\" {\n  description = \"(Optional) Whether to enable dedicated data endpoints for this Container Registry? Defaults to false. This is only supported on resources with the Premium SKU.\"\n  default     = true\n  type        = bool\n}\nvariable \"pe_acr_subresource_names\" {\n  description = \"(Optional) Specifies a subresource names which the Private Endpoint is able to connect to ACR.\"\n  type        = list(string)\n  default     = [\"registry\"]\n}\n</code></pre> Define variables</p> <p>Here is the list of new variables used in this lab</p> <p><code>dev-variables.tfvar</code> - update this existing file for ACR values for development environment. dev-variables.tfvar<pre><code># container registry\nacr_rg_name                         = \"acr\"\nacr_name                            = \"acr1dev\"\nacr_sku                             = \"Basic\"\nacr_admin_enabled                   = true\ndata_endpoint_enabled               = false\n</code></pre></p> <p>output variables</p> <p>Here is the list of output variables used in this lab </p> output.tf<pre><code>// ========================== Azure Container Registry (ACR) ==========================\n\noutput \"acr_name\" {\n  description = \"Specifies the name of the container registry.\"\n  value       = azurerm_container_registry.acr.name\n}\n\noutput \"acr_id\" {\n  description = \"Specifies the resource id of the container registry.\"\n  value       = azurerm_container_registry.acr.id\n}\n\noutput \"acr_resource_group_name\" {\n  description = \"Specifies the name of the resource group.\"\n  value       = azurerm_container_registry.acr.resource_group_name\n}\n\noutput \"acr_login_server\" {\n  description = \"Specifies the login server of the container registry.\"\n  value       = azurerm_container_registry.acr.login_server\n}\n\noutput \"acr_login_server_url\" {\n  description = \"Specifies the login server url of the container registry.\"\n  value       = \"https://${azurerm_container_registry.acr.login_server}\"\n}\n\noutput \"acr_admin_username\" {\n  description = \"Specifies the admin username of the container registry.\"\n  value       = azurerm_container_registry.acr.admin_username\n}\n</code></pre>"},{"location":"azure/9-acr/#task-2-create-a-resource-group-for-acr","title":"Task-2: Create a resource group for ACR","text":"<p>We will create separate resource group for ACR and related resources. add following terraform configuration in <code>acr.tf</code> file for creating ACR resource group.</p> <p>In this task, we will create Azure resource group by using the terraform </p> <p>acr.tf<pre><code># Create the resource group\nresource \"azurerm_resource_group\" \"rg_acr\" {\n  name     = lower(\"${var.rg_prefix}-${var.acr_rg_name}-${local.environment}\")\n  location = var.acr_location\n  tags     = merge(local.default_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/9-acr/#task-3-create-acr-user-assigned-identity","title":"Task-3: Create ACR user assigned identity","text":"<p>Use the following terraform configuration for creating user assigned identity which is going be used in ACR</p> <p>User assigned managed identities enable Azure resources to authenticate to cloud services (e.g. Azure Key Vault) without storing credentials in code. </p> <p>User Assigned Identity in Azure Container Registry provides improved security, simplified management, better integration with Azure services, RBAC, and better compliance, making it a beneficial feature for organizations that use ACR.</p> acr.tf<pre><code># Create ACR user assigned identity\nresource \"azurerm_user_assigned_identity\" \"acr_identity\" {  \n  resource_group_name = azurerm_resource_group.rg_acr.name\n  location            = azurerm_resource_group.rg_acr.location\n  tags                = merge(local.default_tags, var.acr_tags)\n\n  name = \"${var.acr_name}Identity\"\n  depends_on = [\n    azurerm_resource_group.rg_acr,\n  ]\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n}\n</code></pre> <p></p>"},{"location":"azure/9-acr/#task-4-create-azure-container-registry-acr-using-terraform","title":"Task-4: Create Azure Container Registry (ACR) using terraform","text":"<p>Use the following terraform configuration for creating ACR.</p> acr.tf<pre><code># Create the Container Registry\nresource \"azurerm_container_registry\" \"acr\" {  \n  name                = var.acr_name\n  resource_group_name = azurerm_resource_group.rg_acr.name\n  location            = azurerm_resource_group.rg_acr.location\n  sku                 = var.acr_sku\n  admin_enabled       = var.acr_admin_enabled\n  # zone_redundancy_enabled = true\n  data_endpoint_enabled = var.data_endpoint_enabled\n  identity {\n    type = \"UserAssigned\"\n    identity_ids = [\n      azurerm_user_assigned_identity.acr_identity.id\n    ]\n  }\n\n  # dynamic \"georeplications\" {\n  #   for_each = var.acr_georeplication_locations\n\n  #   content {\n  #     location = georeplications.value\n  #     tags     = merge(local.default_tags, var.acr_tags)\n  #   }\n  # }\n  tags = merge(local.default_tags, var.acr_tags)\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg_acr,\n    azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p></p>"},{"location":"azure/9-acr/#task-5-create-diagnostics-settings-for-acr","title":"Task-5: Create Diagnostics Settings for ACR","text":"<p>we are going to use diagnostics settings for all kind of azure resources to manage logs and metrics etc... Let's create diagnostics settings for ACR for storing Logs and Metric with default retention of 30 days or as per the requirements.</p> <p>acr.tf<pre><code># create Diagnostics Settings for ACR\nresource \"azurerm_monitor_diagnostic_setting\" \"diag_acr\" {  \n  name                       = \"DiagnosticsSettings\"\n  target_resource_id         = azurerm_container_registry.acr.id\n  log_analytics_workspace_id = azurerm_log_analytics_workspace.workspace.id\n\n  log {\n    category = \"ContainerRegistryRepositoryEvents\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.acr_log_analytics_retention_days\n    }\n  }\n\n  log {\n    category = \"ContainerRegistryLoginEvents\"\n    enabled  = true\n\n    retention_policy {\n      enabled = true\n      days    = var.acr_log_analytics_retention_days\n    }\n  }\n\n  metric {\n    category = \"AllMetrics\"\n\n    retention_policy {\n      enabled = true\n      days    = var.acr_log_analytics_retention_days\n    }\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> </p>"},{"location":"azure/9-acr/#task-6-lock-the-resource-group","title":"Task-6: Lock the resource group","text":"<p>Finally, it is time to lock the resource group created part of this exercise, so that we can avoid the accidental deletion of the azure resources created here.</p> <p>acr.tf<pre><code># Lock the resource group\nresource \"azurerm_management_lock\" \"rg_acr\" {  \n  name       = \"CanNotDelete\"\n  scope      = azurerm_resource_group.rg_acr.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource group can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_resource_group.rg_acr,\n    azurerm_monitor_diagnostic_setting.diag_acr,    \n  ]\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> list of resources in this ACR resource group</p> <p></p>"},{"location":"azure/9-acr/#task-7-validate-acr-resource","title":"Task-7: Validate ACR resource","text":""},{"location":"azure/9-acr/#task-71-log-in-to-registry","title":"Task-7.1: Log in to registry","text":"<pre><code>az acr login --name acr1dev\n</code></pre>"},{"location":"azure/9-acr/#task-72-push-image-to-registry","title":"Task-7.2: Push image to registry","text":"<pre><code>az acr login --name acr1dev\ndocker tag sample/aspnet-api:20230226.1 acr1dev.azurecr.io/sample/aspnet-api:20230226.1\ndocker push acr1dev.azurecr.io/sample/aspnet-api:20230226.1\nor\naz acr push --name acr1dev sample/aspnet-api:20230226.1\n</code></pre>"},{"location":"azure/9-acr/#task-73-pull-image-from-registry","title":"Task-7.3: Pull image from registry","text":"<pre><code>az acr login --name acr1dev\ndocker pull acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre>"},{"location":"azure/9-acr/#task-74-list-container-images","title":"Task-7.4: List container images","text":"<pre><code>az acr login --name acr1dev\naz acr repository list --name acr1dev\n</code></pre> <p>for more information look into the az acr cheat-sheet az-acr-cheat-sheet</p> <p></p>"},{"location":"azure/9-acr/#task-8-restrict-access-using-private-endpoint","title":"Task-8: Restrict Access Using Private Endpoint","text":"<p>To enhance security and limit access to an Azure Container Registry (ACR), you can utilize private endpoints and Azure Private Link. This approach assigns virtual network private IP addresses to the registry endpoints, ensuring that network traffic between clients on the virtual network and the registry's private endpoints traverses a secure path on the Microsoft backbone network, eliminating exposure from the public internet.</p> <p>Additionally, you can configure DNS settings for the registry's private endpoints, allowing clients and services in the network to access the registry using its fully qualified domain name, such as <code>myregistry.azurecr.io</code>.</p> <p>This section guides you through configuring a private endpoint for your ACR using Terraform. Note that this feature is available in the Premium container registry service tier.</p>"},{"location":"azure/9-acr/#task-81-configure-the-private-dns-zone","title":"Task-8.1: Configure the Private DNS Zone","text":"<p>acr.tf: <pre><code># Create private DNS zone for Azure container registry\nresource \"azurerm_private_dns_zone\" \"pdz_acr\" {\n  name                = \"privatelink.azurecr.io\"\n  resource_group_name = azurerm_virtual_network.vnet.resource_group_name\n  tags                = merge(local.default_tags)\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_virtual_network.vnet\n  ]\n}\n</code></pre></p> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the Private DNS zone configuration by navigating to <code>rg-vnet1-dev -&gt; privatelink.azurecr.io -&gt; Overview blade</code>.</p> <p></p>"},{"location":"azure/9-acr/#task-82-create-a-virtual-network-link-association","title":"Task-8.2: Create a Virtual Network Link Association","text":"<p>acr.tf: <pre><code># Create private virtual network link to Virtual Network\nresource \"azurerm_private_dns_zone_virtual_network_link\" \"acr_pdz_vnet_link\" {\n  name                  = \"privatelink_to_${azurerm_virtual_network.vnet.name}\"\n  resource_group_name   = azurerm_resource_group.rg.name\n  virtual_network_id    = azurerm_virtual_network.vnet.id\n  private_dns_zone_name = azurerm_private_dns_zone.pdz_acr.name\n\n  lifecycle {\n    ignore_changes = [\n      tags\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.rg,\n    azurerm_virtual_network.vnet,\n    azurerm_private_dns_zone.pdz_acr\n  ]\n}\n</code></pre></p> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the Virtual network links configuration by navigating to <code>rg-vnet1-dev -&gt; privatelink.azurecr.io -&gt; Virtual network links</code>.</p> <p></p>"},{"location":"azure/9-acr/#task-83-create-a-private-endpoint-using-terraform","title":"Task-8.3: Create a Private Endpoint Using Terraform","text":"<p>acr.tf: <pre><code># Create private endpoint for Azure container registry\nresource \"azurerm_private_endpoint\" \"pe_acr\" {  \n  name                = lower(\"${var.private_endpoint_prefix}-${azurerm_container_registry.acr.name}\")\n  location            = azurerm_container_registry.acr.location\n  resource_group_name = azurerm_container_registry.acr.resource_group_name\n  subnet_id           = azurerm_subnet.jumpbox.id\n  tags                = merge(local.default_tags, var.acr_tags)\n\n  private_service_connection {\n    name                           = \"pe-${azurerm_container_registry.acr.name}\"\n    private_connection_resource_id = azurerm_container_registry.acr.id\n    is_manual_connection           = false\n    subresource_names              = var.pe_acr_subresource_names\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = \"default\" //var.pe_acr_private_dns_zone_group_name\n    private_dns_zone_ids = [azurerm_private_dns_zone.pdz_acr.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n  depends_on = [\n    azurerm_container_registry.acr,\n    azurerm_private_dns_zone.pdz_acr\n  ]\n}\n</code></pre></p> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan &amp; apply</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Confirm the endpoint configuration by navigating to <code>Container registry -&gt; Networking -&gt; Private access</code> \u2014 you will see the new private endpoint details.</p> <p>Navigate to <code>Private endpoint -&gt; Overview</code> to verify the Virtual network/subnet and Network interface.</p> <p>Navigate to <code>Private endpoint -&gt; DNS Configuration</code> to verify the Network Interface and Configuration name.</p> <p>Navigate to <code>Network interface -&gt; Overview</code> to verify the private IP address attached to properties.</p> <p></p>"},{"location":"azure/9-acr/#task-84-validate-private-link-connection-using-nslookup-or-dig","title":"Task-8.4: Validate private link connection using <code>nslookup</code> or <code>dig</code>","text":"<p>To validate the private link connection, connect to the <code>virtual machine</code> you set up in the virtual network. Run a utility such as <code>nslookup</code> or <code>dig</code> to look up the IP address of your registry over the private link. </p> <p>This will ensures that the private link connection is successfully established and allows for the verification of the expected private IP address associated with the registry in the given virtual network.</p> <p>Validate using <code>dig</code> example:</p> <p>Positive test case connecting from internal vm (private access):</p> <p>Run the <code>dig</code> utility to look up the private IP address (<code>10.64.3.5</code>) of your registry over the private link:</p> <pre><code>dig acr1dev.azurecr.io\n</code></pre> <p>output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; acr1dev.azurecr.io\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 31549\n;; flags: qr rd ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 0\n;; WARNING: recursion requested but not available\n\n;; QUESTION SECTION:\n;acr1dev.azurecr.io.                IN      A\n\n;; ANSWER SECTION:\nacr1dev.azurecr.io. 0       IN      CNAME   acr1dev.privatelink.azurecr.io.\nacr1dev.privatelink.azurecr.io. 0 IN A      10.64.3.5\n\n;; Query time: 10 msec\n;; SERVER: 172.30.80.1#53(172.30.80.1)\n;; WHEN: Tue Dec 26 14:57:14 UTC 2023\n;; MSG SIZE  rcvd: 160\n</code></pre> <p>Nagetive test case connecting from external (public access), compare this result with the public IP address in <code>dig</code> output for the same registry over a public endpoint:</p> <p><pre><code>dig acr1dev.azurecr.io\n</code></pre> output</p> <pre><code>; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; acr1dev.azurecr.io\n;; global options: +cmd\nacr1dev.azurecr.io. 0       IN      CNAME   acr1dev.privatelink.azurecr.io.\nacr1dev.privatelink.azurecr.io. 0 IN CNAME  ncus.fe.azcr.io.\nncus.fe.azcr.io.        0       IN      CNAME   ncus-acr-reg.trafficmanager.net.\nncus-acr-reg.trafficmanager.net. 0 IN   CNAME   r1029ncus.northcentralus.cloudapp.azure.com.\nr1029ncus.northcentralus.cloudapp.azure.com. 0 IN A 52.240.241.132\n\n;; Query time: 50 msec\n;; SERVER: 172.29.48.1#53(172.29.48.1)\n;; WHEN: Tue Dec 26 06:56:26 PST 2023\n;; MSG SIZE  rcvd: 380\n</code></pre> <p>Validate using <code>nslookup</code> example:</p> <p>Connecting from internal VM (private access):</p> <p><pre><code>nslookup acr1dev.azurecr.io\n</code></pre> output</p> <pre><code>Server:         172.30.80.1\nAddress:        172.30.80.1#53\n\nNon-authoritative answer:\nacr1dev.azurecr.io  canonical name = acr1dev.privatelink.azurecr.io.\nName:   acr1dev.privatelink.azurecr.io\nAddress: 10.64.3.5\n</code></pre> <p>Connecting from external (public access):</p> <pre><code>nslookup acr1dev.azurecr.io\n</code></pre> <p>output</p> <pre><code>Server:         172.29.48.1   \nAddress:        172.29.48.1#53\n\nNon-authoritative answer:\nacr1dev.azurecr.io  canonical name = acr1dev.privatelink.azurecr.io.\nacr1dev.privatelink.azurecr.io      canonical name = ncus.fe.azcr.io.\nncus.fe.azcr.io canonical name = ncus-acr-reg.trafficmanager.net.\nncus-acr-reg.trafficmanager.net canonical name = r1029ncus.northcentralus.cloudapp.azure.com.\nName:   r1029ncus.northcentralus.cloudapp.azure.com\nAddress: 52.240.241.132\n</code></pre> <p>This process ensures that the private link connection is successfully established and allows expected private IP address associated with our resource in the private virtual network.</p>"},{"location":"azure/9-acr/#references","title":"References","text":"<ul> <li>Microsoft MSDN - Azure Container Registry documentation</li> <li>Microsoft MSDN - Connect privately to an Azure container registry using Azure Private Link</li> <li>Microsoft MSDN - Quickstart: Create an Azure container registry using the Azure portal</li> <li>Microsoft MSDN - Push your first image to your Azure container registry using the Docker CLI</li> <li>Microsoft MSDN - Push and pull Helm charts to an Azure container registry</li> <li>Terraform Registry - azurerm_resource_group</li> <li>Terraform Registry - azurerm_management_lock</li> <li>Terraform Registry - azurerm_container_registry</li> <li>Terraform Registry - azurerm_user_assigned_identity</li> <li>Terraform Registry - azurerm_monitor_diagnostic_setting</li> <li>Terraform Registry - azurerm_private_dns_zone</li> <li>Terraform Registry - azurerm_private_dns_zone_virtual_network_link</li> <li>Terraform Registry - azurerm_private_endpoint</li> <li>Azure-Samples/private-aks-cluster-terraform-devop</li> </ul>"},{"location":"azure/infrastructure/","title":"Chapter 2: Infrastructure as Code (IaC)","text":"<p>In this second chapter you will learn on how to create the infrastructure in azure cloud, the resources and infrastructure created as part of this chapter will be used for deploying your microservices architecture.</p> <p>Here is the list of labs covered in this chapter:</p> <p>1. Introduction    - Understand Naming Conventions     - Create new azure account    - Create new subscription</p> <p>2. Terraform Foundation Part-1 \u2013 Azure setup</p> <ul> <li>Azure DevOps setup<ul> <li>Create DevOps project &amp; git repo</li> <li>Clone the repo </li> <li>Add terraform ignore file</li> </ul> </li> <li>Terraform Management setup \u2013 manual <ul> <li>Create new resource group</li> <li>Create new storage account &amp; container</li> <li>Create new Key vault </li> <li>Create Service Principle </li> <li>Create secrets in Key Vault </li> <li>Setup Access Policy in Key Vault </li> <li>Configure Service Principal Role Assignment </li> </ul> </li> <li>Terraform Management setup \u2013 automation</li> </ul> <p>3. Terraform Foundation Part-2 \u2013 Project setup</p> <ul> <li>Configure Environments/ workspaces</li> <li>Configure Providers setup</li> <li>Create variable definitions </li> <li>Create variable declaration</li> <li>Setup azure naming conventions </li> <li>Create your first resource group using Terraform </li> <li>Terraform execution<ul> <li>tf_commands list</li> <li>Terraform workspace setup</li> <li>Run Terraform plan </li> <li>Run Terraform apply </li> <li>Commit resource group code</li> </ul> </li> </ul> <p>4. Terraform Foundation Part-3 \u2013 Deployment setup</p> <ul> <li>Create new azure service connection </li> <li>Create variable groups </li> <li>Link Key vault in Azure DevOps</li> <li>Create Azure DevOps YAML pipeline for Terraform<ul> <li>Environment setup  </li> <li>Dev env plan stage</li> <li>Dev env Apply state</li> <li>Test env plan stage</li> <li>Test env Apply state</li> <li>Prod env plan stage</li> <li>Prod env Apply state</li> </ul> </li> <li>Create approval policy</li> <li>Create branch policy</li> <li>Configure required permissions for service principle</li> <li>Run the pipeline for the first time</li> </ul> <p>5. Create Log Analytics workspace using Terraform</p> <ul> <li>Create new resource group    </li> <li>Configure variables</li> <li>Create Log Analytics workspace<ul> <li>provider setup  </li> <li>Terraform Plan</li> <li>Terraform Apply</li> </ul> </li> <li>Validate the resource in the portal</li> <li>Lock the resource group</li> </ul> <p>6. Create new Virtual Network using Terraform</p> <ul> <li>Create vnet resource group </li> <li>Configure variables</li> <li>Create Hub vnet<ul> <li>calculate address ranges</li> <li>Create hub vnet subnets</li> <li>gateway subnet</li> <li>application gateway subnet</li> <li>bastion subnet</li> </ul> </li> <li>Validate resource</li> <li>Create Spoke vnet<ul> <li>Create spoke vnet subnets</li> <li>gateway subnet</li> <li>virtual machine subnet</li> <li>database subnet</li> <li>aks subnet</li> </ul> </li> <li>Lock vnet resource group</li> </ul> <p>7. Create Azure Container Registry (ACR) using Terraform </p> <ul> <li>Create ACR resource group </li> <li>Configure variables</li> <li>Create Azure Container Registry</li> <li>Create ACR user assigned identity</li> <li>Create Diagnostics Settings for ACR</li> <li>Validate ACR resource in the portal</li> <li>Lock ACR resource group</li> </ul> <p>8. Create Azure Kubernetes Service (AKS) </p> <ul> <li>Create a new or use existing resource group </li> <li>Configure AKS variables</li> <li>Create Admin cluster Group</li> <li>Create AKS cluster using terraform<ul> <li>privately enabled</li> <li>Managed Identity enabled</li> <li>Auto Scaling enabled</li> <li>configure log analytics workspace</li> <li>Azure CNI networking</li> </ul> </li> <li>Create diagnostics settings for AKS</li> <li>Review AKS Cluster resource in the portal</li> <li>Validate AKS cluster running Kubectl</li> <li>Lock AKS cluster resource group</li> </ul> <p>9. Create Storage Account </p> <ul> <li>Create a new or use existing resource group </li> <li>Configure Storage variables</li> <li>Create Storage Account using terraform<ul> <li>Create a container for bash / ps scripts</li> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> <li>Create diagnostics settings for AKS</li> <li>Review Storage Account resource in the portal</li> </ul> <p>10. Create Azure Bastion Host </p> <ul> <li>Create a new resource group </li> <li>Define and declare bastion host variables</li> <li>Create a subnet for AzureBastionSubnet</li> <li>Create a Azure Bastion host<ul> <li>Allocation method - static</li> <li>Sku - standard</li> <li>IP Based connection - true</li> </ul> </li> <li>Create bastion host diagnostic settings </li> <li>Create bastion host public ip address</li> <li>Create public ip diagnostic settings <ul> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> <li>Review bastion host &amp; public ip in the portal</li> </ul> <p>11. Create Virtual Machine (Jumpbox)</p> <ul> <li>Create a new resource group </li> <li>Create a virtual network / use existing</li> <li>Define and declare jumpbox variables</li> <li>Create a subnet for VM</li> <li>Create a public IP address (not needed)</li> <li>Create a network security group and SSH inbound rule<ul> <li>Diagnostic settings - enable (Pending)</li> </ul> </li> <li>Create a virtual network interface card<ul> <li>Diagnostic settings - enable (Pending)</li> </ul> </li> <li>Connect the network security group to the  network interface</li> <li>Create a storage account for boot diagnostics</li> <li>Create SSH key</li> <li>Create a Linux virtual machine using terraform<ul> <li>Use SSH to connect to virtual machine</li> <li>boot_diagnostics_storage_account (Pending)</li> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> <li>Review Jummpbox in the portal</li> <li>Use SSH to connect to virtual machine</li> <li>Lock the resource group</li> </ul> <p>12. Create Azure Key Vault</p> <ul> <li>Define and declare key vault variables</li> <li>Create a Azure key vault using Terraform<ul> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> <li>Configure key vault access policy</li> <li>Review key vault in the portal</li> <li>Create diagnostic setting for key vault</li> </ul> <p>13. PostgreSQL database Server</p> <ul> <li>Define and declare variables used in PostgreSQL instance</li> <li>Create a new resource group / use existing rg</li> <li>Create a virtual network / use existing vnet</li> <li>Create a subnet for postgreSQL</li> <li>Define a private DNS zone within an Azure DNS </li> <li>Define a private DNS zone vnet link</li> <li>Deploy a PostgreSQL Flexible Server on which the database runs using terraform</li> <li>Instantiate an Azure PostgreSQL database<ul> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> </ul> <p>13. Azure Event Hub (Kafka)</p> <ul> <li>Define and declare variables used in Event Hub</li> <li>Create new resource group / use existing</li> <li>Create storage account for kafka event storage</li> <li>Storage account container for kafka data store</li> <li>Create kafka event hub Namespace</li> <li>Create shared access policy rules at namespace level</li> <li>Create a azure event hub (receiver topic)</li> <li>Create event hubs consumer groups)</li> <li>Create shared access policy rules at even hub level<ul> <li>Initialize terraform</li> <li>Create a terraform execution plan</li> <li>Apply a terraform execution plan</li> <li>Review Kafka event hub in the portal</li> </ul> </li> </ul>"},{"location":"devops/1-devops-overview/","title":"Azure DevOps Overview","text":"<p>Chapter-4: Microservices CI/CD with Azure DevOps</p>"},{"location":"devops/1-devops-overview/#azure-devops-overview","title":"Azure DevOps Overview","text":""},{"location":"devops/1-devops-overview/#introduction","title":"Introduction:","text":"<p>Welcome to the fourth module of our series. In this module, we will look into fundamental concepts and establish a robust CI/CD pipeline using Azure DevOps. Specifically, we will explore Blue-Green deployment, Canary releases, and CI/CD strategies required for microservices on Kubernetes. Additionally, we will leverage tools like Helm and Argo CD. By the end of this module, you will have a clear understanding of how to create a comprehensive CI &amp; CD pipeline architecture for deploying our Microservices into an AKS cluster.</p> <p>Before we look into Azure DevOps CI &amp; CD strategies, let's lay a solid foundation by revisiting some core DevOps concepts.</p>"},{"location":"devops/1-devops-overview/#what-is-cicd","title":"What is CI/CD?","text":"<p>CI/CD stands for Continuous Integration / Continuous Delivery (or Deployment). It is a set of practices and tools used in software development that aims to automate and streamline the process of building, testing, and deploying code changes to multiple environments like dev, test &amp; production.</p> <ul> <li> <p>Continuous Integration (CI) - is the practice of frequently merging code changes from multiple developers or Teams into a central repository, verifying the changes through automated tests, and reporting any issues that arise. This helps to catch problems early on and ensures that the codebase remains stable and functional.</p> </li> <li> <p>Continuous Delivery (CD) - is the practice of automating the entire software release process, from building the code to deploying it to production environments. This involves using tools and processes that enable the release of code changes quickly and reliably, while ensuring that the software remains stable and functional.</p> </li> </ul> <p>CI/CD helps to improve the speed and quality of software development, as well as reduce the risk of errors and downtime. By automating many of the processes involved in software development, teams can focus on creating value for customers rather than spending time on manual tasks.</p> <p>In a microservices architecture, CI/CD refers to the automated process of building, testing, and deploying individual microservices.</p> <p>CI/CD in microservices architecture helps to ensure that each Microservice is tested and validated before it is deployed, reducing the risk of bugs and errors in production. It also enables faster deployment cycles and more frequent updates, as each Microservice can be updated and deployed independently of the others. This approach makes it easier to scale and maintain complex applications, as individual microservices can be modified and updated without affecting the entire application.</p>"},{"location":"devops/1-devops-overview/#goals-of-a-robust-cicd-process","title":"Goals of a robust CI/CD process:","text":"<p>The goals of a robust CI/CD (Continuous Integration/Continuous Delivery) process include:</p> <ul> <li>Faster delivery of software: A robust CI/CD process should enable developers to deliver software faster, with automated processes for building, testing, and deploying code changes. This can help reduce the time to market for new features and bug fixes.</li> <li>Improved quality of software: Automated testing and validation help catch bugs and errors early in the development process, reducing the likelihood of issues in production. This leads to higher quality software that is more reliable and stable.</li> <li>Better collaboration among team members: CI/CD promotes collaboration among developers, testers, and operations teams, with automated feedback loops that help identify and resolve issues quickly.</li> <li>Greater efficiency in development: CI/CD automates many of the time-consuming and repetitive tasks involved in software development, freeing up developers to focus on creating new features and functionality.</li> <li>Increased agility in development: The ability to rapidly build, test, and deploy code changes enables developers to respond quickly to changing business requirements or customer needs.</li> <li>Lower costs: A robust CI/CD process can help reduce the costs associated with manual testing and deployment processes, as well as minimize the risk of downtime or other issues that can be costly for the organization.</li> </ul> <p>A robust CI/CD process enables organizations to develop high-quality software faster and more efficiently, while also promoting collaboration and agility among development teams.</p>"},{"location":"devops/1-devops-overview/#what-is-blue-green-deployment","title":"What is Blue-green deployment?","text":"<p><code>Blue-green deployment</code> is a technique used in software deployment that enables organizations to deploy new versions of an application without downtime. In this deployment strategy, two identical production environments are created, and one environment is designated as the \"blue\" environment, while the other is designated as the \"green\" environment.</p> <p>The current version of the application is running in the blue environment, and the new version is deployed to the green environment. Once the new version has been fully deployed and tested in the green environment, traffic is redirected from the blue environment to the green environment. This is done by switching the routing rules or updating DNS records.</p> <p>At this point, the green environment becomes the production environment, while the blue environment is retained as a backup. Any issues with the new version can be easily addressed by reverting the routing rules back to the blue environment.</p> <p>Blue-green deployment allows organizations to deploy new versions of an application with minimal risk and downtime. This strategy also enables organizations to test the new version of the application in a production-like environment, ensuring that any issues are identified and addressed before the new version is fully deployed.</p> <p>Blue-green deployment is a powerful tool for software deployment that enables organizations to deploy new versions of their applications with confidence and speed, without risking downtime or disrupting the user experience.</p>"},{"location":"devops/1-devops-overview/#what-is-canary-release","title":"What is Canary release?","text":"<p><code>Canary release</code> is a deployment strategy that allows organizations to test new versions of an application in production, with a subset of users, before making it available to everyone. In a canary release, a small percentage of production traffic is redirected to the new version of the application, while the majority of traffic continues to use the current version.</p> <p>Canary release enables organizations to test new features, performance improvements, or other changes in a production-like environment, without exposing all users to potential issues. By gradually increasing the percentage of traffic directed to the new version, organizations can carefully monitor the performance of the new version and quickly roll back if any issues arise.</p> <p>This approach helps organizations to minimize the risk of deploying new versions of an application and ensure that the user experience remains stable and consistent. It also enables organizations to gather feedback from a small group of users before making changes available to everyone.</p> <p>Canary release is a powerful tool for organizations to test and deploy new versions of their applications with minimal risk and disruption, while also enabling them to gather feedback and improve the user experience.   </p>"},{"location":"devops/1-devops-overview/#cicd-strategy-for-microservices-on-kubernetes","title":"CI/CD strategy for microservices on Kubernetes","text":"<p>Deploying small scale applications to Kubernetes can be easily managed by creating deployment YAML manifest files but when it comes to hundreds of Microservices we need better tools and process for these kind of deployments, also one of our goals is to simplify the development, deployment, and scaling of complex applications and to bring the full power of Kubernetes to all projects. Azure DevOps pipeline provides a platform which allows all projects to develop, deploy and scale container-based applications, highly productive, yet flexible environment for developers.</p> <p>The tools we are going to use to meet the goals are:</p> <ul> <li>Helm Charts</li> <li>ArgoCD</li> </ul>"},{"location":"devops/1-devops-overview/#what-helm-charts","title":"What Helm Charts?","text":"<p>One of the key tools we use from the Kubernetes ecosystem is Helm. Helm is a package manager for Kubernetes, which simplifies the process of installing and managing applications on a Kubernetes cluster. A Helm chart is a collection of files that define how an application should be installed and configured on a Kubernetes cluster.</p>"},{"location":"devops/1-devops-overview/#what-is-argo-cd","title":"What is Argo CD?","text":"<p>The second tool we are going to use for deploying our Microservices to AKS is ArgoCD. <code>ArgoCD</code> is a GitOps-based continuous delivery tool for Kubernetes. It is an open-source tool that allows you to declaratively manage and deploy applications on Kubernetes clusters.</p> <p>We will talk more about these tools in the upcoming labs.</p>"},{"location":"devops/1-devops-overview/#deploying-helm-charts-with-argo-cd","title":"Deploying Helm Charts with Argo CD","text":"<p>The approach we will follow here involves deploying Helm Charts with Argo CD.</p> <p>Argo CD provides flexible options for deploying resources using Helm charts. Helm charts are a convenient way to package, version, and deploy applications on Kubernetes. Here's how you can deploy Helm charts with Argo CD:</p> <p>Argo CD allows you to deploy Helm charts directly from a Git repository. Here's how it works:</p> <ol> <li> <p>Repository Setup: First, ensure that your Helm chart is stored in a Git repository. This repository can be public or private, but Argo CD should have access to it.</p> </li> <li> <p>Argo CD Configuration: In your Argo CD application configuration, specify the Git repository URL and the path to the Chart.yaml file. The Chart.yaml file is essential for Argo CD to understand how to deploy the chart.</p> </li> <li> <p>Automatic Detection: When you sync your Argo CD application, it will automatically detect the Helm chart in the specified Git repository. Argo CD understands the structure of Helm charts and can render them during deployment.</p> </li> <li> <p>Customization: You can customize the deployment by providing a values.yaml file or specifying values directly in the Argo CD application configuration. This allows you to tailor the Helm chart to your specific environment and requirements.</p> </li> </ol> <p>Deploying Helm charts with Argo CD provides a powerful way to manage complex Kubernetes applications. It simplifies the deployment process and allows for fine-grained customization to fit your specific needs.</p>"},{"location":"devops/1-devops-overview/#azure-devops-pipeline","title":"Azure DevOps Pipeline","text":"<p>Azure DevOps Pipeline provides end-to-end CI/CD capabilities for building, testing, and deploying applications to various platforms, including Azure, AWS, and on-premises environments. It allows developers and DevOps teams to create, manage, and run automated build and release pipelines, and to integrate with various tools and services, such as GitHub, Jenkins, Docker, Kubernetes, and more.</p> <p>Azure DevOps Pipeline provides a unified experience for continuous integration and continuous delivery, allowing teams to collaborate and automate the software delivery process, from source code management to production deployment. With Azure DevOps Pipeline, you can automate the building, testing, and deployment of your applications, enabling faster and more reliable releases.</p>"},{"location":"devops/1-devops-overview/#overall-ci-cd-architecture","title":"Overall CI &amp; CD Architecture","text":"<p>The following diagram shows our end-to-end CI/CD process for deploying Microservices in AKS.</p> <p></p> <p>Reference Architecture diagram from MSDN documentation.</p>"},{"location":"devops/1-devops-overview/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/architecture/microservices/ci-cd-kubernetes - Build a CI/CD pipeline for microservices on Kubernetes with Azure DevOps and Helm</li> </ul>"},{"location":"devops/pipelines/1-service-connections/","title":"Create new service connections &amp; variable groups in Azure DevOps","text":""},{"location":"devops/pipelines/1-service-connections/#introduction","title":"Introduction","text":"<p>The purpose of creating new service connections in Azure DevOps is to allow Azure DevOps and Azure DevOps Pipeline to interact with Azure core services that are required for our application deployment activities. By creating these service connections, you can manage the integration between Azure DevOps and connected azure core services, such as:</p> <ul> <li> <p>Azure Resource Manager: Allows you to manage your Azure resources, such as virtual machines and databases.</p> </li> <li> <p>Docker Registry: Allows you to store, manage, and distribute Docker images.</p> </li> <li> <p>Kubernetes: Allows you to manage your containers and applications running in a Kubernetes cluster.</p> </li> </ul> <p>Key purposes of a service connection for Azure subscription access include:</p> <ol> <li> <p>Deployment: Service connections enable automated deployments of applications and infrastructure to Azure resources using Azure DevOps pipelines. This ensures consistency and repeatability in the deployment process.</p> </li> <li> <p>Integration: They allow seamless integration between Azure DevOps and Azure services, such as Azure Kubernetes Service (AKS), Azure Functions, Azure App Service, and more. This integration simplifies CI/CD workflows.</p> </li> <li> <p>Resource Management: Service connections facilitate the management of Azure resources, including provisioning, scaling, and configuration changes, directly from Azure DevOps pipelines.</p> </li> <li> <p>Security: They enhance security by securely storing credentials and access tokens, reducing the need to expose sensitive information within pipeline scripts.</p> </li> <li> <p>Auditing: Service connections provide an auditable trail of actions performed on Azure resources, helping teams track changes and troubleshoot issues effectively.</p> </li> </ol> <p>In summary, a service connection for Azure subscription access serves as a bridge between Azure DevOps and Azure resources, enabling streamlined and secure DevOps processes.</p>"},{"location":"devops/pipelines/1-service-connections/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>DevSecOps Engineer</code>, you have been asked to create new service connections in azure DevOps before start creating any azure devops pipelines to ensure that the right resources and services are available to your pipelines.</p>"},{"location":"devops/pipelines/1-service-connections/#objective","title":"Objective","text":"<p>In this lab you will create and learn how to create new azure DevOps service connections, in the background these new service connections will create the new Service Principles in the Azure AD.</p> <ul> <li>Task-1: Create new service connection for Azure subscription Access</li> <li>Task-2: Create new service connection for Azure container registry (ACR)</li> <li>Task-3: Create new service connection for Azure Kubernetes services (AKS)</li> <li>Task-4: Create new Library - variable group</li> </ul>"},{"location":"devops/pipelines/1-service-connections/#architecture-diagram","title":"Architecture diagram","text":"<p>This diagram depicts the end to end connection between Azure DevOps and external services or resources such as Azure, Docker registry and AKS.</p> <p></p>"},{"location":"devops/pipelines/1-service-connections/#implementation-details","title":"Implementation Details","text":"<p>In this module, I will demonstrate how to create service connections for specific Azure resources. Please note that the same process can be applied to other resources as well.</p>"},{"location":"devops/pipelines/1-service-connections/#task-1-create-new-service-connection-for-azure-subscription-access","title":"Task-1: Create new service connection for Azure subscription Access","text":"<p>To create a new service connection in Azure DevOps, follow these steps:</p> <ol> <li>Go to Project settings in Azure DevOps.</li> <li>In the Project Settings, select the \"Service connections\" section.</li> <li>Click the \"New Service connection\" button.    </li> <li>Select the Azure Resource Manager.    </li> <li>Select the Service principal (manual).   </li> <li>Provide the necessary information for the service connection, such as the name, description, and access credentials.    </li> <li>Click the \"Verify connection\" button to test the connection.    </li> <li>If the connection is successful, click the \"Save\" button.</li> </ol> <p>Once you've created a service connection, you can use it in your pipelines for tasks that need to access the connected service.</p> <p>Important</p> <p>Here we've selected the existing Terraform service principle which was created as part of the Terraform Foundation Labs.</p>"},{"location":"devops/pipelines/1-service-connections/#task-2-create-new-service-connection-for-azure-container-registry-acr","title":"Task-2: Create new service connection for Azure container registry (ACR)","text":"<p>To create a new service connection for an Azure Container Registry (ACR) in Azure DevOps, follow these steps:</p> <ol> <li>Go to Project settings in Azure DevOps.</li> <li>In the Project Settings, select the \"Service connections\" section.</li> <li>Click the \"New Service connection\" button.</li> <li>Select \"Docker Registry\" as the type of service connection. </li> <li>Select <code>Other</code> or \"Azure Container Registry\" as registry type.</li> <li>In the \"Azure Container Registry\" section, select your ACR from the dropdown list.</li> <li>Provide a name and description for the service connection. </li> <li>if you select <code>Others</code> then collect the details from here in Azure ACR </li> <li>Click the \"Save\" button to test the connection and create the service connection.</li> </ol> <p>Once you've created the service connection, you can use it in your pipelines to perform tasks that require access to your ACR, such as pushing and pulling Docker images.</p>"},{"location":"devops/pipelines/1-service-connections/#task-3-create-new-service-connection-for-azure-kubernetes-services-aks","title":"Task-3: Create new service connection for Azure Kubernetes services (AKS)","text":"<p>To create a new service connection for AKS in Azure DevOps, you can follow these steps:</p> <ol> <li>Go to your Azure DevOps organization and navigate to the <code>Project Settings</code> page.</li> <li>In the Project Settings page, select the <code>Service Connections</code> section.</li> <li>Click on the <code>New Service Connection</code> button.</li> <li>In the \"New Service Connection\" menu, select <code>Kubernetes</code> as the connection type. </li> <li>Fill in the connection details, such as the Connection name, AKS cluster name, and the Kubernetes API server URL. </li> <li>Click on <code>Verify connection</code> to ensure that the details you have entered are correct.</li> <li>Finally, click on the <code>Create</code> button to create the new service connection for your AKS cluster in Azure DevOps.</li> </ol> <p>Once you have created the service connection, you can use it to automate tasks such as deploying applications to your AKS cluster, managing cluster configurations, and monitoring cluster health.</p> <pre><code>Name: aks-cluster1-dev\nDescription: use this connection to deploy applications in sample namespace in `aks-cluster1-dev` AKS Cluster.\n</code></pre> <p>Here is the list of new service connections you've created so far.</p> <p></p>"},{"location":"devops/pipelines/1-service-connections/#step-4-create-new-library-variable-group","title":"Step-4: Create new Library - variable group","text":"<p>Follow these steps to create a new Library Variable Group in Azure DevOps:</p> <ul> <li>Go to your Azure DevOps project and navigate to the project settings page.</li> <li>In the settings page, select the <code>Pipelines</code> option and then click on \"Variable groups\".</li> <li>Click the <code>New</code> button to create a new variable group.</li> <li>Give your new variable group a name and description.</li> <li>Add variables to your group by clicking the <code>Add</code> button and specifying a name, value, and any other relevant details.</li> <li>Once you have added all of the variables you need, click the <code>Create</code> button to save your new variable group.</li> </ul> <p></p> <pre><code>Azure DevOps  / Keesari / Microservices / Pipelines / Library\n</code></pre> <p></p> <pre><code>Name: microservices-dev-subscription-connections\nDescription: this group variables will be used in microservices application pipelines to connect to azure service resources\n</code></pre> <p>Your new variable group will now be available for use in your Azure DevOps pipelines and releases. You can also share the variable group with other projects or teams in your organization by granting them access to it.</p> <p>Creating a new Library Variable Group for Service Connections or any other variables in Azure DevOps before creating pipelines offers several benefits:</p> <ol> <li> <p>Centralized Management: Variable groups allow you to centralize the management of variables that are shared across multiple pipelines. When you create variable groups for service connections, you keep all the relevant variables in one place, making it easier to update and maintain them.</p> </li> <li> <p>Reusability: Variables defined in a variable group can be reused across different pipelines. This eliminates the need to redefine the same variables in multiple places, reducing redundancy and ensuring consistency.</p> </li> <li> <p>Security: Variable groups allow you to secure sensitive information, such as secrets and connection strings, by defining them as secret variables. These values are securely stored and can only be accessed by authorized users or services. We can also integrate Azure Key vault service here.</p> </li> <li> <p>Versioning: Variable groups support versioning, enabling you to maintain different versions of the same variable group. This is useful when you need to make changes to variables while preserving the existing definitions for running pipelines.</p> </li> <li> <p>Scalability: As your DevOps environment grows, managing variables in a centralized manner becomes crucial. Variable groups can scale with your organization, ensuring that variables are organized and accessible as needed.</p> </li> <li> <p>Consistency: Using variable groups for service connections ensures that the same set of variables is available consistently across all your pipelines. This promotes standardization and helps prevent configuration errors.</p> </li> <li> <p>Ease of Updates: When you need to update variables related to service connections (e.g., API keys, access tokens, or connection strings), you can update them in the variable group once, and the changes will automatically propagate to all pipelines that use that group.</p> </li> <li> <p>Audit Trail: Variable groups provide an audit trail of changes, making it easier to track who made modifications to variables and when those changes occurred.</p> </li> <li> <p>Role-Based Access Control: Azure DevOps allows you to control who has access to view or edit variable groups. This enables role-based access control to ensure that only authorized users can manage sensitive configuration information.</p> </li> <li> <p>Simplified Pipeline Configuration: Using variable groups reduces the complexity of pipeline configurations. Instead of defining variables directly within each pipeline, you reference the variables from the shared variable group, making pipeline definitions cleaner and more concise.</p> </li> </ol> <p>In summary, creating a Library Variable Group for Service Connections in Azure DevOps is a best practice that enhances the organization, security, and maintainability of your pipelines, promoting efficiency and consistency in your DevOps workflows.</p>"},{"location":"devops/pipelines/1-service-connections/#references","title":"References:","text":"<ul> <li>Microsoft MSDN - Connect to Microsoft Azure with an ARM service connection</li> </ul>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/","title":"Create Azure DevOps pipeline - for .NET Core Web API","text":""},{"location":"devops/pipelines/2-pipeline-aspnet-api/#introduction","title":"Introduction:","text":"<p>In this lab we are going to create an azure DevOps pipeline for our first Microservice built using .NET Core Web API. This pipeline will  automate the continuous integration and continuous deployment (CI/CD) of build, test, and deployment process of a .NET Core Web API. </p> <p>The Build Pipeline we are creating here is designed exclusively for containerized microservices, specifically those based on\u00a0.NET Core Web API. This pipeline is tailored for seamless deployment on Azure Kubernetes Services (AKS).</p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>DevOps Engineer</code>, you've been asked to create a new Azure DevOps pipeline using YAML for .NET Core Web API, this pipeline should provides a flexible way to automate the build and deployment process of .NET Core Web API applications. It should also allow developers to focus on writing code while the pipeline takes care of the rest.</p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure DevOps account  </li> <li>Source code repository</li> <li>Service connections</li> <li>Docker image</li> </ul>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#implementation-details","title":"Implementation details","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step-1: Create the pipeline</li> <li>Step-2: Setup environment</li> <li>Step-3: Create Variables and Variable Groups</li> <li>Step-4: Build and push an image to container registry</li> <li>Step-5: Run your tests</li> <li>Step-6: Collect code coverage</li> <li>Step-7: Update the image tag in ArgoCD or Helm chart</li> <li>Step-8: Run the pipeline</li> </ul>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-1-create-the-pipeline","title":"Step-1: Create the pipeline","text":"<p>Important</p> <p>Before following the create pipeline wizard instructions, let's quickly create a new file called <code>azure-pipelines.yaml</code> in the root folder of our first Microservice and commit this change.</p> <p>Follow these instructions for creating a new Azure DevOps pipeline:</p> <ul> <li>Open your Azure DevOps account and navigate to the project where you want to create the pipeline.</li> <li>Click on the <code>Pipelines</code> menu and then click on the <code>Create pipeline</code> button.   </li> <li>Select <code>Azure repos git</code> YAML    </li> <li>Select the source code repository where your .NET Core Web API code is hosted. </li> <li>Select <code>Select an existing YAML file</code> </li> <li>Select the branch that you want to use for the pipeline. You can either choose the main branch or a specific branch.</li> <li>Choose the template for your pipeline. in this case we are going to use choose an existing template.</li> <li>Click on <code>continue</code> button   </li> <li>Click on the Save from <code>Run</code> dropdown list.   </li> <li>Rename the pipeline.   </li> </ul> <p>That's it! You now have an Azure DevOps pipeline for your .NET Core Web API. your can configure the pipeline according to your needs. You can customize the pipeline by adding or removing stages, jobs, and tasks.</p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-2-setup-environment","title":"Step-2: Setup environment","text":"<p>As part of this step we are going to add or create few configuration changes in the current pipeline.</p> <p><pre><code>trigger:\n  branches:\n    include:\n    - refs/heads/develop\n  paths:\n   include:\n     - aspnet-api/*\nresources:\n  repositories:\n  - repository: self\n    type: git\n    ref: refs/heads/$(Build.SourceBranchName)\n</code></pre> The <code>trigger</code> section specifies that the pipeline should be triggered when changes are made to the <code>develop</code> branch, and specifically to any files in the <code>aspnet-api/</code> directory.</p> <p>The <code>resources</code> section defines a repository resource named \"self\" that points to the current repository, and uses the branch name from the trigger to determine which branch to build from.</p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-3-create-variables-and-variable-groups","title":"Step-3: Create Variables and Variable Groups","text":"<pre><code>variables:\n- group: microservices-dev-subscription-connections\n- name: appName\n  value : 'aspnet-api'\n- name: dockerfilePath\n  value : '$(Build.SourcesDirectory)/$(appName)/Dockerfile'\n- name: imageName\n  value: sample/$(appName)\n- name: system_accesstoken\n  value : $(System.AccessToken)\n</code></pre> <ul> <li>group: <code>microservices-dev-subscription-connections</code> This is the name of a group in an Azure DevOps project, containing resources related to microservices development.</li> <li><code>name: appName</code> This variable stores the name of the application being developed. In this case, the name is <code>aspnet-api</code>.</li> <li><code>name: dockerfilePath</code> This variable stores the path to the Dockerfile used to build the application's Docker image.</li> <li><code>name: imageName</code> This variable stores the name of the Docker image that will be built. </li> <li><code>name: system_accesstoken</code> This variable is used to store a token that allows the build or release pipeline to access system resources or perform actions on behalf of the user. The value is obtained using the <code>$(System.AccessToken)</code> variable.</li> </ul>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-4-build-and-push-an-image-to-container-registry","title":"Step-4: Build and push an image to container registry","text":"<p>Here is the YAML pipeline definition for building and pushing a Docker image to a container registry:</p> <p><pre><code>stages:\n- stage: Build\n  displayName: Build and push stage\n  jobs:\n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - checkout: self\n      persistCredentials: true\n    #  Build and push an image to container registry\n    - task: Docker@2\n      displayName: Login to ACR\n      inputs:\n        command: login\n        containerRegistry: $(azure-container-registry)\n\n    - task: Docker@2\n      displayName: Build an image for $(appName)\n      inputs:\n        command: build\n        buildContext: '$(Build.SourcesDirectory)/$(appName)'\n        repository: $(imageName)\n        dockerfile: $(dockerfilePath)\n        containerRegistry: $(azure-container-registry)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - task: Docker@2\n      displayName: Push an image to container registry\n      inputs:\n        command: push\n        repository: $(imageName)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - powershell: |\n        Set-Content -Path \"$(Build.ArtifactStagingDirectory)/$(appName)\" -Value (docker inspect $(container-registery-loginname)/$(imageName):latest -f '{{ .Id }}')\n      displayName: Saving container hashes\n</code></pre> Here's what each section of the YAML code represents:</p> <ul> <li>stages: This defines the stages of the pipeline. In this case, there is only one stage, called \"Build\", which is responsible for building and pushing a Docker image to a container registry.</li> <li>jobs: This defines the jobs that will be executed as part of the \"Build\" stage. In this case, there is only one job, called \"Build\".</li> <li>pool: This specifies the agent pool that the job will run on. In this case, the job will run on an Ubuntu-based agent with the latest available image.</li> <li>steps: This section lists the individual steps that will be executed as part of the job. The steps are executed in the order that they appear in the list.</li> </ul> <p>Verify the ACR</p> <pre><code>az account set -s \"anji.keesari\"\naz acr login --name acr1dev\n</code></pre> <p><pre><code>az acr repository list --name acr1dev --output table\n</code></pre> output</p> <pre><code>mcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\nsample/react-app\n</code></pre> <pre><code>az acr repository show-tags --name acr1dev --repository sample/aspnet-api --output table\n</code></pre> <p>output <pre><code>Result\n-----------\n20230220.1\n20230226.1\n20230323.7\n20230323.8\nlatest\n</code></pre></p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-5-run-your-tests","title":"Step-5: Run your\u00a0tests","text":"<ul> <li>The primary purpose of this task is to automatically execute various tests on your codebase to ensure its quality and functionality.</li> <li>It's an essential part of the Continuous Integration (CI) and Continuous Deployment (CD) pipeline, where code changes are tested automatically to identify and resolve issues early in the development process.</li> </ul> <pre><code>use this task for running test\n</code></pre>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-6-collect-code-coverage","title":"Step-6: Collect code\u00a0coverage","text":"<ul> <li>After running your automated tests, this task collects information about which parts of your code were executed during the testing process.</li> <li>It uses a code coverage tool specific to your programming language or platform.\u00a0</li> <li>These tools instrument your code to track which lines, functions, or branches are executed during testing.</li> </ul> <pre><code>use this task for running test\n</code></pre>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-7-update-the-image-tag-in-argocd-or-helm-chart","title":"Step-7: Update the image tag in ArgoCD or Helm chart","text":"<p>This is the last step of the pipeline and it is very important part of this pipeline, this step will update the latest docker image tag in deployment.yaml manifest file in the Helm chart or ArgoCD, depending on how your microservices are getting deployed to AKS, Helm chart and ArgoCD manifests will be managed in separate git repos; but in our case here we are keeping it in the same repo for the simplicity.</p> <p>Use the follow PowerShell script to update the image tag in ArgoCD or Helm chart:</p> <pre><code># Update the argocd or helmcharts project deployment.yaml file\n\n- powershell: |\n    git config --global user.email \"devopsagent@example.com\"\n    git config --global user.name \"$(Build.DefinitionName)\"\n    git checkout --force develop\n    # git pull\n\n    $deploymentFile = \"$(Build.SourcesDirectory)/helmcharts/microservices-chart/templates/$(appName)/deployment.yaml\"\n\n    $imageLine = Select-String -Path $deploymentFile -Pattern $(imageName)\n    $tag=$imageLine.ToString().Split(\":\")[4]\n    echo \"tag to be replaced\"\n    $tag\n\n    (Get-Content $deploymentFile -Encoding UTF8) -replace $tag , $(Build.BuildNumber) | Set-Content $deploymentFile\n\n    git add .\n    git commit -a -m \"helmcharts deployment updated by $(Build.Repository.Name) $(Build.SourceVersionMessage) - $(Build.BuildNumber)\"\n    git push origin HEAD:refs/heads/develop\n\n  displayName: Updating helmcharts project\n  enabled: true\n</code></pre> <p>This PowerShell script performs the following actions:</p> <ul> <li>Sets the Git user name and email to the build definition name and a dummy email address.</li> <li>Checks out the develop branch.</li> <li>Defines the path to the deployment file to be updated.</li> <li>Searches for the line in the deployment file that contains the current image tag.</li> <li>Extracts the current tag value from the line and stores it in the $tag variable.</li> <li>Replaces the current tag value with the build number in the deployment file.</li> <li>Adds the modified file to the Git staging area.</li> <li>Commits the changes to Git with a message containing the repository name, source version message, and build number.</li> <li>Pushes the changes to the develop branch.</li> </ul> <p>Here is the complete contents of <code>azure-pipelines.yaml</code> file</p> azure-pipelines.yml<pre><code>trigger:\n  branches:\n    include:\n    - refs/heads/develop\n  paths:\n   include:\n     - aspnet-api/*\nresources:\n  repositories:\n  - repository: self\n    type: git\n    ref: refs/heads/$(Build.SourceBranchName)\n\nvariables:\n- group: microservices-dev-subscription-connections\n- name: appName\n  value : 'aspnet-api'\n- name: dockerfilePath\n  value : '$(Build.SourcesDirectory)/$(appName)/Dockerfile'\n- name: imageName\n  value: sample/$(appName)\n- name: system_accesstoken\n  value : $(System.AccessToken)\n\nstages:\n- stage: Build\n  displayName: Build and push stage\n  jobs:\n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - checkout: self\n      persistCredentials: true\n    #  Build and push an image to container registry\n    - task: Docker@2\n      displayName: Login to ACR\n      inputs:\n        command: login\n        containerRegistry: $(azure-container-registry)\n\n    - task: Docker@2\n      displayName: Build an image for $(appName)\n      inputs:\n        command: build\n        buildContext: '$(Build.SourcesDirectory)/$(appName)'\n        repository: $(imageName)\n        dockerfile: $(dockerfilePath)\n        containerRegistry: $(azure-container-registry)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - task: Docker@2\n      displayName: Push an image to container registry\n      inputs:\n        command: push\n        repository: $(imageName)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - powershell: |\n        Set-Content -Path \"$(Build.ArtifactStagingDirectory)/$(appName)\" -Value (docker inspect $(container-registery-loginname)/$(imageName):latest -f '{{ .Id }}')\n      displayName: Saving container hashes\n\n    # Update the argocd or helmcharts project deployment.yaml file\n\n    - powershell: |\n        git config --global user.email \"devopsagent@example.com\"\n        git config --global user.name \"$(Build.DefinitionName)\"\n        git checkout --force develop\n        # git pull\n\n        $deploymentFile = \"$(Build.SourcesDirectory)/helmcharts/microservices-chart/templates/$(appName)/deployment.yaml\"\n\n        $imageLine = Select-String -Path $deploymentFile -Pattern $(imageName)\n        $tag=$imageLine.ToString().Split(\":\")[4]\n        echo \"tag to be replaced\"\n        $tag\n\n        (Get-Content $deploymentFile -Encoding UTF8) -replace $tag , $(Build.BuildNumber) | Set-Content $deploymentFile\n\n        git add .\n        git commit -a -m \"helmcharts deployment updated by $(Build.Repository.Name) $(Build.SourceVersionMessage) - $(Build.BuildNumber)\"\n        git push origin HEAD:refs/heads/develop\n\n      displayName: Updating helmcharts project\n      enabled: true\n</code></pre>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#step-8-run-the-pipeline","title":"Step-8: Run the pipeline","text":"<p>trigger <code>Run pipeline</code></p> <p></p> <p>Build and push an image to container registry </p> <p>Update the image tag in Helm chart </p>"},{"location":"devops/pipelines/2-pipeline-aspnet-api/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/devops/pipelines/ecosystems/dotnet-core?view=azure-devops&amp;tabs=dotnetfive</li> </ul>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/","title":"Create your second pipeline - for ASP.NET Core MVC Project","text":""},{"location":"devops/pipelines/3-pipeline-aspnet-app/#introduction","title":"Introduction:","text":"<p>In this lab we are going to create an azure DevOps pipeline for our second Microservice built using ASP.NET Core MVC. This pipeline will  automate the continuous integration and continuous deployment (CI/CD) of build, test, and deployment process of a ASP.NET Core MVC project. </p>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>DevOps Engineer</code>, you've been asked to create a new Azure DevOps pipeline using YAML for ASP.NET Core MVC project, this pipeline should provides a way to automate the build and deployment process of ASP.NET Core MVC Project. It should also allow developers to focus on writing code while the pipeline takes care of the rest.</p>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure DevOps account  </li> <li>ASP.NET Core MVC Project</li> <li>Service connections</li> <li>Docker image</li> </ul>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#implementation-details","title":"Implementation details","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step-1: Create the pipeline</li> <li>Step-2: Setup environment</li> <li>Step-3: Create Variables and Variable Groups</li> <li>Step-4: Build and push an image to container registry</li> <li>Step-5: Run your tests</li> <li>Step-6: Collect code coverage</li> <li>Step-7: Update the image tag in ArgoCD or Helm chart</li> <li>Step-8: Run the pipeline</li> </ul>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-1-create-the-pipeline","title":"Step-1: Create the pipeline","text":"<p>Important</p> <p>Before following the create pipeline wizard instructions, let's quickly create a new file called <code>azure-pipelines.yaml</code> in the root folder of our second Microservice (aspnet-app) and commit this change.</p> <p>Follow these instructions for creating a new Azure DevOps pipeline:</p> <ul> <li>Open your Azure DevOps account and navigate to the project where you want to create the pipeline.</li> <li>Click on the <code>Pipelines</code> menu and then click on the <code>Create pipeline</code> button.</li> <li>Select <code>Azure repos git</code> YAML </li> <li>Select the source code repository where your .NET Core Web API code is hosted. </li> <li>Select <code>Select an existing YAML file</code> </li> <li>Select the branch that you want to use for the pipeline. You can either choose the main branch or a specific branch.</li> <li>Choose the template for your pipeline. in this case we are going to use choose an existing template.</li> <li>Click on <code>continue</code> button</li> <li>Click on the Save from <code>Run</code> dropdown list.</li> <li>Rename the pipeline.</li> </ul> <p>That's it! You now have an Azure DevOps pipeline for your ASP.NET Core MVC Project. your can configure the pipeline according to your needs. You can customize the pipeline by adding or removing stages, jobs, and tasks.</p>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-2-setup-environment","title":"Step-2: Setup environment","text":"<p>As part of this step we are going to add or create few configuration changes in the current pipeline.</p> <pre><code>trigger:\n  branches:\n    include:\n    - refs/heads/develop\n  paths:\n   include:\n     - aspnet-app/*\nresources:\n  repositories:\n  - repository: self\n    type: git\n    ref: refs/heads/$(Build.SourceBranchName)\n</code></pre>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-3-create-variables-and-variable-groups","title":"Step-3: Create Variables and Variable Groups","text":"<pre><code>variables:\n- group: microservices-dev-subscription-connections\n- name: appName\n  value : 'aspnet-app'\n- name: dockerfilePath\n  value : '$(Build.SourcesDirectory)/$(appName)/Dockerfile'\n- name: imageName\n  value: sample/$(appName)\n- name: system_accesstoken\n  value : $(System.AccessToken)\n</code></pre>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-4-build-and-push-an-image-to-container-registry","title":"Step-4: Build and push an image to container registry","text":"<p>Here is the YAML pipeline definition for building and pushing a Docker image to a container registry:</p> <p><pre><code>stages:\n- stage: Build\n  displayName: Build and push stage\n  jobs:\n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - checkout: self\n      persistCredentials: true\n    #  Build and push an image to container registry\n    - task: Docker@2\n      displayName: Login to ACR\n      inputs:\n        command: login\n        containerRegistry: $(azure-container-registry)\n\n    - task: Docker@2\n      displayName: Build an image for $(appName)\n      inputs:\n        command: build\n        buildContext: '$(Build.SourcesDirectory)/$(appName)'\n        repository: $(imageName)\n        dockerfile: $(dockerfilePath)\n        containerRegistry: $(azure-container-registry)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - task: Docker@2\n      displayName: Push an image to container registry\n      inputs:\n        command: push\n        repository: $(imageName)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - powershell: |\n        Set-Content -Path \"$(Build.ArtifactStagingDirectory)/$(appName)\" -Value (docker inspect $(container-registery-loginname)/$(imageName):latest -f '{{ .Id }}')\n      displayName: Saving container hashes\n</code></pre> Here's what each section of the YAML code represents:</p> <p>Verify the ACR</p> <pre><code>az account set -s \"anji.keesari\"\naz acr login --name acr1dev\n</code></pre> <p><pre><code>az acr repository list --name acr1dev --output table\n</code></pre> output</p> <pre><code>sample/aspnet-api\nsample/aspnet-app\nsample/react-app\n</code></pre> <pre><code>az acr repository show-tags --name acr1dev --repository sample/aspnet-app --output table\n</code></pre> <p>output <pre><code>Result\n-----------\n20230220.1\nlatest\n</code></pre></p>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-7-update-the-image-tag-in-argocd-or-helm-chart","title":"Step-7: Update the image tag in ArgoCD or Helm chart","text":"<p>This is the last step of the pipeline and it is very important part of this pipeline, this step will update the latest docker image tag in deployment.yaml manifest file in the Helm chart or ArgoCD, depending on how your microservices are getting deployed to AKS, Helm chart and ArgoCD manifests will be managed in separate git repos; but in our case here we are keeping it in the same repo for the simplicity.</p> <p>Use the follow PowerShell script to update the image tag in ArgoCD or Helm chart:</p> <pre><code># Update the argocd or helmcharts project deployment.yaml file\n\n- powershell: |\n    git config --global user.email \"devopsagent@example.com\"\n    git config --global user.name \"$(Build.DefinitionName)\"\n    git checkout --force develop\n    # git pull\n\n    $deploymentFile = \"$(Build.SourcesDirectory)/helmcharts/microservices-chart/templates/$(appName)/deployment.yaml\"\n\n    $imageLine = Select-String -Path $deploymentFile -Pattern $(imageName)\n    $tag=$imageLine.ToString().Split(\":\")[4]\n    echo \"tag to be replaced\"\n    $tag\n\n    (Get-Content $deploymentFile -Encoding UTF8) -replace $tag , $(Build.BuildNumber) | Set-Content $deploymentFile\n\n    git add .\n    git commit -a -m \"helmcharts deployment updated by $(Build.Repository.Name) $(Build.SourceVersionMessage) - $(Build.BuildNumber)\"\n    git push origin HEAD:refs/heads/develop\n\n  displayName: Updating helmcharts project\n  enabled: true\n</code></pre> <p>Here is the complete contents of <code>azure-pipelines.yaml</code> file</p> azure-pipelines.yml<pre><code>trigger:\n  branches:\n    include:\n    - refs/heads/develop\n  paths:\n   include:\n     - aspnet-app/*\nresources:\n  repositories:\n  - repository: self\n    type: git\n    ref: refs/heads/$(Build.SourceBranchName)\n\nvariables:\n- group: microservices-dev-subscription-connections\n- name: appName\n  value : 'aspnet-app'\n- name: dockerfilePath\n  value : '$(Build.SourcesDirectory)/$(appName)/Dockerfile'\n- name: imageName\n  value: sample/$(appName)\n- name: system_accesstoken\n  value : $(System.AccessToken)\n\nstages:\n- stage: Build\n  displayName: Build and push stage\n  jobs:\n  - job: Build\n    displayName: Build\n    pool:\n      vmImage: 'ubuntu-latest'\n    steps:\n    - checkout: self\n      persistCredentials: true\n    #  Build and push an image to container registry\n    - task: Docker@2\n      displayName: Login to ACR\n      inputs:\n        command: login\n        containerRegistry: $(azure-container-registry)\n\n    - task: Docker@2\n      displayName: Build an image for $(appName)\n      inputs:\n        command: build\n        buildContext: '$(Build.SourcesDirectory)/$(appName)'\n        repository: $(imageName)\n        dockerfile: $(dockerfilePath)\n        containerRegistry: $(azure-container-registry)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - task: Docker@2\n      displayName: Push an image to container registry\n      inputs:\n        command: push\n        repository: $(imageName)\n        tags: |\n          latest\n          $(build.buildNumber)\n\n    - powershell: |\n        Set-Content -Path \"$(Build.ArtifactStagingDirectory)/$(appName)\" -Value (docker inspect $(container-registery-loginname)/$(imageName):latest -f '{{ .Id }}')\n      displayName: Saving container hashes\n\n    # Update the argocd or helmcharts project deployment.yaml file\n\n    - powershell: |\n        git config --global user.email \"devopsagent@example.com\"\n        git config --global user.name \"$(Build.DefinitionName)\"\n        git checkout --force develop\n        # git pull\n\n        $deploymentFile = \"$(Build.SourcesDirectory)/helmcharts/microservices-chart/templates/$(appName)/deployment.yaml\"\n\n        $imageLine = Select-String -Path $deploymentFile -Pattern $(imageName)\n        $tag=$imageLine.ToString().Split(\":\")[4]\n        echo \"tag to be replaced\"\n        $tag\n\n        (Get-Content $deploymentFile -Encoding UTF8) -replace $tag , $(Build.BuildNumber) | Set-Content $deploymentFile\n\n        git add .\n        git commit -a -m \"helmcharts deployment updated by $(Build.Repository.Name) $(Build.SourceVersionMessage) - $(Build.BuildNumber)\"\n        git push origin HEAD:refs/heads/develop\n\n      displayName: Updating helmcharts project\n      enabled: true\n</code></pre>"},{"location":"devops/pipelines/3-pipeline-aspnet-app/#step-8-run-the-pipeline","title":"Step-8: Run the pipeline","text":"<p>trigger <code>Run pipeline</code></p> <p></p> <p>Build and push an image to container registry </p> <p>Update the image tag in Helm chart </p>"},{"location":"gettingstarted/about-author/","title":"About the Author","text":"<p>Anji Keesari is a software engineer and cloud architect with over 20 years of experience in the technology industry. He has been involved in numerous projects related to cloud computing, microservices architecture, and technologies such as Kubernetes, Terraform, and containers.</p> <p>Anji has hands-on experience deploying and managing Kubernetes clusters in production environments. He is also proficient in tools like ArgoCD and Helm, utilizing them to deploy microservices applications on Kubernetes. Anji also has extensive knowledge of containers and containerization technologies, including Docker and container orchestration tools such as Kubernetes.</p> <p>Apart from his expertise in Kubernetes and related tools, Anji has a strong background in Terraform, utilizing it to deploy infrastructure on various cloud platforms, including Azure and AWS.</p> <p>Anji has a passion for teaching and sharing his knowledge with others. He has written numerous articles and tutorials on Kubernetes, ArgoCD, and Helm, and published in Medium website.</p> <p>With his extensive knowledge and experience in Kubernetes, ArgoCD, and Helm, Anji is the perfect author for this book on <code>Build and Deploy Microservices Applications on a Kubernetes using ArgoCD and Helm</code>. His passion for teaching and his ability to explain complex concepts in simple terms will make this book a valuable resource for readers of all levels of expertise.</p> <p>Throughout his career, Anji have worked with various companies in diverse domains such as Banking, Healthcare, and Finance, across countries such as India, UK, and US. He is dedicated to making a significant impact in his workplace and helping others along the way.</p> <p>During his free time, Anji finds joy in various activities such as playing soccer, going hiking, exploring new places, and most importantly, spending quality time with his loved ones.</p> <p>For any questions, suggestions, or topic requests, feel free to drop him a message, and he'll get in touch when his schedule permits.</p> <p>Contact Information:</p> <ul> <li>Email: anjkeesari@gmail.com</li> <li>Website: https://anjikeesari.com</li> </ul>"},{"location":"gettingstarted/acknowledgments/","title":"Acknowledgments","text":"<p>Writing a book is a collaborative effort, and I could not have done it without the help and support of many people.</p> <p>First and foremost, I would like to thank my family for their patience, understanding, and encouragement throughout this project, they scarified lot of (long) weekends. Their love and support kept me going during the long hours of writing and editing.</p> <p>During this book writing I had to refer lot of online materials, I would also like to thank the many individuals and organizations who have contributed to the development of Kubernetes, ArgoCD, Helm, Terraform, and containerization technologies. Without their hard work and dedication, this book would not be possible.</p> <p>Finally, I would like to thank the readers of this book for their interest and support. I hope that this book will be a valuable resource for anyone who wants to learn how to build and deploy microservices applications on a Kubernetes using ArgoCD and Helm.</p> <p>Thank you all for your contributions and support.</p> <p>Warm regards,</p> <p>Anji Keesari</p>"},{"location":"gettingstarted/introduction/","title":"Introduction","text":"<p>Welcome to the world of containerized microservices and Kubernetes, where scalability, flexibility, and reliability are most important in modern application development. In recent years, microservices architecture has become increasingly popular as a way to build complex applications that are scalable, reliable, and maintainable. Microservices architecture breaks down large applications into smaller, independent services that communicate with each other through APIs. Each service can be developed, deployed, and scaled independently, allowing for greater flexibility and agility.</p> <p>However, managing microservices can be challenging, particularly when it comes to deployment and scaling. Kubernetes is a powerful tool for managing containerized applications and has become the standard for deploying microservices. Kubernetes provides a robust platform for managing containerized applications, but it can be complex to set up and manage.</p> <p>In this book, \"Build and Deploy Microservices on Kubernetes using ArgoCD &amp; Helm,\" I will show you how to tackle these challenges and successfully build and deploy containerized microservices applications on Kubernetes using ArgoCD and Helm. ArgoCD is a powerful continuous delivery tool that simplifies the deployment of Kubernetes applications. Helm, on the other hand, is a package manager for Kubernetes that makes it easy to deploy and manage complex applications.</p> <p>Throughout this book, I will guide you step by step, providing detailed instructions and practical examples, making it accessible to developers, DevSecOps engineers, and IT professionals. Whether you are new to containerized microservices or have prior experience, this book will helps you with the knowledge and skills necessary to leverage the full potential of microservices on Kubernetes using ArgoCD and Helm.</p> <p>The implementation guide within this book serves as your reference architecture for creating, deploying, and managing your microservices architecture with Azure Kubernetes Services (AKS) using ArgoCD and Helm's CI/CD DevOps process. You will learn how to create multiple microservice applications with different technology stacks, containerize these microservices, push them to Azure Container Registry (ACR), and finally deploy them to Azure Kubernetes Service (AKS). Additionally, you will gain a comprehensive understanding of deploying microservice applications to AKS using ArgoCD and Helm Charts.</p> <p>Get ready to start on a journey of mastering microservice deployment on Kubernetes with ArgoCD and Helm. Let's look into the true potential of cloud-native development!</p>"},{"location":"gettingstarted/introduction/#organization-of-this-book","title":"Organization of this Book:","text":"<p>At a high level, this book is organized into a total of six chapters, each featuring multiple exercises or labs designed to enhance your understanding.</p> <p>Chapter 1: Building Containerized Microservices </p> <p>In this first chapter, \"Building Containerized Microservices\" we will explore the fundamental concepts of microservices, Docker, and dev containers. Additionally, we explore microservices implementation patterns utilizing Azure DevOps Git repositories. I will guide you through the process of creating multiple containerized microservices applications employing various technology stacks. The chapter covers essential topics such as organizing your code in Git repositories, Dockerizing applications with Dockerfiles, and pushing Docker containers to container registries.</p> <p>Throughout the exercises, we will create microservices using different technologies, including .NET Core, Node.js, and React.js. Additionally, we'll delve into the creation of containerized databases, such as SQL Server and PostgreSQL. The containerized microservices applications developed in this chapter will form the foundational basis for subsequent labs.</p> <p>Chapter 2: Create Azure Infrastructure with Terraform </p> <p>In this second chapter, \"Create Azure Infrastructure with Terraform,\" we look into Infrastructure as Code (IaC) and the benefits it brings. Focusing on Terraform as our IaC tool, we explore its setup and address the crucial task of hosting containerized microservices applications.</p> <p>Using Terraform, I guide you through the creation of essential Azure cloud resources. These include a Log Analytics workspace, Virtual Network, Azure Container Registry (ACR), Application Gateway, Azure Kubernetes Service, Azure PostgreSQL Database, Azure Key Vault, Azure Redis Cache, Azure Storage Account, Azure Event Hubs, and the setup of Front Door and CDN profiles. The goal is to automate the infrastructure creation process comprehensively, adhering to best practices in infrastructure management.</p> <p>Chapter 3: Prepare Azure Kubernetes Service (AKS) for Microservices </p> <p>In this third chapter, we delve into the details of preparing applications for Azure Kubernetes Service (AKS). The focus is on deploying a .NET Core API and containerized microservices applications to AKS, complemented by hands-on experience in managing AKS clusters using kubectl. Additionally, we explore the setup of NGINX ingress controller, Application Gateway ingress controller (AGIC), and Cert-Manager in AKS using Terraform.</p> <p>This chapter also covered the issuance of Let's Encrypt SSL Certificates to enhance website security, addresses the integration of Azure Key Vault with AKS, offering robust security measures. Kubernetes Pod troubleshooting techniques are detailed, ensuring a comprehensive understanding. The chapter concludes by guiding you through the creation of a new user node pool in AKS, providing valuable insights into upgrading or resizing node pools. By the end of this chapter, you will possess the knowledge and skills required to proficiently manage containerized applications within a Kubernetes environment.</p> <p>Chapter 4: Microservices CI/CD with Azure DevOps</p> <p>In the fourth chapter, \"Microservices CI/CD with Azure DevOps,\" we will look into the world of DevOps and its relevance to microservices. We will discuss DevOps best practices and outline a CI/CD (Continuous Integration/Continuous Delivery/Deployment) strategy specifically tailored for microservices on Kubernetes. Furthermore, you will learn how to create DevOps pipelines for each of your microservices applications, ensuring streamlined and automated software delivery.</p> <p>Chapter 5: Helm: Managing Kubernetes Deployments</p> <p>Chapter 5 will introduce you to Helm, a powerful package manager for Kubernetes. We will start by creating basic Helm charts and familiarizing ourselves with frequently used Helm commands. You will then learn how to convert the basic Helm charts for your microservices applications and package them for easy deployment. Additionally, we will explore pushing Helm charts to a container registry and deploying them effectively in a Kubernetes environment.</p> <p>Chapter 6: ArgoCD: Continuous Deployment for Kubernetes</p> <p>The final chapter, \"ArgoCD: Continuous Deployment for Kubernetes,\" will introduce you to ArgoCD, a robust continuous delivery tool specifically designed for Kubernetes environments. I will cover the installation of ArgoCD and its command-line interface (CLI). You will gain insights into creating projects, repositories, and applications within ArgoCD, enabling seamless deployment of your microservices. Furthermore, we will explore how to deploy Helm charts using ArgoCD, ensuring efficient and controlled application deployments.</p>"},{"location":"gettingstarted/introduction/#intended-audience-of-the-book","title":"Intended Audience of the Book:","text":"<p>This book is intended for a range of professionals including software developers, architects, DevSecOps engineers, and individuals interested in building microservices applications using Kubernetes and related tools.</p> <p>Specifically, this book is ideal for individuals who already possess some familiarity with Kubernetes and wish to explore how to leverage it for building and deploying microservices applications using Argocd and Helm. It is also well-suited for those who are new to Kubernetes and want to learn how to use it for microservices development.</p> <p>The content of this book will be particularly valuable to professionals working in organizations that are transitioning to a microservices architecture or utilizing Kubernetes for container orchestration. It may also be relevant for students or researchers seeking to expand their knowledge of modern software development practices.</p> <p>While the book assumes a basic level of knowledge in software development or DevSecOps, it can still be accessible to individuals new to these subjects who are willing to dedicate the necessary time and effort to learn. The book provides explanations and guidance throughout, enabling readers to grasp the concepts and successfully apply them in practice.</p> <p>By addressing the needs of both experienced practitioners and newcomers to the field, this book aims to empower a diverse audience with the skills and knowledge required to build robust and scalable microservices applications using Kubernetes, Argocd, and Helm.</p>"},{"location":"gettingstarted/introduction/#key-benefits-of-reading-this-book","title":"Key Benefits of Reading This Book:","text":"<p>By reading this book, you will gain a variety of key benefits, including:</p> <ol> <li> <p>Practical Guidance on Building Microservices Applications: This book offers practical guidance and best practices for designing, developing, and deploying microservices applications using Kubernetes and related tools. You will learn how to effectively utilize Helm and ArgoCD to streamline your microservices workflow and achieve efficient application deployment.</p> </li> <li> <p>Comprehensive Coverage of Key Topics: With this book, you will receive comprehensive coverage of various topics essential to microservices development. From containerization to deployment strategies and beyond, you will gain an in-depth understanding of the core concepts and techniques involved in building microservices applications on Kubernetes.</p> </li> <li> <p>Real-World Examples and Case Studies: To provide practical insights, this book includes real-world examples and case studies. These scenarios will help you grasp how microservices applications can be successfully built and deployed using Kubernetes and associated tools. You will be able to apply the concepts and techniques from the book to real-world situations.</p> </li> <li> <p>Hands-On Exercises and Code Samples: To enhance your learning experience, this book offers hands-on exercises and code samples. These activities provide opportunities to practice your skills and build your own microservices applications. By engaging in these exercises, you will gain practical experience working with Kubernetes and related tools.</p> </li> <li> <p>Guidance on Best Practices: This book offers guidance on best practices for building microservices applications on Kubernetes. You will learn how to design and deploy scalable, reliable, and maintainable microservices applications. Following the best practices outlined in this book ensures optimal application performance and manageability.</p> </li> </ol> <p>In summary, this book serves as a comprehensive guide to building microservices applications on Kubernetes, providing practical guidance, real-world examples, hands-on exercises, and best practices. By leveraging the knowledge and skills acquired from this book, you will be equipped to design, develop, and deploy microservices applications effectively in a Kubernetes environment, using the latest tools and industry best practices.</p>"},{"location":"gettingstarted/lab-format/","title":"Lab Format","text":"<p>In each chapter, you will find multiple labs, each following a similar structure with subheadings. Understanding the purpose of each subheading beforehand will help you relate to the lab activities more effectively. The lab format includes the following sections:</p> <p>Introduction:</p> <p>This section provides a brief overview of the lab's concepts and definitions. It serves as a quick introduction to get you familiarized with the lab and ready to perform the necessary actions. If you require in-depth knowledge on a specific topic, you can refer to the links provided at the end of each lab.</p> <p>Technical Scenario:</p> <p>This section outlines the purpose of the lab within a practical context. It explains where and how you can apply the implementation guide in real-world scenarios, encouraging you to relate the lab scenario to your own Microservice architecture with Kubernetes.</p> <p>Prerequisites:</p> <p>This section lists the tools, software, or any prerequisites that need to be installed or set up before starting the lab. Ensuring you have the necessary prerequisites in place beforehand will help you avoid interruptions during the lab.</p> <p>For example:</p> <ul> <li>Active Azure subscription</li> <li>Download and Install Terraform</li> <li>Download and Install Azure CLI</li> </ul> <p>Objective:</p> <p>This section presents objective of tasks or steps to be implemented in the lab. It provides a comprehensive list of all the actions you will perform during the lab.</p> <p>For example:</p> <p>In this exercise, we will accomplish and learn how to implement the following:</p> <ul> <li>Task-1: Define and declare variables</li> <li>Task-2: Create an Azure service using Terraform</li> <li>Task-3: Initialize Terraform</li> <li>Task-4: Create a Terraform execution plan</li> <li>Task-5: Apply a Terraform execution plan</li> </ul> <p>Architecture Diagram (Optional):</p> <p>This optional section provides an architecture diagram that illustrates the lab scenario. Architecture diagrams are helpful in visualizing the connections and relationships between components, making it easier to understand the overall scenario.</p> <p>Implementation details</p> <p>Step-by-step implementation details will be covered in this section.</p> <p>login to Azure - Verify that you are logged into the correct Azure subscription before starting anything in Visual Studio Code.</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set &lt;subscription_id&gt;\n</code></pre> <p>Example of tasks:</p> <p>Task-1: Define and declare variables</p> <p>Task-1 implementation details goes here</p> <pre><code>Source code or commands related to Task 1 will be placed here\n</code></pre> <p>Task-2: Create a Azure service using Terraform</p> <p>Task-2 implementation details goes here</p> <pre><code>Source code or commands related to Task 2 will be placed here\n</code></pre> <p>References</p> <p>This section will cover a list of references for further reading and additional actions.</p> <p>For example:</p> <p>References</p> <ul> <li>https://learn.microsoft.com/en-us/azure/architecture/reference-architectures/containers/aks-microservices/aks-microservices</li> </ul> <p>Note</p> <p>The above lab format will be consistently followed throughout the book, providing clear implementation details for each task and guiding you through the practical steps necessary to complete the labs.</p>"},{"location":"gettingstarted/tech-stack/","title":"Tech stack","text":"<p>Here is a list of technologies that can be used to build, manage Microservices and deploy &amp; run those applications on Kubernetes cluster:</p> <p>Here you will also see the list of topics from each technology so that you can learn them offline when needed.</p> <p>You may encounter some specific tools and technologies that are not listed here but you will see the references end of each lab for further learning.</p>"},{"location":"gettingstarted/tech-stack/#c-programming-language","title":"C# Programming language","text":"C# Programming language <p>Here is the list of topics you need to know as a <code>C# programming developer</code>:</p> <ol> <li>Introduction to C#:<ul> <li>What is C#</li> <li>Why use C#</li> <li>History of C#</li> </ul> </li> <li>Getting Started with C#:<ul> <li>Setting up a development environment</li> <li>Creating your first C# program</li> <li>Understanding the structure of a C# projects</li> </ul> </li> <li>Variables and Data Types:<ul> <li>Understanding variables</li> <li>Data types in C#</li> <li>Declaring and initializing variables</li> <li>Converting between data types</li> </ul> </li> <li>Control Structures:<ul> <li>Conditional statements (if/else)</li> <li>Loops (for, while, do while)</li> <li>Switch statements</li> </ul> </li> <li>Functions:<ul> <li>Understanding functions</li> <li>Creating and calling functions</li> <li>Return values and parameters</li> </ul> </li> <li>Object-Oriented Programming in C#:<ul> <li>Understanding objects and classes</li> <li>Creating and using objects</li> <li>Inheritance and polymorphism</li> <li>Encapsulation and access modifiers</li> </ul> </li> <li>Arrays and Collections:<ul> <li>Understanding arrays</li> <li>Declaring and using arrays</li> <li>Understanding collections</li> <li>Using lists and dictionaries</li> </ul> </li> <li>Debugging and Exception Handling:<ul> <li>Debugging techniques in Visual Studio</li> <li>Understanding exceptions and errors</li> <li>Using try/catch blocks to handle exceptions</li> </ul> </li> <li>Advanced Topics:<ul> <li>File Input/Output</li> <li>LINQ</li> <li>Delegates and events</li> <li>Asynchronous programming</li> </ul> </li> <li>Building Applications with C#:<ul> <li>Windows Forms applications</li> <li>ASP.NET web applications</li> <li>Mobile applications with Xamarin</li> <li>Games with Unity</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#net-core-web-api","title":".NET Core Web API","text":".NET Core Web API <p>Here is the list of topics that you need to know as a .NET Core Web API Backend (BE) developer:</p> <ol> <li>Introduction to .NET Core Web API:<ul> <li>What is a .NET Core Web API</li> <li>Why use .NET Core for building web APIs</li> <li>The benefits of using .NET Core</li> </ul> </li> <li>Setting up the Development Environment:<ul> <li>Installing the .NET Core SDK</li> <li>Installing Visual Studio Code or Visual Studio</li> <li>Setting up the project structure</li> </ul> </li> <li>Creating Your First Web API:<ul> <li>Creating a new project</li> <li>Defining the API endpoints</li> <li>Implementing the API logic</li> <li>Testing the API</li> </ul> </li> <li>Routing and Controller:<ul> <li>Understanding routing in .NET Core Web API</li> <li>Creating controllers and defining endpoints</li> <li>Implementing HTTP verbs (GET, POST, PUT, DELETE)</li> <li>Handling request and response data</li> </ul> </li> <li>Data Access with Entity Framework Core:<ul> <li>Understanding Entity Framework Core</li> <li>Setting up the database and the context</li> <li>Defining entities and relationships</li> <li>Querying data with LINQ</li> </ul> </li> <li>Authentication and Authorization:<ul> <li>Understanding authentication and authorization</li> <li>Implementing authentication with JWT</li> <li>Implementing authorization with policies</li> </ul> </li> <li>Deployment and Hosting:<ul> <li>Deploying the API to Azure or another cloud provider</li> <li>Hosting the API on IIS or a reverse proxy server</li> <li>Securing the API with SSL</li> </ul> </li> <li>Advanced Topics:<ul> <li>Versioning the API</li> <li>Documentation with Swagger</li> <li>Logging and exception handling</li> <li>Performance optimization and caching</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#react-js","title":"React JS","text":"React JS <p>Here is the list of topics that you need to know as a <code>Frontend (FE) React Developer</code>:</p> <ol> <li>Introduction to React<ul> <li>What is React</li> <li>Why use React</li> </ul> </li> <li>Getting started with React<ul> <li>Setting up a React development environment</li> <li>Creating a React application</li> <li>Understanding the structure of a React application</li> </ul> </li> <li>Working with React components<ul> <li>Defining and rendering components</li> <li>Props and state in components</li> <li>Lifecycle methods in components</li> </ul> </li> <li>Building user interfaces with React<ul> <li>Working with JSX</li> <li>Using React hooks</li> <li>Managing data with context</li> </ul> </li> <li>Managing application state with Redux<ul> <li>Introduction to Redux</li> <li>Setting up Redux in a React application</li> <li>Working with actions, reducers, and the store</li> </ul> </li> <li>Routing in React applications<ul> <li>Introduction to React Router</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#blazor","title":"Blazor","text":"Blazor <p>Here is the list of topics you need to know as a <code>Fontend (FE) Blazor Developer</code>:</p> <ol> <li>Introduction to Blazor<ul> <li>What is Blazor</li> <li>Benefits of using Blazor</li> </ul> </li> <li>Getting started with Blazor<ul> <li>Setting up a Blazor development environment</li> <li>Creating a Blazor application</li> <li>Understanding the structure of a Blazor application</li> </ul> </li> <li>Building user interfaces with Blazor<ul> <li>Defining and rendering components</li> <li>Working with JSX</li> <li>Using Blazor templates</li> </ul> </li> <li>Managing application state with Blazor<ul> <li>Introduction to component state and lifecycle</li> <li>Using component parameters and events</li> <li>Sharing state between components</li> </ul> </li> <li>Data access in Blazor<ul> <li>Introduction to Entity Framework and LINQ</li> <li>Querying and updating data with Entity Framework Core</li> <li>Using dependency injection to manage data access</li> </ul> </li> <li>Routing in Blazor applications<ul> <li>Introduction to Blazor routing</li> <li>Setting up routes and navigating between them</li> <li>Working with dynamic routes and parameters</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#nodejs","title":"Node.js","text":"Node.js <p>Here is the list of topics that you need to know as a <code>Frontend (FE) developer</code>:</p> <ol> <li>Introduction to Node.js<ul> <li>What is Node.js</li> <li>Why use Node.js</li> <li>History of Node.js</li> <li>Key features of Node.js</li> </ul> </li> <li>Setting up a Development Environment<ul> <li>Installing Node.js</li> <li>Setting up a development environment</li> <li>Understanding npm (Node Package Manager)</li> </ul> </li> <li>Understanding Node.js Fundamentals<ul> <li>Understanding event-driven programming</li> <li>Understanding non-blocking I/O</li> <li>Understanding modules and require</li> <li>Understanding the Node.js runtime environment</li> </ul> </li> <li>Building a Simple Web Server with Node.js<ul> <li>Introduction to Express</li> <li>Setting up a basic Express server</li> <li>Handling HTTP requests and responses</li> <li>Serving static files</li> </ul> </li> <li>Working with Data in Node.js<ul> <li>Understanding the basics of databases</li> <li>Connecting to a database in Node.js</li> <li>Querying data in Node.js</li> <li>Storing data in Node.js</li> </ul> </li> <li>Building Advanced Applications with Node.js<ul> <li>Understanding middleware</li> <li>Creating a REST API</li> <li>Authenticating and securing applications</li> <li>Using WebSockets for real-time communication</li> <li>Deploying Node.js applications</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#azure-devops","title":"Azure DevOps","text":"Azure DevOps <p>Here is the list of topics that you need to know as a <code>Azure DevOps Engineer</code>:</p> <ol> <li>Introduction to Azure DevOps<ul> <li>What is Azure DevOps</li> <li>Key features and benefits</li> <li>Overview of Azure DevOps services and offerings</li> </ul> </li> <li>Setting up an Azure DevOps Environment<ul> <li>Creating an Azure DevOps account</li> <li>Setting up an organization and project</li> <li>Configuring users and permissions</li> </ul> </li> <li>Source Control with Azure DevOps<ul> <li>Overview of Azure DevOps source control options</li> <li>Using Git for source control</li> <li>Managing code branches and pull requests</li> <li>Integrating with other source control tools</li> </ul> </li> <li>Work Item Tracking in Azure DevOps<ul> <li>Overview of Azure DevOps work item tracking</li> <li>Creating and managing work items</li> <li>Using Azure Boards for project management and planning</li> <li>Integrating with other project management tools</li> </ul> </li> <li>Continuous Integration and Delivery with Azure DevOps<ul> <li>Overview of Azure DevOps CI/CD options</li> <li>Creating and managing build pipelines</li> <li>Configuring automated deployments</li> <li>Implementing continuous testing</li> <li>Azure Artifacts</li> <li>learning YAML Schema</li> </ul> </li> <li>Testing in Azure DevOps<ul> <li>Overview of Azure DevOps testing options</li> <li>Using Azure Test Plans for manual and exploratory testing</li> <li>Settingup continuous testing with Azure Pipelines</li> <li>Integrating with other testing tools</li> </ul> </li> <li>Best Practices for Azure DevOps Deployments<ul> <li>Build, Test, &amp; deploy .NET Core apps to Azure</li> <li>Build, Test, &amp; deploy React JS apps to Azure</li> <li>Build, Test, &amp; deploy Blazor apps to Azure</li> <li>Build &amp; deploy Azure SQL database using [DACPAC]</li> <li>Validate and deploy ARM Templates</li> <li>Git Clone, Code review with Pull request</li> <li>Private Agent configuration, Approvals, Artifact</li> <li>Release variables, Replace Tokens</li> <li>Security setup in Azure DevOps</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#azure-cloud","title":"Azure Cloud","text":"Azure Cloud <p>Here is the list of topics that you need to know as a <code>Azure Cloud Engineer</code>:</p> <ol> <li>Introduction to Azure Cloud<ul> <li>What is Microsoft Azure</li> <li>Key features and benefits</li> <li>Overview of Azure services and offerings</li> </ul> </li> <li>Getting Started with Azure<ul> <li>Setting up an Azure account</li> <li>Navigating the Azure portal</li> </ul> </li> <li>Creating your first Azure resource<ul> <li>Virtual Machines in Azure</li> <li>Overview of Azure virtual machines</li> <li>Creating and managing virtual machines</li> <li>Virtual machine sizing and scalability</li> </ul> </li> <li>Storage in Azure<ul> <li>Overview of Azure storage options</li> <li>Managing storage accounts and containers</li> <li>Blob, table, and queue storage</li> <li>Storing and retrieving files in Azure</li> </ul> </li> <li>Networking in Azure<ul> <li>Overview of Azure networking options</li> <li>Virtual networks and subnets</li> <li>Load balancing and traffic management</li> <li>ExpressRoute and VPN connectivity</li> </ul> </li> <li>Databases in Azure<ul> <li>Overview of Azure database services</li> <li>Managing SQL databases</li> <li>NoSQL options with Azure Cosmos DB</li> <li>Using Azure Database for MySQL and PostgreSQL</li> </ul> </li> <li>Practical knowledge on<ul> <li>Azure Accounts, Subscriptions, and Billing</li> <li>Manage access to Azure resources using RBAC</li> <li>Create Resource Groups</li> <li>Create Azure App Service Plan &amp; ASE</li> <li>Create Web App, API App, Mobil App</li> <li>Create Azure SQL Database, Cosmos DB</li> <li>Create Storage account</li> <li>Application Insights</li> <li>Redis Cache</li> <li>Azure API Management</li> <li>Create Azure Key Vault for secrets</li> <li>Provision Azure resources via ARM Templates &amp; Portal</li> <li>Azure Monitor &amp; Log Analytics</li> <li>Calculate Azure Pricing</li> <li>Create API Gateway using API Management</li> <li>Create Azure Application Gateway</li> <li>Create VNet, Subnet with Network Security Groups</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#kubernetes-cluster","title":"Kubernetes Cluster","text":"Kubernetes Cluster <p>Here is the list of topics that you need to know as <code>Azure</code>Cloud Engineer`` to build and deploy application in Kubernetes. </p> <ul> <li>Introduction to AKS<ul> <li>What is AKS</li> <li>Key features and benefits</li> </ul> </li> <li>Kubernetes Architecture Components<ul> <li>Kubernetes \u2500 Master Machine Components</li> <li>Kubernetes \u2500 Node Components</li> </ul> </li> <li>Creating AKS cluster<ul> <li>Creating an AKS cluster using the Azure portal</li> <li>Creating an AKS cluster using Terraform</li> <li>Testing cluster connection &amp; creating namespace</li> <li>Configuring node pools and scaling</li> </ul> </li> <li>Deploying applications to AKS<ul> <li>Creating a container image</li> <li>Pushing the image to a container registry</li> <li>Deploying the application to AKS</li> <li>Managing and scaling an AKS cluster</li> </ul> </li> <li>Networking in AKS<ul> <li>Overview of AKS networking options</li> <li>Managing virtual networks and subnets</li> <li>Load balancing and traffic management</li> </ul> </li> <li>Monitoring and Management in AKS<ul> <li>Upgrading AKS clusters</li> <li>Scaling the number of nodes in an AKS cluster</li> <li>Monitoring and logging AKS clusters</li> </ul> </li> <li>Deploying k8s ingress controller</li> <li>Adding TLS/SSL to the ingress</li> <li>K8s horizontal pod autoscaler [HPA]<ul> <li>K8s horizontal pod autoscaler [HPA]</li> <li>HPA in action</li> <li>AKS cluster autoscaling</li> </ul> </li> <li>Integrating AKS with Azure Monitor</li> <li>AKS Storage and Networks</li> <li>AKS storage overview<ul> <li>Creating storage classes</li> <li>Storage: Persistent claims</li> <li>Shared volumes</li> <li>Create resource for shared volume</li> <li>Challenge: Lost volumes</li> <li>Solution: Find and remove PVs</li> <li>Networking and AKS</li> <li>Load balancing and Ingress: Setup</li> </ul> </li> </ul>"},{"location":"gettingstarted/tech-stack/#terraform","title":"Terraform","text":"Terraform <p>Here is the list of topics that you need to know as a <code>Azure Cloud Terraform Developer</code>:</p> <ol> <li>Introduction<ul> <li>What is Terraform?</li> <li>Why use Terraform?</li> <li>Key concepts</li> </ul> </li> <li>Setting up Terraform<ul> <li>Installing Terraform</li> <li>Configuring Terraform</li> <li>Creating your first Terraform configuration</li> </ul> </li> <li>The Terraform Configuration Language (HashiCorp Configuration Language, HCL)<ul> <li>Syntax and structure</li> <li>Variables</li> <li>Outputs</li> <li>Modules</li> </ul> </li> <li>Providers<ul> <li>Overview of available providers</li> <li>Managing multiple providers</li> <li>Using provider-specific resources</li> </ul> </li> <li>Resource Types<ul> <li>Overview of supported resource types</li> <li>Creating, updating, and destroying resources</li> <li>Importing existing resources into Terraform</li> </ul> </li> <li>Modules<ul> <li>Creating and using modules</li> <li>Sharing modules with others</li> </ul> </li> <li>Workspaces<ul> <li>Managing multiple environments with workspaces</li> <li>Workspace management commands</li> </ul> </li> <li>State Management<ul> <li>Understanding Terraform state</li> <li>Backing up and sharing state files</li> </ul> </li> <li>Advanced Terraform Features<ul> <li>Input and output variables</li> <li>Loops and conditions</li> <li>Data sources</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable configurations</li> <li>Managing state effectively</li> <li>Collaborating with others</li> </ul> </li> <li>Troubleshooting<ul> <li>Common issues and errors</li> <li>Debugging techniques</li> <li>Tips for avoiding common mistakes</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#argocd","title":"Argocd","text":"ArgoCD <p>Here is the list of topics that you need to know as <code>DevSecOps Engineer</code> to deploy and manage application in Kubernetes using Argocd. </p> <ol> <li>Introduction to Argo CD<ul> <li>What is Argo CD</li> <li>Key features and benefits</li> </ul> </li> <li>Setting up an Argo CD Environment<ul> <li>Prerequisites</li> <li>Installing Argo CD</li> <li>Setting up an Argo CD server and CLI</li> <li>Configuring users and permissions</li> </ul> </li> <li>Deploying Applications with Argo CD<ul> <li>Defining an application in a Git repository</li> <li>Adding an application to Argo CD</li> <li>Integrating with other CI/CD tools</li> <li>Updating and rolling back application deployments</li> </ul> </li> <li>Working with Argo CD configurations<ul> <li>Managing application configurations in Git</li> <li>Using Git branches and tags in Argo CD</li> <li>Handling conflicts and errors in configurations</li> <li>Implementing continuous delivery</li> </ul> </li> <li>Collaborating with Argo CD<ul> <li>Using Argo CD with Git hosting platforms</li> <li>Setting up multi-user access to Argo CD</li> </ul> </li> <li>Advanced Argo CD features<ul> <li>Using Argo CD with GitOps workflows</li> <li>Integrating Argo CD with CI/CD pipelines</li> <li>Using Argo CD with GitLab CI/CD</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#helm-chart","title":"Helm Chart","text":"Helm Chart <p>Here is the list of topics that you need to know as <code>DevSecOps Engineer</code> to deploy and manage application in Kubernetes using Helm Chart. </p> <ol> <li>Introduction to Helm<ul> <li>What is Helm?</li> <li>Benefits of using Helm</li> </ul> </li> <li>Getting started with Helm<ul> <li>Installing Helm</li> <li>Initializing Helm </li> </ul> </li> <li>Working with Helm charts<ul> <li>Finding and downloading charts from the Helm chart repository</li> <li>Creating and deploying your own charts</li> <li>Upgrading and rolling back chart deployments</li> </ul> </li> <li>Managing dependencies with Helm<ul> <li>Using Helm to manage the dependencies of a chart</li> <li>Sharing charts and chart dependencies with the Helm chart repository</li> </ul> </li> <li>Collaborating with Helm<ul> <li>Using Helm with version control systems</li> <li>Collaborating with team members using Helm</li> </ul> </li> <li>Advanced Helm features<ul> <li>Using Helm with continuous delivery pipelines</li> <li>Customizing Helm behavior with hooks</li> <li>Extending Helm with plugins</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#application-testing","title":"Application Testing","text":"Application Testing <p>Here is the list of topics that you need to know as <code>QA Engineer</code> for supporting Unit, Integration and Functional testing.</p> <ol> <li>Introduction to Unit and Integration Testing<ul> <li>Overview of testing in software development</li> <li>Importance of unit and integration testing</li> <li>Understanding the differences between unit and integration testing</li> </ul> </li> <li>Setting up a Testing Environment<ul> <li>Understand testing tools and frameworks</li> <li>Installing and configuring testing tools</li> <li>Creating a testing project </li> <li>setting up testing frameworks</li> </ul> </li> <li>Unit Testing <ul> <li>Understand unit testing technologies</li> <li>Writing unit tests using MSTest, NUnit, and XUnit, Moq &amp; AutoFixure.</li> <li>Debugging and troubleshooting unit tests</li> <li>Managing and organizing unit tests</li> </ul> </li> <li>Integration Testing<ul> <li>Understanding integration testing technologies</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting integration tests</li> <li>Integrating with other testing tools and frameworks</li> <li>Create BDDs with SpectFlow</li> </ul> </li> <li>Functional Testing<ul> <li>Understanding Functional testing technologies</li> <li>Writing Functional tests</li> <li>Debugging and troubleshooting Functional tests</li> </ul> </li> <li>Testing ASP.NET Core Web API <ul> <li>Writing unit tests</li> <li>Writing integration tests</li> <li>Debugging and troubleshooting tests</li> </ul> </li> <li>Testing React JS application<ul> <li>Writing UI test with Selenium</li> <li>Use Jest for react js test cases</li> </ul> </li> <li>Automating Testing with Continuous Integration/Continuous Deployment (CI/CD)<ul> <li>Setting up a CI/CD pipeline</li> <li>Automating Unit tests with CI/CD</li> <li>Automating Integration tests with CI/CD</li> <li>Automating Functional tests with CI/CD</li> </ul> </li> <li>Best Practices<ul> <li>Writing maintainable and reusable tests</li> <li>Implementing code coverage and test coverage analysis</li> <li>Managing testing data and test environment</li> <li>Implementing testing as part of the development process</li> <li>Review test results</li> <li>Test Analytics</li> <li>Review &amp; Report code coverage results</li> <li>API Automation testing using SOAP UI</li> <li>Create Regression test suite</li> <li>Create Smoke / Sanity test suite</li> <li>Performance testing- to ensure that it can handle a high load of users and data.</li> </ul> </li> </ol>"},{"location":"gettingstarted/tech-stack/#development-tools","title":"Development Tools","text":"Development Tools <p>Here is the list of tools that are commonly used in the development of software applications. </p> <ul> <li>Visual Studio Code</li> <li>SQL Server Management Studio, SQL Profiler</li> <li>Node JS, NPM, Notepad++</li> <li>Postman, SOAP UI</li> <li>Browser Developer Tools</li> <li>Agile &amp; Scrum with JIRA or Azure Board</li> <li>Nuget Manager, GitHub Desktop</li> <li>Open SSL</li> <li>JSON Viewer/Formatter, JWT debugger, SAML-Tracer</li> <li>Azure Storage Explorer</li> <li>Visual Studio 2022/2019 [optional]</li> </ul>"},{"location":"gettingstarted/tech-stack/#networking-troubleshooting-tools","title":"Networking troubleshooting Tools","text":"Networking troubleshooting Tools <p>There are several networking troubleshooting tools that can be used to diagnose and resolve network issues:</p> <ol> <li>Ipconfig - Used to get the IP address</li> <li>Ping - Used to send signals to another device on the network to see if it is active.</li> <li>Traceroute - Used to see the step by step route a packet takes to the destination.</li> <li>nslookup - used to check firewall, fetches the DNS records for given domain name or IP address</li> <li>Telnet -  used to test the connectivity between two networked devices and to troubleshoot issues with a specific service or application.</li> <li>Netstat - used to view of active ports on your machine and their status. This helps user to understand which ports are open, closed, or listening for incoming connections.</li> <li>Wireshark: used to capture, inspect, and analyze network traffic.</li> <li>tcpdump:  used to capture and inspect network packets.</li> <li>iperf: This is a network performance testing tool that can be used to measure the bandwidth, delay, jitter, and loss of a network connection.</li> </ol>"},{"location":"gettingstarted/toc%20copy/","title":"Table of Contents","text":"<ul> <li>About the Author</li> <li>Acknowledgments</li> <li>Introduction</li> </ul> <p>Chapter-1: Getting started with Microservices</p> <ul> <li>Lab-1: Getting started with Microservices<ul> <li>Task-1: Identify Microservices List:</li> <li>Task-2: Identify the List of Git Repositories Needed</li> <li>Task-3: Create new Azure DevOps Organization</li> <li>Task-4: Create new Azure DevOps Project</li> </ul> </li> <li>Lab-2: Create your first Microservice with .NET Core<ul> <li>Step-1: Create a new repo in azure DevOps</li> <li>Step-2: Clone the repo from azure DevOps</li> <li>Step-3: Create a new .NET Core Web API project</li> <li>Step-4: Test the new .NET core Web API project</li> <li>Step-5: Add Dockerfiles to the API project</li> <li>Step-6: Docker Build &amp; Run</li> <li>Step-7: Push docker container to ACR</li> <li>Step-8: Pull docker container from ACR</li> </ul> </li> <li>Lab-3: Create your second Microservice with Node JS<ul> <li>Step-1: Create a new Node JS API Project</li> <li>Step-2: Test the Node JS API project</li> <li>Step-3: Add Dockerfiles to the Node JS API</li> <li>Task-4: Docker Build locally</li> <li>Step-5: Docker Run locally</li> <li>Step-6: Push docker container to ACR</li> </ul> </li> <li>Lab-4: Create your first website using ASP.NET Core MVC<ul> <li>Step-1: Create a new ASP.NET Core Web App (MVC project)</li> <li>Step-2: Test the new ASP.NET core Web App project</li> <li>Step-3: Update home page contents [Optional]</li> <li>Step-4: Add Dockerfiles to the MVC project</li> <li>Task-5: Docker Build locally</li> <li>Step-6: Docker Run locally</li> <li>Step-7: Push docker container to ACR</li> </ul> </li> <li>Lab-5: Create your second website using React JS<ul> <li>Step-1: Install Node.js and NPM</li> <li>Step-2: Create a new React JS application</li> <li>Step-3: Add Dockerfiles to the MVC project</li> <li>Step-4: Docker Build locally</li> <li>Step-5: Docker Run locally</li> <li>Step-6: Push docker container to ACR</li> </ul> </li> </ul> <ul> <li>Lab-6: Create your first database with SQL server<ul> <li>Introduction</li> <li>Databases concepts</li> <li>SQL Server Management Studio (SSMS)</li> <li>Create Azure SQL Server</li> <li>Creating your first database</li> <li>Creating Tables</li> <li>Inserting Data</li> <li>Retrieving Data</li> <li>Deleting Data</li> <li>Dropping a Database</li> </ul> </li> <li>Lab-7: Create your second database with PostgreSQL</li> </ul> <p>Chapter 2: Azure Cloud- Infrastructure as Code (IaC)</p> <ul> <li>Lab-1: Infrastructure as Code (IaC) <ul> <li>Overview</li> <li>Infrastructure as Code (IaC)</li> <li>Hight level Architecture</li> <li>Benefits of IaC</li> <li>Terraform Benefits</li> </ul> </li> <li>Lab-2: Microservices Architecture on AKS<ul> <li>High Level Architecture</li> <li>Azure Reference Architecture Components </li> </ul> </li> <li>Lab-3: Azure Account &amp; Subscription Management<ul> <li>Create New Azure Account</li> <li>Create New Azure Subscription</li> </ul> </li> <li>Lab-4: Create Terraform Foundation Part-1<ul> <li>Task-1: Create a new project in azure DevOps</li> <li>Task-2: Create a new Azure DevOps Repo for terraform</li> <li>Task-3: Clone the git repo</li> <li>Task-4: Add .gitignore file in the git repo for terraform</li> <li>Task-5: Create Terraform Service Principle Credentials</li> <li>Task-6: Create new resource group</li> <li>Task-7: Create new storage account &amp; container</li> <li>Task-8: Create new Key vault</li> <li>Task-9: Create secrets in Key Vault</li> <li>Task-10: Setup Access Policy in Key Vault</li> <li>Task-11: Configure Service Principal Role Assignment</li> </ul> </li> <li>Lab-5: Create Terraform Foundation Part-2<ul> <li>Task-1: Create terraform environment variables</li> <li>Task-2: Create terraform providers</li> <li>Task-3: Configure terraform backend state</li> <li>Task-4: Create terraform variables</li> <li>Task-5: Create locals file</li> <li>Task-6: Create azure resource group</li> <li>Task-7: Store terraform commands</li> <li>Task-8: Initialize Terraform</li> <li>Task-9: Setup Terraform workspace</li> <li>Task-10: Create a Terraform execution plan</li> <li>Task-11: Apply a Terraform execution plan</li> <li>Task-12: Verify the results</li> <li>Task-13: Verify terraform state file.</li> </ul> </li> <li>Lab-6: Create Log Analytics Workspace using terraform<ul> <li>Task-1: Configure terraform variables for Log Analytics workspace</li> <li>Task-2: Create new resource group for Log Analytics workspace</li> <li>Task-3: Create Log Analytics workspace using terraform</li> <li>Task-4: Validate Log Analytics workspace in the Azure portal</li> <li>Task-5: Lock the resource group</li> </ul> </li> <li>Lab-7: Create Virtual Network using terraform<ul> <li>Task-1: Define and declare virtual network variables</li> <li>Task-2: Create a resource group for virtual network</li> <li>Task-3: Create Hub virtual network using terraform</li> <li>Task-4: Create Spoke virtual network using terraform</li> <li>Task-5: Create Diagnostics Settings for Networking</li> <li>Task-6: Lock the resource group</li> </ul> </li> <li>Lab-8: Create Azure Container Registry (ACR) using terraform<ul> <li>Task-1: Define and declare ACR variables</li> <li>Task-2: Create a resource group for ACR</li> <li>Task-3: Create ACR user assigned identity</li> <li>Task-4: Create Azure Container Registry (ACR) using terraform</li> <li>Task-5: Create Diagnostics Settings for ACR</li> <li>Task-6: Lock the resource group</li> <li>Task-7: Validate ACR resource<ul> <li>Task-7.1: Log in to registry</li> <li>Task-7.2: Push image to registry</li> <li>Task-7.3: Pull image from registry</li> <li>Task-7.4: List container images</li> </ul> </li> <li>Task-8: Restrict Access Using Private Endpoint<ul> <li>Task-8.1: Configure the Private DNS Zone</li> <li>Task-8.2: Create a Virtual Network Link</li> <li>Task-8.3: Create a Private Endpoint Using Terraform</li> <li>Task-8.4: Validate private link connection using <code>nslookup</code> or <code>dig</code></li> </ul> </li> </ul> </li> <li>Lab-9: Create Azure Kubernetes Service (AKS) using terraform<ul> <li>Task-1: Define and declare AKS variables</li> <li>Task-2: Create a resource group for AKS</li> <li>Task-3: Create AKS user assigned identity</li> <li>Task-4: Create Azure Kubernetes Services (AKS) using terraform</li> <li>Task-5: Create Diagnostics Settings for AKS</li> <li>Task-6: Review AKS Cluster resource in the portal</li> <li>Task-7: Validate AKS cluster running Kubectl</li> <li>Task-8: Allow AKS Cluster access to Azure Container</li> <li>Task-9: Lock the resource group</li> </ul> </li> <li>Lab-10: Create Azure PostgreSQL - Flexible Server using terraform<ul> <li>Task-1: Define and declare PostgreSQL - Flexible Server variables</li> <li>Task-2: Create an Azure resource group for PostgreSQL</li> <li>Task-3: Create an Azure Virtual Network</li> <li>Task-4: Create an Azure Subnet for PostgreSQL</li> <li>Task-5: Create a Private DNS zone for PostgreSQL</li> <li>Task-6: Associate PostgreSQL Private DNS zone with virtual network</li> <li>Task-7: Generate PostgreSQL admin random password &amp; store in Key Vault</li> <li>Task-8: Create Azure PostgreSQL - Flexible Server using Terraform</li> <li>Task-9: Configure Diagnostic Settings for Azure PostgreSQL - Flexible Server</li> <li>Task-10: Set a user or group as the AD administrator for a PostgreSQL Flexible Server</li> <li>Task-11: Create new Databases in PostgreSQL Server</li> <li>Task-12: Create AD groups for database access</li> </ul> </li> <li>Lab-11: Create Azure Key Vault using Terraform<ul> <li>Task-1: Define and declare Azure Key vault variables</li> <li>Task-2: Create Azure Key Vault using Terraform</li> <li>Task-3: Configure diagnostic settings for Azure Key Vault using terraform</li> <li>Task-4: Configure access policy for developer in Azure Key Vault</li> <li>Task-5: Restrict Access Using Private Endpoint<ul> <li>Task-5.1: Configure the Private DNS Zone</li> <li>Task-5.2: Create a Virtual Network Link</li> <li>Task-5.3: Create a Private Endpoint Using Terraform</li> <li>Task-5.4: Validate private link connection using <code>nslookup</code> or <code>dig</code></li> </ul> </li> <li>Task-6: Created azure Key Vault secrets for sensitive information</li> </ul> </li> <li>Lab-12: Create Azure Cache for Redis using Terraform<ul> <li>Task-1: Define and declare Azure Cache for Redis variables</li> <li>Task-2: Create Azure Cache for Redis using terraform</li> <li>Task-3: Configure diagnostic settings for Azure Cache for Redis using terraform    </li> <li>Task-4: Securing an Azure Cache for Redis instance<ul> <li>Task-4.1: Create private DNS zone for Redis Cache using terraform</li> <li>Task-4.2: Create virtual network link to associate redis private DNS zone to vnet</li> <li>Task-4.3: Configure private endpoint for Azure Cache for Redis using terraform</li> <li>Task-4.4: Validate private link connection using <code>nslookup</code> or <code>dig</code> </li> </ul> </li> </ul> </li> <li>Lab-13: Create Front Door and CDN profile using Terraform<ul> <li>Task-1: Define and Declare Azure Front Door &amp; CDN variables.</li> <li>Task-2: Create a Front Door CDN profile using Terraform.</li> <li>Task-3: Create a Front Door Endpoint using Terraform.</li> <li>Task-4: Create a Front Door Origin Group using Terraform.</li> <li>Task-5: Create a Front Door Origin using Terraform.</li> <li>Task-6: Create custom domains for the Front Door using Terraform.</li> <li>Task-7: Create a Front Door Route using Terraform</li> <li>Task-8: Create a DNS TXT (temporary) record in DNS Zone</li> <li>Task-9: Create DNS CNAME records in DNS Zone</li> <li>Task-10: Configure diagnostic settings for CDN profile</li> <li>Task-11: Apply lock on Front Door Profile</li> </ul> </li> <li>Lab-14: Create Storage Account using terraform<ul> <li>Task-1: Define and declare azure storage account variables</li> <li>Task-2: Create azure storage account using terraform</li> <li>Task-3: Create azure storage account container using terraform</li> <li>Task-4: Configure diagnostic settings for azure storage account using terraform</li> <li>Task-5: Configure diagnostic settings for azure storage account container using terraform</li> <li>Task-6: Create storage account's Files Share using terraform</li> <li>Task-7: Restrict Access Using Private Endpoint<ul> <li>Task-7.1: Configure the Private DNS Zone</li> <li>Task-7.2: Create a Virtual Network Link Association</li> <li>Task-7.3: Create Private Endpoints for azure Storage</li> <li>Task-7.4: Validate private link connection using nslookup or dig</li> </ul> </li> </ul> </li> <li>Lab-15: Azure Event Hubs for Apache Kafka Introduction Part-1<ul> <li>What is Azure Event Hub?</li> <li>Components of Azure Event Hubs</li> <li>Why Azure Event Hubs?</li> <li>Can you explain Apache Kafka on Azure Event Hubs?</li> <li>Azure Event Hubs vs Apache Kafka</li> <li>What is Azure Event Hubs namespace?</li> <li>What are Partitions?</li> <li>What are Shared Access Policies?</li> <li>What are Event publishers?</li> <li>What are Event consumers?</li> <li>What is Event retention?</li> <li>Explain Capture events:</li> <li>Consumer groups</li> <li>Application groups</li> <li>What is Avro format in Apache Kafka?</li> <li>What is Azure Schema Registry?</li> </ul> </li> <li> <p>Lab-15: Create Azure Event Hubs for Apache Kafka using Terraform Part-2</p> <ul> <li>Task-1: Define and declare azure event hubs variables.</li> <li>Task-2: Create storage account resources using terraform.</li> <li>Task-3: Create Kafka azure event hubs namespace.</li> <li>Task-4: Create diagnostic settings for event hub namespace.</li> <li>Task-5: Shared access policies for event hub namespace Level<ul> <li>Task-5.1: Create shared access policy rule for listen.</li> <li>Task-5.2: Create shared access policy rule for send.</li> <li>Task-5.3: Create shared access policy rule for manage.</li> </ul> </li> <li>Task-6: Restrict access using private endpoint &amp; virtual network.<ul> <li>Task-6.1: Configure the private DNS zone</li> <li>Task-6.2: Create a virtual network link association</li> <li>Task-6.3: Create private endpoints for azure event hubs</li> <li>Task-6.4: Validate private link connection using nslookup or dig</li> </ul> </li> <li>Task-7: Create azure event hubs or Kafka topics.</li> </ul> </li> <li> <p>Lab-16: Create Virtual Machine (Jumpbox) using terraform</p> </li> <li>Lab-17: Configure Private Endpoint &amp; Private Links using terraform</li> <li>Lab-18: Azure DNS setup</li> </ul> <p>Chapter 3: Kubernetes - Azure Kubernetes Service (AKS)</p> <ul> <li>Prepare an application for Azure Kubernetes Service (AKS)</li> <li>Deploying an application to Azure Kubernetes Service (AKS)<ul> <li>Step-1: Create a new namespace</li> <li>Step-1: Create a Kubernetes deployment manifest</li> <li>Step-2: Create a Kubernetes service manifest</li> <li>Step-3: Apply the manifests to your AKS cluster</li> <li>Step-4: Port forwarding</li> <li>Step-5: Expose the service externally</li> <li>Step-6: Verify that the application is running</li> </ul> </li> <li>Working with AKS cluster using kubectl<ul> <li>Login to azure &amp; set subscription</li> <li>Connect to AKS cluster</li> <li>Test workloads in AKS</li> <li>Cluster-info</li> <li>cluster resources </li> <li>Cluster details</li> <li>kubectl commands</li> <li>Troubleshooting errors</li> <li>install kubelogin</li> </ul> </li> <li>Setup NGINX ingress controller in AKS using Terraform<ul> <li>Step-1: Configure Terraform providers</li> <li>Step-2: Create a new namespace for ingress</li> <li>Step-2: Install ingress resources with helm-chart using terraform</li> <li>Step 3. Verify Ingress-Nginx resources in AKS.</li> <li>Step 4: Deploy sample applications for Ingress testing</li> <li>Step-5: Create an ingress route</li> <li>Step-6: Test the ingress controller</li> <li>Step-7: Add DNS recordset in DNS Zone</li> </ul> </li> <li>Kubernetes Pod troubleshooting<ul> <li>Login to Azure</li> <li>Connect to Cluster</li> <li>Verify the pod status</li> <li>Describe the pod</li> <li>Check the logs</li> <li>Debug the container</li> <li>Exit from the container</li> <li>Install curl</li> <li>Curl internal pod URL</li> <li>Restart the pod:</li> </ul> </li> </ul> <p>Chapter 4: Azure DevOps - Build &amp; Release Pipelines</p> <ul> <li>Azure DevOps Overview</li> <li>Create new service connections</li> <li>Create Pipeline for .NET Core Web API</li> <li>Create Pipeline for ASP.NET Core Website</li> <li>Create Pipeline for Node JS API</li> <li>Create Pipeline for React JS website</li> <li>Create Pipeline for Database deployment</li> </ul> <p>Chapter 5: Argo CD - Deployments to Kubernetes - Getting started with Argo CD     - What is GitOps?     - Core components of GitOps     - What is ArgoCD?     - How ArgoCD works?     - Why do we need to use ArgoCD?     - Key features of ArgoCD     - ArgoCD architecture components     - ArgoCD supports tools - Install ArgoCD     - Step-1: Configure Terraform providers     - Step-2: Create namespace for ArgoCD     - Step-3: Install argocd in AKS with helm-chart using terraform     - Step 4. Verify ArgoCD resources in AKS.     - Step-5. port forwarding for login screen     - Step-6. ArgoCD login with localhost - Install ArgoCD CLI     - Step 1.Install ArgoCD CLI     - Step 2. Access the ArgoCD API Server     - Step 3. Login to ArgoCD     - Step 4. Logout ArgoCD - Registering an AKS Cluster with ArgoCD - Deploy your first application with ArgoCD     - Task-1: Creating ArgoCD application using ArgoCD CLI     - Task-2: Creating ArgoCD application using ArgoCD UI     - Task-3: Creating ArgoCD application using YAML manifest - Create new ArgoCD Project - Create new Repository in ArgoCD - Helm-Charts Introduction     - Introduction     - What is Helm Chart?     - When do we need to use helm chart?     - key features of Helm     - How it works?     - Explain Helm Architecture</p> <p>Chapter 6: Helm-Charts - Deployments to Kubernetes</p> <ul> <li>Install Cert-Manager &amp; Let's Encrypt helm charts on Kubernetes</li> <li>Install argocd helm chart using terraform</li> <li>Install pgadmin4 helm chart using terraform</li> <li>Install minio helm chart using terraform</li> <li>Helm hooks examples in Kubernetes</li> </ul>"},{"location":"gettingstarted/toc/","title":"Table of Contents","text":"<ul> <li>About the Author</li> <li>Acknowledgments</li> <li>Introduction</li> </ul> <p>Chapter-1: Building Containerized Microservices</p> <ul> <li>Lab-1: Getting Started with Microservices</li> <li>Lab-2: Getting Started with Docker</li> <li>Lab-3: Create Your First Microservice with .NET Core Web API</li> <li>Lab-4: Create Your Second Microservice with Node.js</li> <li>Lab-5: Create Your First Website using .NET Core MVC Application</li> <li>Lab-6: Create Your Second Website using React.js</li> <li>Lab-7: Create your First Database with SQL Server</li> <li>Lab-8: Create your First Database with PostgreSQL</li> <li>Lab-9: Setting up Keycloak in a Docker Container</li> <li>Lab-10: Setting up Drupal in a Docker Container</li> </ul> <p>Chapter-2: Create Azure Infrastructure with Terraform</p> <ul> <li>Lab-1: Infrastructure as Code (IaC) </li> <li>Lab-2: Microservices Architecture on AKS</li> <li>Lab-3: Azure Account &amp; Subscription Management</li> <li>Lab-4: Create Terraform Foundation Part-1</li> <li>Lab-5: Create Terraform Foundation Part-2</li> <li>Lab-6: Create Log Analytics Workspace using terraform</li> <li>Lab-7: Create Virtual Network using terraform</li> <li>Lab-8: Create Azure Container Registry (ACR) using terraform</li> <li>Lab-9: Create Azure Kubernetes Service (AKS) using terraform</li> <li>Lab-10: Create Azure PostgreSQL - Flexible Server using terraform</li> <li>Lab-11: Create Azure Key Vault using Terraform</li> <li>Lab-12: Create Azure Cache for Redis using Terraform</li> <li>Lab-13: Create Front Door and CDN profile using Terraform</li> <li>Lab-14: Create Storage Account using terraform</li> <li>Lab-15: Azure Event Hubs for Apache Kafka Introduction Part-1</li> <li>Lab-16: Create Azure Event Hubs for Apache Kafka using Terraform Part-2</li> </ul> <p>Chapter-3: Prepare Azure Kubernetes Service (AKS) for Microservices</p> <ul> <li>Lab-1: Getting Started with Docker Container</li> <li>Lab-2: Prepare an application for Azure Kubernetes Service (AKS)</li> <li>Lab-3: Deploying an .NET Core API to Azure Kubernetes Service (AKS)</li> <li>Lab-4: Deploying an ASP.NET Core MVC to Azure Kubernetes Service (AKS)</li> <li>Lab-5: Working with AKS cluster using kubectl</li> <li>Lab-6: Setup NGINX ingress controller in AKS using Terraform</li> <li>Lab-7: Setup Application Gateway ingress controller (AGIC) in AKS using Terraform</li> <li>Lab-8: Setup Cert-Manager in AKS using Terraform</li> <li>Lab-9: Issue the Let's Encrypt SSL Certificate to the Website</li> <li>Lab-10: Troubleshooting Problems with Let's Encrypt Certificates</li> <li>Lab-11: Integrating Azure Key Vault with AKS using Terraform</li> <li>Lab-12: Kubernetes Pod troubleshooting</li> <li>Lab-13: Create a new user node pool in AKS</li> <li>Lab-14: Upgrade or Resize node pools in AKS</li> </ul> <p>Chapter 4: Azure DevOps - Build &amp; Release Pipelines</p> <ul> <li>Azure DevOps Overview</li> <li>Create new service connections</li> <li>Create Pipeline for .NET Core Web API</li> <li>Create Pipeline for ASP.NET Core Website</li> </ul> <p>Chapter 5: Argo CD - Continuous Deployment and GitOps</p> <ul> <li>Getting started with Argo CD</li> <li>Install Argo CD in AKS with helm chart using terraform</li> <li>Install &amp; Interact with ArgoCD CLI</li> <li>Registering an AKS Cluster with Argo CD</li> <li>Deploy Your First application with Argo CD</li> <li>Create new ArgoCD Project</li> <li>Create new Repository in ArgoCD</li> </ul> <p>Chapter 6: Helm-Charts - Managing Kubernetes Deployments</p> <ul> <li>Introduction to Helm Charts</li> <li>Create Your First Helmchart</li> <li>Frequently used Helm commands details</li> <li>Create Helmchart for Microservices</li> </ul>"},{"location":"helmchart/1-introduction/","title":"Introduction to Helmcharts","text":"<p>Chapter-5: Helm: Managing Kubernetes Deployments</p>"},{"location":"helmchart/1-introduction/#introduction-to-helm-charts","title":"Introduction to Helm Charts","text":""},{"location":"helmchart/1-introduction/#introduction","title":"Introduction","text":"<p>One of my goal is to simplify the development, deployment, and scaling of complex applications and to bring the full power of Kubernetes to all projects. </p> <p>One of the key tools we use from the Kubernetes ecosystem is <code>Helm.</code></p> <p><code>Helm</code> is the package manager for Azure Kubernetes Cluster (AKS) deployments. Helm uses a packaging format called charts. A <code>chart</code> is a collection of manifest files that describe a related set of AKS Cluster Kubernetes resources.</p> <p>Helm charts deployment make perfect sense for the complex workload deployments.</p>"},{"location":"helmchart/1-introduction/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn following:</p> <ul> <li>What is Helm Chart?</li> <li>How helm chart works?</li> <li>When do we need to use helm chart?</li> <li>Key Features of Helm</li> <li>Explain Helm Architecture</li> </ul>"},{"location":"helmchart/1-introduction/#what-is-helm-chart","title":"What is Helm Chart?","text":"<p>Helm is a package manager for Kubernetes that allows you to manage and install software in your cluster. A Helm chart is a package that contains all of the necessary configuration files and resources to deploy an application in a Kubernetes cluster.</p> <p>A Helm chart includes information such as the resources to be deployed (e.g., pods, services, and ingress), the container images to be used, and the configuration values for the deployment. Charts can be stored in a central repository, making it easy to share and reuse applications and components across multiple clusters and teams.</p> <p>Using Helm, you can install and manage complex applications with a single command, simplifying the process of deploying and managing applications in your cluster. Helm charts also make it easier to manage versioning and upgrades of your applications, as well as rollback to previous versions if necessary.</p>"},{"location":"helmchart/1-introduction/#when-do-we-need-to-use-helm-chart","title":"When do we need to use helm chart?","text":"<p>You might need to use Helm charts when deploying and managing applications in a Kubernetes cluster. Some of the key reasons to use Helm charts include:</p> <ul> <li>Simplified application deployment: Helm charts make it easier to deploy and manage complex applications, as you can define the resources and configuration in a single package, and then deploy and upgrade the application with a single command.</li> <li>Reusable components: Helm charts can be stored in a central repository, making it easy to share and reuse components across multiple clusters and teams.</li> <li>Versioning and upgrades: Helm charts provide versioning and upgrade capabilities, making it easier to manage the lifecycle of your applications and rollback to previous versions if necessary.</li> <li>Customization: Helm charts provide a flexible and customizable way to manage deployments, as you can define variables and configuration values that can be modified during installation.</li> <li>Dependency management: Helm charts can manage dependencies between different components, making it easier to deploy and manage complex applications that consist of multiple components.</li> <li>Scalability: Helm charts are designed to be scalable, making it easy to manage deployments across multiple clusters and teams.</li> </ul> <p>Helm charts are a powerful tool for automating and simplifying the process of deploying and managing applications in a Kubernetes cluster. If you need to deploy and manage applications in a large-scale or complex environment, using Helm charts might be a good option to consider.</p>"},{"location":"helmchart/1-introduction/#key-features-of-helm","title":"key features of Helm","text":"<ul> <li> <p>\u200bTemplates: - allows easily customize deployments for different environments.\u200b</p> </li> <li> <p>Dependencies: making it easy to manage and deploy complex, multi-tier applications\u200b</p> </li> <li> <p>Versioning: makes it easy to roll back to a previous version.\u200b</p> </li> <li> <p>Declarative Deployments: define the desired state of a deployment.\u200b</p> </li> <li> <p>Rollbacks: allow to rollback deployments to the previous version.\u200b</p> </li> <li> <p>Namespaces: enables easy manage resource allocation and security within the cluster. This is very helpful for multitenant systems \u200b</p> </li> </ul>"},{"location":"helmchart/1-introduction/#how-it-works","title":"How it works?","text":"<p>A chart is organized as a collection of files inside of a directory called <code>Helm-Charts</code>.  Created separate folder for each deployment or application in side the templates folder and organized files for each resource separately.</p> <p>A chart typically includes the following files:</p> <ul> <li>Chart.yaml: contains metadata about the chart, such as its name, version, and description.</li> <li>values.yaml: contains default configuration values for the chart.</li> <li>templates directory: contains Kubernetes manifests (in YAML format) for the resources that the chart will create.</li> <li>charts directory: contains any dependent charts that the chart requires.</li> </ul> <p>When you run the <code>helm install</code> command, it installs the chart in the Kubernetes cluster by creating resources defined in the templates, based on the configuration values specified in the <code>values.yaml</code> file. You can also pass additional values to the command line, which will override the defaults from the <code>values.yaml</code> file.</p> <p>Once the application is deployed, Helm monitors the state of the resources and can perform updates or rollbacks if necessary. You can also use the Helm CLI to upgrade or delete the application, as well as manage the dependencies between different charts.</p> <p>In this way, Helm charts provide a flexible and scalable way to manage and deploy applications in your Kubernetes cluster, making it easier to automate and simplify the process of application management.</p>"},{"location":"helmchart/1-introduction/#explain-helm-architecture","title":"Explain Helm Architecture","text":"<p>The architecture of Helm consists of the following components:</p> <ul> <li> <p>Helm CLI: The Helm CLI is a command-line tool that allows you to manage and install Helm charts. The Helm CLI communicates with the Helm Tiller server to deploy charts in your cluster.</p> </li> <li> <p>Helm Tiller server: The Helm Tiller server is a component that runs in your Kubernetes cluster and is responsible for managing the installation and lifecycle of Helm charts. The Tiller server receives requests from the Helm CLI and communicates with the Kubernetes API to create, update, and manage the resources defined in the chart.</p> </li> <li> <p>Chart repository: The chart repository is a centralized storage location for Helm charts. It can be a local or remote repository, and it provides a way to share and distribute charts across multiple teams and clusters.</p> </li> <li> <p>Chart: A Helm chart is a package that contains all of the necessary configuration files and resources to deploy an application in a Kubernetes cluster. Charts can be stored in a chart repository and can be installed in your cluster using the Helm CLI.</p> </li> <li> <p>Template: The templates in a Helm chart are written in YAML format and use the Go Templating Language (Golang) to define variables and control structures. When you install a chart, the Helm Tiller server evaluates the templates and generates the necessary Kubernetes resources based on the configuration values defined in the chart.</p> </li> </ul> <p>The architecture of Helm provides a simple and flexible way to manage and deploy applications in your Kubernetes cluster, making it easier to automate and simplify the process of application management. The Helm CLI provides a user-friendly interface for interacting with the Tiller server, while the Tiller server communicates with the Kubernetes API to manage the resources in the cluster. The chart repository provides a centralized storage location for charts, making it easy to share and distribute applications across multiple teams and clusters.</p>"},{"location":"helmchart/1-introduction/#references","title":"References","text":"<ul> <li>Helm Docs - Charts</li> </ul>"},{"location":"helmchart/10-install-adcs-issuer/","title":"Install ADCS Issuer Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/10-install-adcs-issuer/#introduction","title":"Introduction","text":"<p>In this article, we will go over how to install the ADCS (Active Directory Certificate Services) Issuer Helm chart on an Azure Kubernetes Service (AKS) cluster. ADCS is a Windows-based Certificate Authority (CA) that is used to issue digital certificates. The ADCS Issuer integrates with Kubernetes and provides an option to issue certificates directly from Active Directory. </p> <p>ADCS Issuer is a\u00a0cert-manager\u2019s CertificateRequest controller that uses MS Active Directory Certificate Service to sign certificates. ADCS provides HTTP GUI that can be normally used to request new certificates or see status of existing requests.</p> <p>The ADCS Issuer allows us to configure Kubernetes to request certificates from an ADCS certificate authority and manage the certificates with Kubernetes resources. It simplifies the process of obtaining certificates, especially in a corporate environment where Active Directory is already in use.</p>"},{"location":"helmchart/10-install-adcs-issuer/#objective","title":"Objective","text":"<p>The objective of this guide is to show you how to:</p> <ol> <li>Install the ADCS Issuer Helm chart on AKS.</li> <li>Request a certificate from an ADCS server.</li> <li>Configure an Ingress resource to use the issued certificate.</li> <li>Verify that the certificates are correctly issued and used.</li> </ol>"},{"location":"helmchart/10-install-adcs-issuer/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, make sure you have the following prerequisites in place:</p> <ul> <li>Azure Account: An active Azure account with necessary permissions to manage resources.</li> <li>Azure CLI: Installed and configured on your local machine.</li> <li>kubectl: Installed and configured to interact with your AKS cluster.</li> <li>Helm: Helm is installed and configured to manage Kubernetes applications.</li> <li>Cert-Manager: Cert-manager is required for managing certificates in Kubernetes. It will be used as the controller to handle certificate requests, issuance, and renewal.</li> <li>Active Directory Certificate Services (ADCS): Access to an ADCS server for issuing certificates.</li> <li>AKS Cluster: An existing AKS cluster that can be connected to from your local machine.</li> </ul>"},{"location":"helmchart/10-install-adcs-issuer/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>First, log into your Azure account using the Azure CLI.</p> <p><pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> Follow the on-screen instructions to complete the login process.    </p>"},{"location":"helmchart/10-install-adcs-issuer/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Next, ensure that you are connected to your AKS cluster using the <code>kubectl</code> command. Replace <code>&lt;resource-group&gt;</code> and <code>&lt;aks-cluster-name&gt;</code> with your actual resource group and AKS cluster name.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-3-add-the-helm-repository","title":"Step 3: Add the Helm Repository","text":"<p>Now, we need to add the Helm chart repository that contains the ADCS Issuer chart. In this case, we will add the <code>djkormo-adcs-issuer</code> repository, which provides the <code>adcs-issuer</code> Helm charts.</p> <pre><code># add helm repo\nhelm repo add djkormo-adcs-issuer https://djkormo.github.io/adcs-issuer/\n\n# update \nhelm repo update djkormo-adcs-issuer\n\n# check all versions \nhelm search repo adcs-issuer  --versions\n\n# download values file for some version\n\nhelm show values djkormo-adcs-issuer/adcs-issuer --version 3.0.0 &gt; adcs-issuer-values.yaml\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-4-install-adcs-issuer-helm-chart","title":"Step 4: Install ADCS-Issuer Helm Chart","text":"<p>Now that you have the necessary repositories added, you can install the ADCS Issuer Helm chart. Replace <code>&lt;namespace&gt;</code> with the namespace where you want to install it.</p> <p>Install via commands <pre><code># test installation\nhelm install adcs-issuer  djkormo-adcs-issuer/adcs-issuer --version 2.1.1 \\\n  --namespace adcs-issuer --values values.yaml  --dry-run\n\n#  install\nhelm install adcs-issuer  djkormo-adcs-issuer/adcs-issuer --version 2.1.1 \\\n  --namespace adcs-issuer --values values.yaml  --dry-run\n\n# upgrade\nhelm upgrade project-operator djkormo-adcs-issuer/adcs-issuer  --version 2.1.1 \\\n  --namespace adcs-issuer --values values.yaml\n\n# uninstall \nhelm uninstall adcs-issuer  --namespace  adcs-issuer\n</code></pre></p> <p>Install via terraform</p> <pre><code>locals {\n  adcsissuer_values = \"adcsissuer/values.yaml\"\n}\n\nresource \"kubernetes_namespace\" \"adcsissuer\" {\n  metadata {\n    name = \"adcs-issuer\"\n  }\n\n  depends_on = [\n    data.azurerm_kubernetes_cluster.main\n  ]\n  lifecycle {\n    ignore_changes = [\n      metadata\n    ]\n  }\n}\n\nresource \"helm_release\" \"adcsissuer\" {\n  name       = \"adcs-issuer\"\n  namespace  = kubernetes_namespace.adcsissuer.metadata[0].name\n  repository = \"https://djkormo.github.io/adcs-issuer/\"\n  chart      = \"adcs-issuer\"\n  version    = var.helm_chart_adcsissuer_version\n\n  values = [\n    \"${file(local.adcsissuer_values)}\"\n  ]\n\n  depends_on = [\n    kubernetes_namespace.adcsissuer,\n    data.azurerm_kubernetes_cluster.main,\n  ]\n\n  lifecycle {\n    ignore_changes = [\n      # metadata\n    ]\n  }\n}\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-5-verify-adcs-issuer-resources-in-aks","title":"Step 5: Verify ADCS-Issuer Resources in AKS","text":"<p>To ensure that the ADCS Issuer was installed correctly, list the resources in your Kubernetes cluster:</p> <pre><code>kubectl get customresourcedefinitions | grep adcs\n</code></pre> <p>This should show the <code>adcsissuer</code> resource type, which is used to define the ADCS issuer. Additionally, check if the deployment and other resources are correctly created in the specified namespace:</p> <pre><code>helm list -aA\nhelm list --namespace adcs-issuer\n\nkubectl get deployments -n adcs-issuer\nkubectl get pods -n adcs-issuer\nkubectl get services -n adcs-issuer\n\nkubectl get all,secret,configmap -n adcs-issuer\n\n# check the pod logs\nkubectl logs pod/adcs-issuer-controller-manager-957fc79bf-8cg52 -n adcs-issuer\n</code></pre> <p>This should show the ADCS Issuer pods and related resources in the AKS cluster.</p>"},{"location":"helmchart/10-install-adcs-issuer/#step-6-create-certificate-resources-in-aks","title":"Step 6: Create Certificate Resources in AKS","text":"<p>Now we need to create the resources needed by the ADCS Issuer to request certificates from Active Directory.</p>"},{"location":"helmchart/10-install-adcs-issuer/#step-61-create-credentials-for-adcs","title":"Step 6.1: Create Credentials for ADCS","text":"<p>In this step, we will create a service account in Active Directory (AD) that will be used to authenticate and interact with the Active Directory Certificate Services (ADCS) to issue certificates.</p> <p>example:</p> <p><pre><code>username: svcd_aks_adcs\npassword: xxxx\n</code></pre> Then we need to use base64encoding these credentials and use it in aks secret object.</p> <p>We need to create a Kubernetes secret to store the credentials required to connect to the ADCS CA. The secret should contain the necessary certificates, credentials, and configurations.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: adcs-issuer-credentials\n  namespace: adcs-issuer\ntype: Opaque\ndata:\n  password: M2kxUkVuMTNBSW9TZ1piR3RYXzlgfH18dUFV\n  username: a3ZjZF9ha3NfYWRjcw==\n</code></pre> <p>Apply the configuration:</p> <pre><code>kubectl apply -f adcsissuer/adcs-issuer-credentials.yaml\n</code></pre> <p>This will create a secret named <code>adcs-issuer-credentials</code> that stores your ADCS credentials.</p>"},{"location":"helmchart/10-install-adcs-issuer/#step-62-create-adcs-issuer-object","title":"Step 6.2: Create ADCS-Issuer Object","text":"<p>Next, create an <code>ADCSIssuer</code> custom resource that specifies the ADCS server details and credentials. This resource will allow Kubernetes to request certificates from ADCS. Create a file called <code>adcs-issuer.yaml</code> and define the <code>ADCSIssuer</code> object:</p> <p>The ADCS service data can be configured in\u00a0<code>AdcsIssuer</code>\u00a0or\u00a0<code>ClusterAdcsIssuer</code>\u00a0CRD objects e.g.:</p> <pre><code>apiVersion: adcs.certmanager.csf.nokia.com/v1\nkind: ClusterAdcsIssuer\nmetadata:\n\u00a0\u00a0name: adcs-cluster-issuer\nspec:\n\u00a0\u00a0caBundle: REDACTED # ca certificate\n\u00a0\u00a0credentialsRef:\n\u00a0\u00a0\u00a0\u00a0name: adcs-issuer-credentials # secret with username and password\n\u00a0\u00a0statusCheckInterval: 5m\n\u00a0\u00a0retryInterval: 5m\n  url: https://adcs-host/ # external host\n\u00a0\u00a0templateName: adcsTemplate # external template\n</code></pre> <ul> <li> <p>The\u00a0<code>caBundle</code>\u00a0parameter is BASE64-encoded CA certificate which is used by the ADCS server itself.</p> </li> <li> <p>The\u00a0<code>statusCheckInterval</code>\u00a0indicates how often the status of the request should be tested. Typically, it can take a few hours or even days before the certificate is issued.</p> </li> <li> <p>The\u00a0<code>retryInterval</code>\u00a0says how long to wait before retrying requests that errored.</p> </li> <li> <p>The\u00a0<code>credentialsRef.name</code>\u00a0is name of a secret that stores user credentials used for NTLM authentication. The secret must be\u00a0<code>Opaque</code>\u00a0and contain\u00a0<code>password</code>\u00a0and\u00a0<code>username</code>\u00a0fields only </p> </li> </ul> <p>Apply the configuration:</p> <p><pre><code>kubectl apply -f adcsissuer/dev/clusterissuer-adcs.yaml\n</code></pre> Note: download the certificate from AD and base64encode</p> <p></p> <p>Validate ClusterAdcsIssuer </p> <pre><code>kubectl get ClusterAdcsIssuer -n adcs-issuer\n\nkubectl describe ClusterAdcsIssuer/adcs-cluster-issuer\nkubectl edit ClusterAdcsIssuer/adcs-cluster-issuer\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-7-request-certificate-for-a-custom-domain","title":"Step 7: Request Certificate for a Custom Domain","text":"<p>Once the ADCS Issuer is configured, you can request a certificate for your custom domain. The certificate request is initiated by creating a <code>Certificate</code> resource that references the ADCS Issuer. For example, create a <code>certificate.yaml</code> file:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  annotations:\n  name: adcs-cert\n  namespace: &lt;namespace&gt;\nspec:\n  commonName: example.com\n  dnsNames:\n  - service1.example.com\n  - service2.example.com\n  issuerRef:\n    group: adcs.certmanager.csf.nokia.com\n    kind: AdcsIssuer\n    name: test-adcs\n  organization:\n  - Your organization\n  secretName: adcs-cert\n</code></pre> <p>Apply the file: <pre><code>kubectl apply -f certificate.yaml\n</code></pre></p> <p>This will request a certificate from the ADCS server. Cert-manager is responsible for creating the\u00a0<code>Secret</code>\u00a0with a key and\u00a0<code>CertificateRequest</code>\u00a0with proper CSR data.</p> <p>ADCS Issuer creates\u00a0<code>AdcsRequest</code>\u00a0CRD object that keep actual state of the processing. Its name is always the same as the corresponding\u00a0<code>CertificateRequest</code>\u00a0object</p> <p>Validate Certificate objects</p> <pre><code>kubectl get AdcsRequest -n adcs-issuer\nkubectl get Certificate -n adcs-issuer\n\nkubectl describe Certificate/adcs-cert -n adcs-issuer\n\nkubectl get CertificateRequest -n adcs-issuer\nkubectl describe CertificateRequest/adcs-cert-1 -n adcs-issuer`\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-8-configure-ingress-for-a-domain","title":"Step 8: Configure Ingress for a Domain","text":"<p>After the certificate is issued, you need to configure an Ingress resource to use this certificate for your domain.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/issuer: argocd-adcs-issuer # issuser name\n    cert-manager.io/issuer-kind: AdcsIssuer # ClusterAdcsIssuer or AdcsIssuer\n    cert-manager.io/issuer-group: adcs.certmanager.csf.nokia.com # api group, here adcs.certmanager.csf.nokia.com\n    cert-manager.io/renew-before: 48h # renew 48 hour before\n\n  name: argo-cd-argocd-server\n  namespace: argocd\n\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: argocd.sample.host\n    http:\n      paths:\n      - backend:\n          service:\n            name: argocd-server\n            port:\n              number: 443\n        path: /(.*)\n        pathType: Prefix\n  tls:\n  - hosts:\n    - argocd.sample.host\n    secretName: argocd-tls-certificate # secret for storing certificate\n</code></pre>"},{"location":"helmchart/10-install-adcs-issuer/#step-9-validate-new-certificate-object","title":"Step 9: Validate New Certificate Object","text":"<p>To ensure the certificate is successfully issued, check the status of the <code>Certificate</code> resource:</p> <pre><code>kubectl -n argocd get certificate,certificaterequests\n\n# check logs\nkubectl logs -n cert-manager -l app=cert-manager\n</code></pre> <p>The status should show that the certificate is ready and has been issued by the ADCS Issuer.</p>"},{"location":"helmchart/10-install-adcs-issuer/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to deploy ADCS-Issue on Azure Kubernetes Services using Helm charts. We have learned how to set up the ADCS Issuer, request a certificate, and configure an Ingress resource to use the issued certificate for new domain.</p>"},{"location":"helmchart/10-install-adcs-issuer/#reference","title":"Reference","text":"<ul> <li>djkormo - Documentation</li> <li>djkormo - Github</li> <li>nokia/adcs-issuer- Github</li> <li>artifacthub</li> <li>Cert-Manager</li> <li>Azure Kubernetes Service (AKS) Documentation</li> </ul>"},{"location":"helmchart/11-install-jaeger/","title":"Install Jaeger Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/11-install-jaeger/#introduction","title":"Introduction","text":"<p>Distributed tracing is a critical component in modern observability, especially when we're working with microservices architectures. Jaeger is one of the most popular open-source distributed tracing tools, and with Helm, it\u2019s easy to deploy and manage on Kubernetes.</p> <p>In this article, we will walk through how to install the Jaeger Helm Chart in Azure Kubernetes Service (AKS). Helm simplifies application deployment on Kubernetes, and combining it with Jaeger allows you to get valuable insights into your services' latency, dependencies, and performance metrics.</p>"},{"location":"helmchart/11-install-jaeger/#objective","title":"Objective","text":"<p>The objective of this article is to provide step-by-step instructions to install Jaeger on an Azure Kubernetes Service (AKS) cluster using Helm, access the Jaeger UI locally, expose it through Ingress, and use Jaeger for distributed tracing. </p> <p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Login into Azure</li> <li>Step 2. Connect to AKS Cluster</li> <li>Step 3. Add the Jaeger Helm Chart Repository</li> <li>Step 4. Install Jaeger with Helm</li> <li>Step 5. Install Jaeger Helmchart using terraform</li> <li>Step 6. Verify Jaeger Installation in AKS</li> <li>Step 7. Access Jaeger Locally - port forwarding</li> <li>Step 8. Configure Ingress for Jaeger UI</li> <li>Step 9. Jaeger Integration with OpenTelemetry</li> <li>Step 10. Uninstalling the Chart</li> </ul>"},{"location":"helmchart/11-install-jaeger/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation of Jaeger in AKS, ensure that you have the following prerequisites:</p> <ul> <li>Azure CLI \u2013 Ensure that the Azure CLI is installed on your local machine.  Install Azure CLI</li> <li>kubectl \u2013 Kubernetes command-line tool installed.  Install kubectl</li> <li>Helm \u2013 Helm should be installed for managing Kubernetes applications.   Install Helm</li> <li>Azure Subscription \u2013 You need an Azure account and sufficient permissions to create AKS clusters.    </li> <li>Terraform (Optional) \u2013 If you choose to install Jaeger via Terraform, ensure Terraform is installed.  Install Terraform</li> <li>AKS Cluster \u2013 An existing AKS cluster should be available, or you can create one using the Azure CLI.</li> </ul>"},{"location":"helmchart/11-install-jaeger/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Ensure that you are logged into the correct Azure subscription before proceeding.</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/11-install-jaeger/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/11-install-jaeger/#step-3-add-the-jaeger-helm-chart-repository","title":"Step 3: Add the Jaeger Helm Chart Repository","text":"<p>Jaeger\u2019s Helm chart is hosted in the GitHub / Artifact. To begin, add the Jaeger Helm repository to your Helm setup:</p> <ul> <li>GitHub reference</li> <li>Artifact Hub reference</li> </ul> <p>List all added Helm repositories:</p> <pre><code>helm repo list\n</code></pre> <p>Add the Jaeger Helm repository:</p> <pre><code>helm repo add jaegertracing https://jaegertracing.github.io/helm-charts\n</code></pre> <p>Update the Helm repositories:</p> <pre><code>helm repo update jaegertracing\n</code></pre>"},{"location":"helmchart/11-install-jaeger/#step-4-install-jaeger-helmchart","title":"Step 4: Install Jaeger Helmchart","text":"<p>Now, you can install Jaeger using Helmchart. Execute the following command:</p> <p>Let's first search the helm chart which we want to install. here I am installing official jaegertracing helmchart.</p> <p>Search for available charts and versions in the Jaeger repository:</p> <pre><code>helm search repo jaegertracing\n</code></pre> <p>output</p> <pre><code>NAME                            CHART VERSION   APP VERSION     DESCRIPTION\njaegertracing/jaeger            3.4.1           1.53.0          A Jaeger Helm chart for Kubernetes       \njaegertracing/jaeger-operator   2.57.0          1.61.0          jaeger-operator Helm chart for Kubernetes\n</code></pre> <p>Let's first see the values:</p> <pre><code>helm show values jaegertracing/jaeger --version 3.4.1 &gt; jaeger\\values.yaml\n</code></pre> <p>Customize the values.yaml file for OpenTelemetry Integration later.</p> <pre><code># before \ncassandra: true\n# after\ncassandra: false\n\n# before \nallInOne:\n  enabled: false\n\n# after\nallInOne:\n  enabled: true\n\n# before \nstorage:  \n  type: cassandra\n# after\nstorage:  \n  type: memory\n\n# before \nagent:\n  enabled: true\n# after\nagent:\n  enabled: false\n\n# before \ncollector:\n  enabled: true\n\n# after\ncollector:\n  enabled: false\n\n# before \nquery:\n  enabled: true\n# after\nquery:\n  enabled: false\n</code></pre> <p>Install : use this command if you need to create namespace along with helm install</p> <pre><code>helm install jaeger jaegertracing/jaeger -n jaeger --create-namespace --version 3.4.1 --values jaeger1\\values.yaml\n</code></pre> <p>Upgrade : use this command if you already have namespace created</p> <pre><code>helm upgrade --install jaeger jaegertracing/jaeger -n jaeger --version 3.4.1\n</code></pre> <p></p> <p>This command installs Jaeger in your AKS cluster. You can customize the installation by providing values files or using Helm chart options.</p>"},{"location":"helmchart/11-install-jaeger/#step-5-install-jaeger-helmchart-using-terraform","title":"Step 5. Install Jaeger Helmchart using terraform","text":"<p>Declare Variables for Jaeger Installation</p> <p>To begin with, we\u2019ll define a few Terraform variables that will control whether Jaeger is enabled and specify the version of Jaeger we want to install.</p> <pre><code># declare variables section\nvariable \"jaeger_enabled\" {\n  description = \"Use this variable to skip the Jaeger Helm install\"\n  type        = bool\n  default     = true\n}\n\nvariable \"jaeger_version\" {\n  type    = string\n  default = \"3.4.1\"\n}\n</code></pre> <ul> <li><code>jaeger_enabled</code>: This boolean variable allows you to control whether Jaeger should be installed. Set it to <code>true</code> to install Jaeger, or <code>false</code> to skip the installation.</li> <li><code>jaeger_version</code>: This specifies the version of the Jaeger Helm chart to install. In this example, we\u2019re using version <code>3.4.0</code>.</li> </ul> <p>Create a Namespace for Jaeger Resources</p> <p>Next, we need to create a dedicated namespace for Jaeger resources in the Kubernetes cluster. We'll ensure that the namespace is created only if Jaeger installation is enabled (<code>jaeger_enabled</code> is <code>true</code>).</p> <pre><code># create a new namespace for Jaeger resources \nresource \"kubernetes_namespace\" \"jaeger\" {\n  count = var.jaeger_enabled ? 1 : 0\n  metadata {\n    name = \"jaeger\"\n  }\n  depends_on = [\n    azurerm_kubernetes_cluster.aks\n  ]\n}\n</code></pre> <p>This step ensures that all Jaeger-related resources are deployed in a dedicated <code>jaeger</code> namespace, which keeps the cluster organized. The <code>depends_on</code> ensures the Kubernetes cluster is available before the namespace creation.</p> <p>Install Jaeger Using Helm Chart</p> <p>Now, we will use the <code>helm_release</code> resource in Terraform to install the Jaeger Helm chart into the Kubernetes cluster.</p> <pre><code># Install Jaeger Helm chart using Terraform\nresource \"helm_release\" \"jaeger\" {\n  count      = var.jaeger_enabled ? 1 : 0\n  name       = \"jaeger\"\n  repository = \"https://jaegertracing.github.io/helm-charts\"\n  chart      = \"jaeger\"\n  version    = var.jaeger_version\n  namespace  = kubernetes_namespace.jaeger[0].metadata.0.name\n  values = [\n    \"jaeger1/values.yaml\"\n  ]\n  depends_on = [\n    azurerm_kubernetes_cluster.aks,\n    kubernetes_namespace.jaeger\n  ]\n}\n</code></pre> <ul> <li>name: Defines the name of the Helm release, in this case, <code>jaeger</code>.</li> <li>repository: Specifies the repository URL where the Jaeger Helm chart is hosted.</li> <li>chart: The name of the chart you want to install, which is <code>jaeger</code>.</li> <li>version: This is set to the version you declared earlier (<code>3.4.0</code>).</li> <li>namespace: This specifies the <code>jaeger</code> namespace where the release will be installed.</li> <li>values: The path to your <code>values.yaml</code> file, which can be customized as per your Jaeger configuration needs.</li> <li>depends_on: Ensures that the Kubernetes cluster and namespace are created before deploying Jaeger.</li> </ul> <p>Apply the Terraform Configuration</p> <p>Once you\u2019ve defined your Terraform configuration, you can run the following commands to apply the changes and install Jaeger in your Kubernetes cluster.</p> <pre><code>terraform init\nterraform plan\nterraform apply\n</code></pre> <ul> <li><code>terraform init</code>: Initializes your Terraform configuration.</li> <li><code>terraform plan</code>: plan your Terraform configuration.</li> <li><code>terraform apply</code>: Applies the configuration, provisioning the Jaeger Helm chart on your Kubernetes cluster.</li> </ul>"},{"location":"helmchart/11-install-jaeger/#step-6-verify-jaeger-resources-in-aks","title":"Step 6: Verify Jaeger Resources in AKS","text":"<p>To verify that Jaeger has been successfully installed, you can list the Kubernetes resources:</p> <p><pre><code>helm list --namespace jaeger\n</code></pre> output</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\njaeger  jaeger          1               2025-03-23 19:38:38.1348651 -0700 PDT   deployed        jaeger-3.4.1    1.53.0\n</code></pre> <pre><code>kubectl get pods -n jaeger\n# List all pods in the default namespace\nkubectl get all -n jaeger\nkubectl get configmap,secret,ingress,all -n jaeger\n</code></pre> <p>output</p> <p><pre><code>NAME                         DATA   AGE\nconfigmap/kube-root-ca.crt   1      12m\n\nNAME                                  TYPE                 DATA   AGE\nsecret/sh.helm.release.v1.jaeger.v1   helm.sh/release.v1   1      3m32s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/jaeger-bb7764c48-4bfbb   1/1     Running   0          3m31s\n\nNAME                       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                                                    AGE\nservice/jaeger-agent       ClusterIP   None         &lt;none&gt;        5775/UDP,5778/TCP,6831/UDP,6832/UDP                        3m32s\nservice/jaeger-collector   ClusterIP   None         &lt;none&gt;        9411/TCP,14250/TCP,14267/TCP,14268/TCP,4317/TCP,4318/TCP   3m32s\nservice/jaeger-query       ClusterIP   None         &lt;none&gt;        16686/TCP,16685/TCP                                        3m32s\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/jaeger   1/1     1            1           3m32s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/jaeger-bb7764c48   1         1         1       3m32s\n</code></pre> You should see Jaeger pods and services running in your cluster.</p> <p>Describe Jaeger pod for more details</p> <pre><code>kubectl describe pod/jaeger-bb7764c48-4bfbb -n jaeger\n</code></pre> <p>Describe Jaeger pod logs for more details</p> <pre><code>kubectl logs pod/jaeger-bb7764c48-4bfbb -n jaeger\n</code></pre>"},{"location":"helmchart/11-install-jaeger/#step-7-access-jaeger-locally-port-forwarding","title":"Step 7: Access Jaeger Locally - port forwarding","text":"<p>To connect to your Jaeger server using a client, you need to create a port forward:</p> <pre><code>kubectl port-forward service/jaeger-query -n jaeger 16686:16686\nor\nkubectl port-forward service/jaeger-query -n jaeger 80:16686\n</code></pre> <p>To access the Jaeger web UI:</p> <pre><code>http://localhost:16686/search\n# or\nhttp://localhost:80\n</code></pre> <p>Now, you can access Jaeger by opening your web browser. Use the default username (admin) to log in.</p> <p>After running the above commands, you should be able to access Jaeger  in your web browser. </p> <p>http://localhost:16686/search</p> <p>Jaeger &gt; Login page</p> <p></p>"},{"location":"helmchart/11-install-jaeger/#step-8-configure-ingress-for-jaeger","title":"Step 8: Configure Ingress for Jaeger","text":"<p>If you want to access Jaeger externally, you can configure Ingress. First, create an Ingress resource:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: jaeger-ingress\n  namespace: jaeger\nspec:\n  rules:\n  - host: jaeger.example.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: jaeger-query\n            port:\n              number: 16686\n</code></pre> <p>Apply the Ingress configuration:</p> <pre><code>kubectl apply -f jaeger-ingress.yaml\n</code></pre>"},{"location":"helmchart/11-install-jaeger/#step-9-jaeger-integration-with-opentelemetry","title":"Step 9: Jaeger Integration with OpenTelemetry","text":"<p>When integrating Jaeger with OpenTelemetry (OTEL) and deploying it on Azure Kubernetes Service (AKS), there are some specific adjustments you'll need to make to your setup. The OpenTelemetry Operator and OpenTelemetry Collector play key roles in this integration, and the configuration of Jaeger needs to be aligned with the OpenTelemetry ecosystem. Here\u2019s an updated version of the <code>values.yaml</code> and the overall architecture, with the changes aligned to the OpenTelemetry Operator and OpenTelemetry Collector integration:</p> <ul> <li> <p>OpenTelemetry Operator manages OpenTelemetry resources like the OpenTelemetry Collector, which is a vendor-agnostic component that can receive, process, and export telemetry data from your applications.  The OpenTelemetry Operator helps deploy and manage the OpenTelemetry Collector and other OpenTelemetry components in Kubernetes.</p> </li> <li> <p>OpenTelemetry Collector will receive trace data from your microservices and forward it to Jaeger for storage and visualization.   The OpenTelemetry Collector will act as the intermediary between your applications and Jaeger. The collector receives telemetry data from OpenTelemetry-instrumented services (using the OpenTelemetry SDK or auto-instrumentation) and forwards that data to Jaeger.</p> </li> <li> <p>Jaeger will act as the trace storage backend, which stores and visualizes trace data collected by the OpenTelemetry Collector. In this setup:</p> </li> </ul> <p>Customization of <code>values.yaml</code> Aligned with OpenTelemetry Integration</p> <p>Given that we're using OpenTelemetry along with Jaeger, the <code>values.yaml</code> customizations should reflect the fact that OpenTelemetry will handle the tracing and then forward it to Jaeger. Here\u2019s the corrected and aligned configuration:</p> <p>1. Disable Cassandra Storage for Jaeger</p> <pre><code># before \ncassandra: true\n# after \ncassandra: false\n</code></pre> <p>Explanation: By default, Jaeger is configured to use Cassandra as its storage backend. However, in an OpenTelemetry setup, you might not want to use Cassandra due to scalability or configuration complexities, especially in a simple setup. For OpenTelemetry, we are likely configuring Jaeger to use a more lightweight storage solution (such as memory) instead of Cassandra, or you may have already set up an external storage solution for OpenTelemetry traces (like Elasticsearch, or other cloud-native backends). Disabling Cassandra is necessary to avoid unnecessary resource usage and conflicts.</p> <p>2. Enable All-in-One Jaeger Mode</p> <pre><code># before \nallInOne:\n  enabled: true\n\n# after \nallInOne:\n  enabled: false\n</code></pre> <p>Explanation: In the default setup, Jaeger runs in a distributed fashion, with separate components for the agent, collector, and query services. In contrast, when integrating Jaeger with OpenTelemetry in a simpler environment (such as for development, testing, or small-scale usage), you often use the all-in-one deployment. This enables Jaeger's agent, collector, and query components to run together in a single pod, which simplifies the setup and eliminates the need for complex network configurations between components. This change is particularly useful when deploying Jaeger for OpenTelemetry in a Kubernetes environment where managing multiple components as separate pods can introduce unnecessary complexity.</p> <p>3. Use In-Memory Storage (instead of Cassandra)</p> <pre><code># before \nstorage:  \n  type: memory \n# after \nstorage:  \n  type: cassandra\n</code></pre> <p>Explanation:   OpenTelemetry usually works with simpler backends, particularly during testing or smaller environments. In-memory storage is a lightweight and easy-to-use storage option, where traces are stored temporarily in memory. This is ideal when working with OpenTelemetry for short-term or small-scale trace collection without the overhead of persistent storage like Cassandra. By using memory storage, traces will not be saved long-term, but it helps avoid setting up complex storage backends and simplifies the deployment, especially in development or evaluation environments.</p> <p>4. Disable Jaeger Agent</p> <pre><code># before \nagent:\n  enabled: true\n\n# after\nagent:\n  enabled: false\n</code></pre> <p>Explanation: The Jaeger agent is typically used to collect traces from client applications and forward them to the Jaeger collector. In an OpenTelemetry setup, OpenTelemetry Collector usually takes over the role of the agent. Therefore, disabling Jaeger's native agent is essential because OpenTelemetry already provides an agent (the OpenTelemetry Collector) that performs this functionality. This change ensures that there is no conflict or duplication of trace collection tasks, and that the OpenTelemetry Collector properly handles trace data.</p> <p>5. Enable Jaeger Collector</p> <p>Now that you\u2019re disabling the Jaeger agent, you need to ensure that the Jaeger collector is enabled, as the OpenTelemetry Collector will forward the trace data to it.</p> <pre><code># before \ncollector:\n  enabled: true\n\n# after\ncollector:\n  enabled: false\n</code></pre> <p>Explanation: allInOne covers this</p> <p>6. Enable Jaeger Query UI</p> <p>The Jaeger query UI is essential for visualizing trace data, and it should be enabled so that your team can access the data.</p> <pre><code># before \nquery:\n  enabled: true\n\n# after\nquery:\n  enabled: false\n</code></pre> <p>Explanation: allInOne covers this</p>"},{"location":"helmchart/11-install-jaeger/#step-10-uninstalling-the-chart","title":"Step 10: Uninstalling the Chart","text":"<p>Once you're done experimenting, you can delete the Jaeger deployment and associated resources from AMK:</p> <p>To uninstall/delete the jaeger helm deployment run:</p> <p><pre><code>helm list --namespace jaeger\nhelm delete jaeger -n jaeger\nkubectl delete namespace jaeger\n</code></pre> The command removes all the Kubernetes components associated with the chart and deletes the release.</p> <p>Uninstall helm chart</p> <pre><code>helm uninstall jaeger -n jaeger\n</code></pre>"},{"location":"helmchart/11-install-jaeger/#conclusion","title":"Conclusion","text":"<p>In this guide, we have walked through the process of installing Jaeger in an Azure Kubernetes Service (AKS) cluster using Helm. We covered verifying the installation, accessing the Jaeger UI locally and through Ingress, and using Jaeger for distributed tracing in your applications. </p> <p>Jaeger is a powerful tool for gaining observability into your microservices architecture, and deploying it in AKS can help you monitor and troubleshoot your applications effectively.</p>"},{"location":"helmchart/11-install-jaeger/#reference","title":"Reference","text":"<ul> <li>Jaeger Helm Charts</li> <li>Azure Kubernetes Service Documentation</li> <li>Jaeger Official Documentation</li> <li>Helm Documentation</li> <li>Terraform Helm Provider</li> <li>troubleshooting</li> </ul>"},{"location":"helmchart/12-install-otel-operator/","title":"Install OpenTelemetry Operator Helmchart in AKS","text":""},{"location":"helmchart/12-install-otel-operator/#introduction","title":"Introduction","text":"<p>The OpenTelemetry Operator is a Kubernetes Operator that simplifies the deployment and management of OpenTelemetry components in a Kubernetes cluster. OpenTelemetry is a set of APIs, libraries, agents, and instrumentation to enable observability in cloud-native applications. It is designed to collect metrics, traces, and logs from applications, microservices, and infrastructure. </p> <p>In this article, we will guide you through the process of installing the OpenTelemetry Operator Helm Cha</p>"},{"location":"helmchart/12-install-otel-operator/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Ensure that you are logged into the correct Azure subscription before proceeding.</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/12-install-otel-operator/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/12-install-otel-operator/#step-3-add-helm-chart-repository","title":"Step 3: Add Helm Chart Repository","text":"<p>Run the following commands to add the OpenTelemetry operator's Helm chart repository:</p> <pre><code># add helm repo\nhelm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts\nhelm repo update\nhelm repo update open-telemetry\n\n# check all versions \nhelm search repo opentelemetry-operator\nhelm search repo open-telemetry/opentelemetry-operator  --versions\n\n# download values file for some version\nhelm show values open-telemetry/opentelemetry-operator --version 0.83.1 &gt; otel-operator-values.yaml\n</code></pre> <p>References:</p> <ul> <li>GitHub reference</li> <li>Artifact Hub reference</li> </ul>"},{"location":"helmchart/12-install-otel-operator/#step-4-install-opentelemetry-operator","title":"Step 4: Install OpenTelemetry Operator","text":"<p>To install the operator with default settings, use the following command: <pre><code>helm install opentelemetry-operator open-telemetry/opentelemetry-operator `\n--namespace opentelemetry-operator --create-namespace `\n--set \"manager.collectorImage.repository=otel/opentelemetry-collector-contrib\"\n# --version 3.4.1 otel\\otel-operator-values.yaml\n</code></pre></p>"},{"location":"helmchart/12-install-otel-operator/#step-5-verify-opentelemetry-operator-resources","title":"Step 5: Verify OpenTelemetry Operator Resources","text":"<pre><code>kubectl get pods -n opentelemetry-operator\nkubectl get svc -n opentelemetry-operator\nkubectl get all -n opentelemetry-operator\nkubectl get all,secrets,configmap,ingress -n opentelemetry-operator\n</code></pre> <p>output <pre><code>NAME                                          READY   STATUS    RESTARTS   AGE\npod/opentelemetry-operator-86d4f76d84-hbjsn   2/2     Running   0          19h\n\nNAME                                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nservice/opentelemetry-operator           ClusterIP   10.25.240.34   &lt;none&gt;        8443/TCP,8080/TCP   19h\nservice/opentelemetry-operator-webhook   ClusterIP   10.25.44.68    &lt;none&gt;        443/TCP             19h\n\nNAME                                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/opentelemetry-operator   1/1     1            1           19h\n\nNAME                                                DESIRED   CURRENT   READY   AGE\nreplicaset.apps/opentelemetry-operator-86d4f76d84   1         1         1       19h\n\nNAME                                                            TYPE                 DATA   AGE\nsecret/opentelemetry-operator-controller-manager-service-cert   kubernetes.io/tls    3      19h\nsecret/sh.helm.release.v1.opentelemetry-operator.v1             helm.sh/release.v1   1      19h\n\nNAME                         DATA   AGE\nconfigmap/kube-root-ca.crt   1      19h\n</code></pre></p>"},{"location":"helmchart/12-install-otel-operator/#step-6-port-forwarding-testing","title":"Step 6: Port Forwarding Testing","text":"<p>To interact with the OpenTelemetry operator locally, you can port-forward the operator's service to your local machine.</p> <p>Find the operator\u2019s service:</p> <pre><code>kubectl get svc -n opentelemetry-operator\n</code></pre> <pre><code>kubectl port-forward service/opentelemetry-operator -n opentelemetry-operator 8080:8080\n</code></pre> <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\n</code></pre> <p>http://localhost:8080</p> <pre><code>404 page not found\n</code></pre> <pre><code>kubectl port-forward service/opentelemetry-operator -n opentelemetry-operator 8443:8443\n</code></pre> <pre><code>Forwarding from 127.0.0.1:8443 -&gt; 8443\nForwarding from [::1]:8443 -&gt; 8443\nHandling connection for 8443\n</code></pre> <p>https://localhost:8443</p> <pre><code>Unauthorized\n</code></pre>"},{"location":"helmchart/12-install-otel-operator/#reference","title":"Reference","text":"<ul> <li>https://github.com/open-telemetry/opentelemetry-operator/blob/main/README.md</li> <li>https://github.com/open-telemetry/opentelemetry-operator</li> <li>https://artifacthub.io/packages/helm/opentelemetry-helm/opentelemetry-operator</li> <li>https://github.com/open-telemetry/opentelemetry-helm-charts</li> </ul>"},{"location":"helmchart/2-create-helm/","title":"Create Your First Helmchart","text":""},{"location":"helmchart/2-create-helm/#introduction","title":"Introduction","text":"<p>In our previous labs we've already created deployment, service and ingress YAML manifest files for all of our Microservice and applied some of those manifests using <code>kubectl</code> commands, this is not ideal way to deploy applications to kubernetes cluster especially for microservice architecture, As part of this lab we are going to use those YAML manifest files and create Helm chart for  AKS deployment so that we are deploying as package instead of individual manifest files. Helm charts are a package manager for Kubernetes that make it easier to deploy, manage, and upgrade applications on a Kubernetes cluster.</p>"},{"location":"helmchart/2-create-helm/#technical-scenario","title":"Technical Scenario","text":"<p>As a DevOps Engineer, you've been asked to create a basic Helm chart so that you can deploy your applications to Azure Kubernetes Service (AKS) cluster in a consistent and repeatable manner.</p>"},{"location":"helmchart/2-create-helm/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster: You will need a Kubernetes cluster to deploy your Helm chart to. </li> <li>Helm installed: You will need to have Helm installed on your local machine. </li> <li>Azure CLI installed: You will need to have the Azure CLI installed on your local machine. </li> <li>A basic understanding of Kubernetes: You should have a basic understanding of Kubernetes concepts such as Pods, Deployments, Services, ConfigMaps, and Secrets.</li> <li>An application to deploy: You will need an application that you want to deploy to your AKS cluster. This could be a containerized application, a set of microservices, or any other application that can run on Kubernetes.</li> <li>Azure Container Registry (ACR) for storing the application image: You will need to have an Azure Container Registry (ACR) where you can store the container image for your application.</li> </ul>"},{"location":"helmchart/2-create-helm/#implementation-details","title":"Implementation details","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step-1: Install Helm locally</li> <li>Step-2: Create a new Helm chart</li> <li>Step-3: Validate Helm chart</li> <li>Step-4: Install Helm chart in AKS</li> <li>Step-5: UnInstall Helm chart</li> </ul>"},{"location":"helmchart/2-create-helm/#step-1-install-helm-locally","title":"Step-1: Install Helm locally","text":"<p>You need to install Helm on your local machine. You can find the installation instructions in the Helm documentation: https://helm.sh/docs/intro/install/</p> <p>Windows OS users</p> <p>To install Helm on Windows using Chocolatey package manager, follow these steps:</p> <ul> <li>Install Chocolatey by following the instructions on the Chocolatey website: https://chocolatey.org/install</li> <li>Once Chocolatey is installed, open a Command Prompt or PowerShell window with administrator privileges.</li> <li>Run the following command to install Helm using Chocolatey: <pre><code>choco install kubernetes-helm\n</code></pre></li> </ul> <p>Mac OS users</p> <p>Install Helm using Homebrew package manager by running the following command in a terminal window:</p> <pre><code>brew install helm\n</code></pre> <p>Verify the Helm installation</p> <p>After the installation is complete, run the <code>helm version</code> command to verify that Helm is installed and working properly.</p> <pre><code>helm version\n</code></pre> <p>your output will look like below: <pre><code>version.BuildInfo{Version:\"v3.8.2\", GitCommit:\"6e3701edea09e5d55a8ca2aae03a68917630e91b\", GitTreeState:\"clean\", GoVersion:\"go1.17.5\"}\n</code></pre></p>"},{"location":"helmchart/2-create-helm/#step-2-create-a-new-helm-chart","title":"Step-2: Create a new Helm chart","text":"<p>To create a new chart, you can use the <code>helm create</code> command. to create a chart named <code>microservices-chart</code>, you can run the following command:</p> <pre><code>helm create microservices-chart\n</code></pre> <p>Let's quickly view the list of files and folders created here by running following command in command prompt.</p> <p><pre><code>tree /F\n</code></pre> output</p> <pre><code>C:.\n\u2502   .helmignore\n\u2502   Chart.yaml\n\u2502   values.yaml\n\u251c\u2500\u2500\u2500charts\n\u2514\u2500\u2500\u2500templates\n    \u2502   deployment.yaml\n    \u2502   hpa.yaml\n    \u2502   ingress.yaml\n    \u2502   NOTES.txt\n    \u2502   service.yaml\n    \u2502   serviceaccount.yaml\n    \u2502   _helpers.tpl\n    \u2514\u2500\u2500\u2500tests\n            test-connection.yaml\n</code></pre> <p>We don\u2019t need all the files here, let me cleanup some files which we are not going use it immediately and keep only files we need it for now to continue our lab work.</p> <p>Here is the new tree structure of our files &amp; folders after the cleanup</p> <pre><code>C:.\n\u2502   .helmignore\n\u2502   Chart.yaml\n\u2502   values.yaml\n\u251c\u2500\u2500\u2500charts\n\u2514\u2500\u2500\u2500templates\n        deployment.yaml\n        service.yaml\n</code></pre> <p>Now let's discuss the purpose of each file and folders used here and update with new contents.</p> <p>.helmignore: </p> <p>The <code>.helmignore</code> file is used by the Helm package manager to specify files and directories that should be excluded from a chart when it is packaged. The syntax of the .helmignore file is similar to that of a .gitignore file. </p> <p>We will continue using this file without any changes.</p> .helmignore<pre><code># Patterns to ignore when building packages.\n# This supports shell glob matching, relative path matching, and\n# negation (prefixed with !). Only one pattern per line.\n.DS_Store\n# Common VCS dirs\n.git/\n.gitignore\n.bzr/\n.bzrignore\n.hg/\n.hgignore\n.svn/\n# Common backup files\n*.swp\n*.bak\n*.tmp\n*.orig\n*~\n# Various IDEs\n.project\n.idea/\n*.tmproj\n.vscode/\n</code></pre> <p>Chart.yaml: </p> <p>This file contains the chart's metadata, including its name, version, and description. Again we don't need to make any changes here for now.</p> Chart.yaml<pre><code>apiVersion: v2\nname: microservices-chart\ndescription: A Helm chart for Kubernetes\n.\n.\naccess the the git repo for entire source code ...\n</code></pre> <p>values.yaml: </p> <p>This file contains default values for the chart's templates. These values can be overridden when the chart is installed using the --set or --values flags.</p> <p>Let's replace the contents of values.yaml file with following:</p> values.yaml<pre><code>global:\n  environment: dev\n  namespace: sample\n  registryPrefix: acr1dev.azurecr.io\n\nmicroservices:\n  aspnetapi:\n    replicaCount: 1    \n  aspnetapp:\n    replicaCount: 1\n</code></pre> <p>templates/: </p> <p>This directory contains the Kubernetes manifests that make up the chart. Each file defines a different Kubernetes resource, such as a deployment, service, or ConfigMap.</p> <p>deployment.yaml: </p> <p>This file defines a Kubernetes deployment that runs the application. It includes information about the container image to use, the number of replicas to run, and other relevant details.</p> <p>Let's replace the <code>deployment.yaml</code> file contents with one of our Microservice deployment.yaml file and make few changes here to read values from values.yaml file. </p> <p>If you notice we've reading the namespace, replicas &amp; image values from values.yaml files; that's how helm syntax works.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: {{ .Values.global.namespace }}\nspec:\n  replicas: {{ .Values.microservices.aspnetapi.replicaCount }}\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api-image\n          image: {{ .Values.global.registryPrefix }}/sample/aspnet-api:20230302.5\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          env:\n            - name: KEY\n              value: value\n</code></pre> <p>service.yaml: </p> <p>This file defines a Kubernetes service that exposes the application to the network. It includes information about the type of service (ClusterIP, NodePort, etc.), the port to use, and other relevant details.</p> <p>Let's replace the <code>service.yaml</code> file contents with one of our microservices service.yaml file and make few changes here to read values from values.yaml file. </p> <p>service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: {{ .Values.global.namespace }}\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n</code></pre> That's it! Together, these components make up a Helm chart that can be used to deploy an application to a Kubernetes cluster. When the chart is installed, Helm will use the templates in the templates/ directory to generate Kubernetes manifests that define the resources needed to run the application.</p> <p>Before we install this Helm chart, we are going to validate helm chart to make sure that no errors in the chart. the first command we are going to use is <code>helm lint</code></p> <p>Helm Lint</p> <p>It is always recommended to run <code>helm lint</code> before installing helm chart, <code>Helm Lint</code> is a tool that checks the syntax and structure of a Helm chart and ensures that it follows best practices and conventions</p> <p><pre><code>helm lint microservices-chart\n</code></pre> example of the output without any errors <pre><code>==&gt; Linting microservices-chart\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre></p> <p>Helm template</p> <p>After the <code>helm lint</code>, it is also recommended to generate the yaml manifest from helm chart and review the yaml manifest to make sure all the values are replaced correctly or not, for that we are going to use <code>helm template</code> command.</p> <p><code>helm template</code> command does not install the chart or make any changes to the Kubernetes cluster. It simply generates YAML manifests based on the chart and the values provided. This can be useful for reviewing the manifests before applying them to a Kubernetes cluster.</p> <pre><code>helm template 'microservices-chart' './microservices-chart' --namespace='sample' &gt; microservices-manifests.yaml\n</code></pre> <p>Note</p> <p>you will notice a new file <code>microservices-manifests.yaml</code> created in your folder for your review, after the reveiw you can delete this file so that you don't commit this file in your git repo.</p>"},{"location":"helmchart/2-create-helm/#step-3-installing-helm-chart","title":"Step-3: Installing Helm Chart","text":"<p>To install a local Helm chart, you can use the <code>helm install</code> command.</p> <pre><code>helm install 'microservices-release' './microservices-chart' --namespace 'sample'\n</code></pre> <ul> <li><code>microservices-release</code>  - release name for the chart.   </li> <li><code>./microservices-chart</code>  - directory where chart located.</li> <li><code>sample</code> - namespace of AKS</li> </ul> <p>output</p> <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 17:10:36 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>This command installs the chart from the current directory (./microservices-chart) and gives it a release name of <code>microservices-release</code></p> <p>When you run the <code>helm install</code> command to install a Helm chart, the new release is placed in your Kubernetes cluster. The release is installed as a set of Kubernetes resources, which can include Deployments, Services, ConfigMaps, and more, depending on the contents of your Helm chart.</p> <p>verify the installation</p> <p>You can verify that the installation was successful by running helm ls, which will show a list of installed releases.</p> <p><pre><code>helm ls -n sample\n</code></pre> output</p> <pre><code>NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\nmicroservices-release   sample          1               2023-03-14 17:10:36.1926652 -0700 PDT   deployed        microservices-chart-0.1.0       1.16.0\n</code></pre> <p>helm status</p> <p>Check the status of the release using the <code>helm status</code> command to ensure that it has been installed successfully.</p> <p><pre><code>helm status 'microservices-release' -n 'sample'\n</code></pre> output</p> <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 17:10:36 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre>"},{"location":"helmchart/2-create-helm/#step-4-upgrade-helm-chart","title":"Step-4: Upgrade Helm chart","text":"<p>If you've any changes in the application code or configuration then you need to use the <code>helm upgrade</code> command to promote the new changes in the existing helm release.</p> <p>Run the <code>helm upgrade</code> command followed by the <code>release name</code> and <code>chart name</code>.</p> <pre><code>helm upgrade 'microservices-release' 'microservices-chart' --namespace 'sample'\n</code></pre> <p>output <pre><code>Release \"microservices-release\" has been upgraded. Happy Helming!\nNAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 21:34:46 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n</code></pre></p> <p>helm status</p> <p>After the upgrade is complete, check the status of the release using the helm status command to ensure that it has been updated successfully.</p> <p><pre><code>helm status 'microservices-release' -n 'sample'\n</code></pre> output <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 21:34:46 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n</code></pre></p> <p>release history</p> <p>Display the release history</p> <p><pre><code>helm history 'microservices-release'\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 17:10:36 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 21:34:46 2023        deployed        microservices-chart-0.1.0       1.16.0          Upgrade complete\n</code></pre></p>"},{"location":"helmchart/2-create-helm/#step-5-uninstall-helm-chart","title":"Step-5: UnInstall Helm chart","text":"<p>To uninstall a Helm chart, you can use the helm uninstall command followed by the name of the release you wish to delete. </p> <pre><code># list helm release\nhelm ls -n 'sample'\nhelm uninstall 'microservices-release' -n 'sample'\n</code></pre> <p>output <pre><code>release \"microservices-release\" uninstalled\n</code></pre></p> <p>Helm simplifies the process of installing and managing Kubernetes applications and provides an easy-to-use command-line interface for managing charts and releases.</p>"},{"location":"helmchart/3-helm-cmds/","title":"Frequently used Helm commands details","text":"<p>Here are some of the most frequently used Helm commands and their details:</p>"},{"location":"helmchart/3-helm-cmds/#helm-lint","title":"Helm lint","text":"<p><code>Helm lint</code> is like your C# source code compilation. It is always recommended to run <code>helm lint</code> before installing helm chart, <code>Helm Lint</code> is a tool that checks the syntax and structure of a Helm chart and ensures that it follows best practices and conventions</p> <p>Helm Lint helps you catch errors and issues with your Helm charts before you deploy them to a Kubernetes cluster, which can save you time and avoid potential problems down the line.</p> <p>When you run helm lint on a Helm chart, it checks for the following:</p> <ul> <li>Valid YAML syntax</li> <li>Valid Helm chart structure</li> <li>Properly formatted values files</li> <li>Best practices for Helm chart development, such as using templates, labels, and annotations correctly</li> </ul> <p>Helm Lint can be run on a Helm chart directory like this:</p> <pre><code>helm lint 'microservices-chart'\n</code></pre> <p>If Helm Lint finds any issues with the Helm chart, it will output them to the console along with suggestions for how to fix them.</p> <p>example of output with error <pre><code>==&gt; Linting microservices-chart\n[INFO] Chart.yaml: icon is recommended\n[ERROR] templates/: parse error at (microservices-chart/templates/aspnet-api/deployment.yaml:7): bad character U+002D '-'\n\nError: 1 chart(s) linted, 1 chart(s) failed\n</code></pre></p> <p>Read the error details and fix the YAML manifest accordingly before installing Helm chart, you will not be able to install Helm with errors in the Chart.</p> <p>example of output without any error <pre><code>==&gt; Linting microservices-chart\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre></p>"},{"location":"helmchart/3-helm-cmds/#helm-template","title":"Helm template","text":"<p>After the <code>helm lint</code>, it is also recommended to generate the yaml manifest from helm chart and review the yaml manifest to make sure all the values are replaced correctly or not, for that we are going to use <code>helm template</code> command.</p> <p><code>helm template</code> command does not install the chart or make any changes to the Kubernetes cluster. It simply generates YAML manifests based on the chart and the values provided. This can be useful for reviewing the manifests before applying them to a Kubernetes cluster.</p> <pre><code>helm template 'microservices-chart' './microservices-chart' --namespace='sample' &gt; microservices-manifests.yaml\n</code></pre> <p>Note</p> <p>you will notice new file <code>microservices-manifests.yaml</code> created in your folder for your review, after the reveiw you can delete this file so that you don't commit this file in your git repo.</p> <p>generate with environment values.yaml file</p> <pre><code>helm template 'microservices-chart' './microservices-chart' --namespace='sample' &gt; microservices-manifests.yaml\nhelm template 'microservices-chart' './microservices-chart' --namespace='sample' --values 'microservices-chart/values-dev.yaml' &gt; microservices-manifests.yaml\n</code></pre>"},{"location":"helmchart/3-helm-cmds/#-dry-run","title":"--dry-run","text":"<p>As part of the <code>helm install</code> command you can use the --dry-run flag.</p> <p>The <code>--dry-run</code> flag is used to perform a simulated installation of a chart using the helm install command without actually installing the chart or creating a new release. When this flag is used, Helm will generate the Kubernetes manifest files that would be applied to the cluster during the installation process, but it will not actually make any changes to the cluster.</p> <p>The purpose of performing a dry run is to preview the changes that would be made to the cluster by the installation process before actually applying those changes. This can be useful for testing and debugging purposes, as well as for verifying that the desired configuration settings are being applied correctly.</p> <p>By using the --dry-run flag, you can identify any potential issues or conflicts that may arise during the installation process before making any changes to the cluster. This can help you to avoid making unintended changes or causing downtime for your applications.</p> <pre><code>helm install 'microservices-release' './microservices-chart' --namespace 'sample' --dry-run\nhelm install 'microservices-release' './microservices-chart' --namespace 'sample' --values 'microservices-chart/values-dev.yaml' --dry-run\n</code></pre> <p>output</p> <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 17:16:05 2023\nNAMESPACE: sample\nSTATUS: pending-install\nREVISION: 1\nTEST SUITE: None\nHOOKS:\nMANIFEST:\n---\n# Source: microservices-chart/templates/aspnet-api/service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: aspnet-api\n---\n# Source: microservices-chart/templates/aspnet-api/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5\n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api-image\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230302.5\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          env:\n            - name: KEY\n              value: value\n</code></pre>"},{"location":"helmchart/3-helm-cmds/#packaging-the-helm-chart","title":"Packaging the Helm chart","text":"<p>Use the helm package command to create the chart package. This command takes the name of the chart directory as its argument and creates a compressed archive file with the extension .tgz.</p> <p><pre><code>helm package './microservices-chart'\n</code></pre> This will create a file named microservices-chart-.tgz in the current directory. <p>output <pre><code>Successfully packaged chart and saved it to: C:\\Source\\Repos\\microservices\\helmcharts\\microservices-chart-0.1.0.tgz\n</code></pre></p> <p>You can use the --version flag to specify the version number for the chart package.  <pre><code>helm package --version 1.0.0 './microservices-chart'\n</code></pre></p> <p>Once you have created a Helm chart package, you can share it with others by hosting it in a public or private chart repository. Other users can then install and manage your application or service in their Kubernetes clusters using the helm install command and the name of your chart package.</p>"},{"location":"helmchart/3-helm-cmds/#rollback-helm-chart","title":"Rollback helm chart","text":"<p>If you encounter any issues during the upgrade process, you can roll back the release to the previous version using the helm rollback command. For example, to roll back the my-release release to version 1, run the following command:</p> <p>first list helm releases</p> <p><pre><code>helm ls -n sample\nhelm history 'microservices-release' -n 'sample'\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 22:31:08 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 22:31:22 2023        deployed        microservices-chart-0.1.0       1.16.0          Upgrade complete\n</code></pre></p> <p>helm rollback</p> <p><pre><code>helm rollback 'microservices-release' 1 -n 'sample'\n</code></pre> output <pre><code>Rollback was a success! Happy Helming!\n</code></pre> check the history again <pre><code>helm history 'microservices-release' -n 'sample'\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 22:31:08 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 22:31:22 2023        superseded      microservices-chart-0.1.0       1.16.0          Upgrade complete\n3               Tue Mar 14 22:32:10 2023        deployed        microservices-chart-0.1.0       1.16.0          Rollback to 1\n</code></pre></p> <p>This will revert the release to the previous version and restore the previous configuration.</p>"},{"location":"helmchart/3-helm-cmds/#helm-chart-history","title":"helm chart history","text":"<p>The <code>helm history</code> command is used to view the revision history of a Helm release, which includes information about the dates, times, and details of previous upgrades or rollbacks of the release. This command is useful for tracking the lifecycle of a release, understanding the changes made to a release over time, and diagnosing issues that may have occurred during previous upgrades or rollbacks.</p> <p><pre><code>helm history 'microservices-release' -n 'sample'\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 22:31:08 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 22:31:22 2023        superseded      microservices-chart-0.1.0       1.16.0          Upgrade complete\n3               Tue Mar 14 22:32:10 2023        deployed        microservices-chart-0.1.0       1.16.0          Rollback to 1\n</code></pre></p> <p>By default, the helm history command displays the last five revisions of the release. However, you can specify a different number of revisions to display using the --max flag. For example, to display the last ten revisions of a release, you can run the following command:</p> <p><pre><code>helm history 'microservices-release' -n 'sample' --max=10\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 22:31:08 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 22:31:22 2023        superseded      microservices-chart-0.1.0       1.16.0          Upgrade complete\n3               Tue Mar 14 22:32:10 2023        superseded      microservices-chart-0.1.0       1.16.0          Rollback to 1\n4               Tue Mar 14 22:37:02 2023        superseded      microservices-chart-0.1.0       1.16.0          Upgrade complete\n5               Tue Mar 14 22:37:09 2023        superseded      microservices-chart-0.1.0       1.16.0          Upgrade complete\n6               Tue Mar 14 22:37:14 2023        deployed        microservices-chart-0.1.0       1.16.0          Upgrade complete\n</code></pre></p>"},{"location":"helmchart/3-helm-cmds/#helm-cheat-sheet","title":"Helm Cheat Sheet","text":"<p>For more information about Helm commands look into the Helm Cheat Sheet.</p>"},{"location":"helmchart/4-helm-microservices/","title":"Create Helmchart for Microservices","text":""},{"location":"helmchart/4-helm-microservices/#introduction","title":"Introduction","text":"<p>As part of Create your first Helm chart Lab we've already created a basic helm chart, in this lab we are going to extend the same helm chart and make it for Microservices architecture, that means we are going to create helm chart which runs for multiple applications as a single package manager.  </p>"},{"location":"helmchart/4-helm-microservices/#technical-scenario","title":"Technical Scenario","text":"<p>As a DevOps Engineer, you've been asked to create a Helm chart for Microservices Architecture so that you can deploy Microservices applications to Azure Kubernetes Service (AKS) cluster in a consistent and repeatable manner.</p>"},{"location":"helmchart/4-helm-microservices/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Kubernetes cluster: You will need a Kubernetes cluster to deploy your Helm chart to. </li> <li>Basic helm chart:  We've already created this in our previous lab</li> <li>Microservice applications yaml manifests: You will need Microservice applications YAML manifests that you want to deploy to your AKS cluster. These Microservices are containerized and ready AKS deployment.</li> <li>Azure Container Registry (ACR) for storing the application images: You will need to have an Azure Container Registry (ACR) where you can store the container images for your Microservice applications.</li> </ul>"},{"location":"helmchart/4-helm-microservices/#implementation-details","title":"Implementation details","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Task-1: Update basic Helm chart for Microservices</li> <li>Task-2: Validate Helm chart</li> <li>Task-3: Install Helm chart in AKS</li> <li>Task-4: UnInstall Helm chart</li> </ul>"},{"location":"helmchart/4-helm-microservices/#task-1-update-basic-helm-chart-for-microservices","title":"Task-1: Update basic helm chart for Microservices","text":"<p>Before updating the existing Helm chart, let's review the tree structure of our files &amp; folders from previous lab.</p> <pre><code>C:\n\u2502   .helmignore\n\u2502   Chart.yaml\n\u2502   values.yaml\n\u251c\u2500\u2500\u2500charts\n\u2514\u2500\u2500\u2500templates\n        deployment.yaml\n        service.yaml\n</code></pre> <p>Let's discuss about the new changes in each file in the existing Helm chart to support Microservice applications deployment. </p> <p>.helmignore - no changes in this file</p> <p>Chart.yaml - no changes in this file</p> <p>values.yaml: </p> <p>One of the key features of Helm Chart is the ability to use <code>values.yaml</code> files to parameterize your deployment configuration. as we know that <code>values.yaml</code> contains the default values for the chart's templates, These values can be overridden when the chart is installed.</p> <p>Now let's consider the real scenario and use the <code>values.yaml</code> for multiple environments. Here we want to install same helm chart for multiple environments; in this case we need to maintain separate <code>values.yaml</code> file for each environment; Here is how you can create separate values.yaml file for each environment.</p> <pre><code># file naming convention \nvalues-{environment}.yaml\n</code></pre> <p>For example </p> <pre><code>- values-dev.yaml  - use this for dev environment\n- values-test.yaml  - use this for qa/test environment\n- values-uat.yaml  - use this for uat/staging environment\n- values-prod.yaml  - use this for production environment\n</code></pre> <p>By using a separate <code>values.yaml</code> file for each environment, you can easily manage configuration for multiple environment and use the same manifest yaml or helm chart for each environment.</p> <p>Let's take a look inside the each values.yaml files </p> <p>dev environment values-dev.yaml<pre><code>global:\n  environment: dev\n  namespace: sample\n  registryPrefix: acr1dev.azurecr.io\n\nmicroservices:\n  aspnetapi:\n    replicaCount: 1    \n  aspnetapp:\n    replicaCount: 1\n</code></pre> test environment</p> <p>values-test.yaml<pre><code>global:\n  environment: test\n  namespace: sample\n  registryPrefix: acr1test.azurecr.io\n\nmicroservices:\n  aspnetapi:\n    replicaCount: 2  \n  aspnetapp:\n    replicaCount: 2\n</code></pre> prod environment</p> values-prod.yaml<pre><code>global:\n  environment: prod\n  namespace: sample\n  registryPrefix: acr1prod.azurecr.io\n\nmicroservices:\n  aspnetapi:\n    replicaCount: 3\n  aspnetapp:\n    replicaCount: 3\n</code></pre> <p>if you notice each files has different configuration as per the specific environment need.</p> <p>templates/: </p> <p>As we know this directory contains the Kubernetes manifests, we are going to create separate folder for each application inside this template folder and create the deployment, service, or ConfigMap manifest files for specific to single application. </p> <p>Here is how the folder structure for single application inside the template folder</p> <pre><code>template:\n\u2514\u2500\u2500\u2500aspnet-app\n    |__ deployment.yaml\n    |__ service.yaml\n    |__ ConfigMap.yaml - # create only if it is required\n   aspnet-api\n    |__ deployment.yaml\n    |__ service.yaml\n    aspnet-db\n    |__ deployment.yaml\n    |__ service.yaml\n</code></pre> <p>Now let's look at some of our Microservices manifest yaml files created in our previous labs and package them in the template folder.</p> <p>aspnet-api - create this new folder inside template folder</p> <p><code>deployment.yaml</code> - create this new file inside the <code>aspnet-api</code> folder and copy this yaml manifest.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: {{ .Values.global.namespace }}\nspec:\n  replicas: {{ .Values.microservices.aspnetapi.replicaCount }}\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api-image\n          image: {{ .Values.global.registryPrefix }}/sample/aspnet-api:20230302.5\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          env:\n            - name: KEY\n              value: value\n</code></pre> <p><code>service.yaml</code> - create this new file inside the <code>aspnet-api</code> folder and copy this yaml manifest.</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: {{ .Values.global.namespace }}\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n</code></pre> <p>repeat above step for each service, Here is the example for aspnet-app.</p> <p>aspnet-app - create this new folder inside template folder</p> <p><code>deployment.yaml</code> - create this new file inside the <code>aspnet-app</code> folder and copy this yaml manifest.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-app\n  namespace: {{ .Values.global.namespace }}\nspec:\n  replicas: {{ .Values.microservices.aspnetapp.replicaCount }}\n  selector: \n    matchLabels:\n      app: aspnet-app\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-app\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-app-image\n          image: {{ .Values.global.registryPrefix }}/sample/aspnet-app:20230302.5\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          env:\n            - name: KEY\n              value: value\n</code></pre> <p><code>service.yaml</code> - create this new file inside the <code>aspnet-app</code> folder and copy this yaml manifest.</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-app\n  namespace: {{ .Values.global.namespace }}\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-app\n</code></pre> <p>We'll stop here with 2 applications but same goes for the rest of the containers. Now let's talk about the ingress files.</p> <p>Ingress - Create a new folder called <code>ingress</code>, we are going to create separate folder for managing all ingress files used in our microservices applications.</p> <p>We are going to create separate ingress file for each scenario, for example </p> <p><code>ingress-no-pathprefix.yaml</code> - This file contains the ingress routing rules to support the API swagger URL rules</p> ingress-no-pathprefix.yaml<pre><code>replace yaml file here\n</code></pre> <p><code>ingress-pathprefix.yaml</code> - This file contains the ingress routing rules to support the Web Application URL rules</p> ingress-pathprefix.yaml<pre><code>replace yaml file here\n</code></pre> <p>Here is how the new tree structure looks like after adding few microservices YAML manifest files in template folder.</p> <pre><code>C:.\n\u2502   .helmignore\n\u2502   Chart.yaml\n\u2502   values.yaml\n\u251c\u2500\u2500\u2500charts\n\u2514\u2500\u2500\u2500templates\n    \u251c\u2500\u2500\u2500aspnet-api\n    \u2502       deployment.yaml\n    \u2502       service.yaml\n    \u251c\u2500\u2500\u2500aspnet-app\n    \u2502       deployment.yaml\n    \u2502       service.yaml\n    \u2514\u2500\u2500\u2500ingress\n            ingress-no-pathprefix.yaml\n            ingress-pathprefix.yaml\n</code></pre> <p>Together, these components make up a Helm chart that can be used to deploy an application to a Kubernetes cluster. When the chart is installed, Helm will use the templates in the templates/ directory to generate Kubernetes manifests that define the resources needed to run the application.</p>"},{"location":"helmchart/4-helm-microservices/#task-2-validate-helm-chart","title":"Task-2: Validate Helm chart","text":"<p>Before we install this Helm chart, we are going to validate helm chart to make sure that no errors in the chart. the first command we are going to use is <code>helm lint</code></p> <p>Helm Lint</p> <p>It is always recommended to run <code>helm lint</code> before installing helm chart, <code>Helm Lint</code> is a tool that checks the syntax and structure of a Helm chart and ensures that it follows best practices and conventions</p> <p><pre><code>helm lint microservices-chart\n</code></pre> example of the output without any errors</p> <pre><code>==&gt; Linting microservices-chart\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n</code></pre> <p>Helm template</p> <p>After the <code>helm lint</code>, it is also recommended to generate the yaml manifest from helm chart and review the yaml manifest to make sure all the values are replaced correctly or not, for that we are going to use <code>helm template</code> command.</p> <p><code>helm template</code> command does not install the chart or make any changes to the Kubernetes cluster. It simply generates YAML manifests based on the chart and the values provided. This can be useful for reviewing the manifests before applying them to a Kubernetes cluster.</p> <pre><code># generate with environment values.yaml file\nhelm template 'microservices-chart' './microservices-chart' --namespace='sample' --values 'microservices-chart/values-dev.yaml' &gt; microservices-manifests.yaml\n</code></pre> <p>You will notice new file <code>microservices-manifests.yaml</code></p>"},{"location":"helmchart/4-helm-microservices/#task-3-install-helm-chart-in-aks","title":"Task-3: Install Helm chart in AKS","text":"<p>To install a local Helm chart, you can use the <code>helm install</code> command.</p> <pre><code>helm install 'microservices-release' './microservices-chart' --namespace 'sample'\n</code></pre> <ul> <li><code>microservices-release</code>  - release name for the chart.   </li> <li><code>./microservices-chart</code>  - directory where chart located.</li> <li><code>sample</code> - namespace of AKS</li> </ul> <p>output</p> <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 17:10:36 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>This command installs the chart from the current directory (./microservices-chart) and gives it a release name of <code>microservices-release</code></p> <p>verify the installation</p> <p>You can verify that the installation was successful by running helm ls, which will show a list of installed releases.</p> <p><pre><code>helm ls -n sample\n</code></pre> output</p> <pre><code>NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\nmicroservices-release   sample          1               2023-03-14 17:10:36.1926652 -0700 PDT   deployed        microservices-chart-0.1.0       1.16.0\n</code></pre> <p>helm status</p> <p>Check the status of the release using the <code>helm status</code> command to ensure that it has been installed successfully.</p> <p><pre><code>helm status 'microservices-release' -n 'sample'\n</code></pre> output</p> <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 17:10:36 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre>"},{"location":"helmchart/4-helm-microservices/#step-4-upgrade-helm-chart","title":"Step-4: Upgrade Helm chart","text":"<p>If you've any changes in the application code or configuration then you need to use the <code>helm upgrade</code> command to promote the new changes in the existing helm release.</p> <p>Run the <code>helm upgrade</code> command followed by the <code>release name</code> and <code>chart name</code>.</p> <pre><code>helm upgrade 'microservices-release' 'microservices-chart' --namespace 'sample'\n</code></pre> <p>output <pre><code>Release \"microservices-release\" has been upgraded. Happy Helming!\nNAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 21:34:46 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n</code></pre></p> <p>helm status</p> <p>After the upgrade is complete, check the status of the release using the helm status command to ensure that it has been updated successfully.</p> <p><pre><code>helm status 'microservices-release' -n 'sample'\n</code></pre> output <pre><code>NAME: microservices-release\nLAST DEPLOYED: Tue Mar 14 21:34:46 2023\nNAMESPACE: sample\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None\n</code></pre></p> <p>release history</p> <p>Display the release history</p> <p><pre><code>helm history 'microservices-release'\n</code></pre> output <pre><code>REVISION        UPDATED                         STATUS          CHART                           APP VERSION     DESCRIPTION     \n1               Tue Mar 14 17:10:36 2023        superseded      microservices-chart-0.1.0       1.16.0          Install complete\n2               Tue Mar 14 21:34:46 2023        deployed        microservices-chart-0.1.0       1.16.0          Upgrade complete\n</code></pre></p>"},{"location":"helmchart/4-helm-microservices/#step-5-uninstall-helm-chart","title":"Step-5: UnInstall Helm chart","text":"<p>To uninstall a Helm chart, you can use the helm uninstall command followed by the name of the release you wish to delete. </p> <pre><code># list helm release\nhelm ls -n 'sample'\nhelm uninstall 'microservices-release' -n 'sample'\n</code></pre> <p>output <pre><code>release \"microservices-release\" uninstalled\n</code></pre></p> <p>Overall, Helm simplifies the process of installing and managing Kubernetes microservices applications and provides an easy-to-use command-line interface for managing charts and releases.</p>"},{"location":"helmchart/5-install-pgadmin4/","title":"Install pgadmin4 in Azure Kubernetes Services (AKS) with Helmchart using Terraform","text":""},{"location":"helmchart/5-install-pgadmin4/#introduction","title":"Introduction","text":"<p>PostgreSQL is a powerful, open-source object-relational database management system (ORDBMS) that we use in most of our microservices architecture. Managing databases is always crucial in any development environment. PgAdmin4 plays a very important role here in managing the PostgreSQL database.</p> <p>In this session, we'll explore how to set up pgAdmin4 on a Azure Kubernetes Service (AKS). We'll use tools like Helmcharts and Terraform to make the deployment process smooth and manageable. By end of this session, you'll have a clear understanding of how to deploy pgAdmin4 effectively using IaC tools, making database management in your Kubernetes environment. </p> <p>pgAdmin</p> <p>pgAdmin is a free and open-source administration and management tool for the PostgreSQL database. It provides a graphical user interface (GUI) for managing and interacting with PostgreSQL databases, allowing users to perform tasks such as creating and modifying tables, managing users and permissions, and running SQL queries. </p> <p>PostgreSQL</p> <p>PostgreSQL is a powerful, open-source object-relational database management system (ORDBMS) PostgreSQL is developed by a global community of contributors. It is available for multiple platforms such as Windows, Linux, and MacOS.</p> <p>pgAdmin vs. PostgreSQL</p> <p>While there are other ways to manage a PostgreSQL database, the simplest approach is to use a GUI tool to make interactions more visual and user-friendly. </p> <p>pgAdmin was created to help PostgreSQL users get the most out of their database. The purpose is to provide a graphical administration tool to make it easier to manipulate schema and data in PostgreSQL</p>"},{"location":"helmchart/5-install-pgadmin4/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Setup terraform configure for pgadmin4 Install</li> <li>Step 2. Create a new namespace for pgadmin4</li> <li>Step 3. Install pgadmin4 helmchart using terraform</li> <li>Step 4. Verify pgadmin4 resources in AKS</li> <li>Step 5. Verify pgadmin4 resources in AKS Cluster</li> <li>Step 6. Access pgadmin4 locally - port forwarding</li> <li>Step 7. Configure Ingress for pgadmin4</li> <li>Step 8. Troubleshooting</li> </ul> <p>By following these steps, you will have a securely configured pgAdmin4 instance running in AKS, accessible for database management tasks.</p>"},{"location":"helmchart/5-install-pgadmin4/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with the installation of pgadmin4 in AKS using terraform, ensure you have the following prerequisites in place:</p> <ul> <li>Azure subscription</li> <li>Download and Install Terraform</li> <li>Install azure CLI</li> <li>Install and set up kubectl</li> <li>AKS cluster - Ensure you have a running Kubernetes cluster available for pgadmin4 deployment</li> <li>An Ingress controller installed in your cluster (e.g., application-gateway, nginx-ingress)</li> </ul>"},{"location":"helmchart/5-install-pgadmin4/#implementation-details","title":"Implementation Details","text":"<p>Let's look into the step-by-step implementation details:</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>Use the <code>az aks get-credentials</code> command to configure kubectl to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-1-setup-terraform-configure-for-pgadmin4-install","title":"Step-1: Setup terraform configure for pgadmin4 Install","text":"<p>Launch Visual Studio Code and open your current terraform repository to begin working on your terraform configuration.</p> <p>In order to install any Helmcharts using terraform configuration we need to have following terraform providers.</p> <ul> <li>helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider </li> </ul> <p>Terraform Providers</p> <p>You can install the necessary providers by adding the following code in your Terraform configuration file:</p> <p>Let's update our existing <code>provider.tf</code> file with new kubernetes, helm and kubectl providers:</p> provider.tf<pre><code>terraform {\n\n  required_version = \"&gt;=0.12\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;2.0\"\n    }\n\n    azuread = {\n      version = \"&gt;= 2.26.0\" // https://github.com/terraform-providers/terraform-provider-azuread/releases\n    }\n     kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"&gt;= 2.0.3\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"&gt;= 2.1.0\"\n    }\n\n     kubectl = {\n      source  = \"gavinbunney/kubectl\"\n      version = \"&gt;= 1.7.0\"\n    }\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  #load_config_file       = false\n}\n\nprovider \"helm\" {\n  debug = true\n  kubernetes {\n    host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n\n  }\n}\nprovider \"kubectl\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  load_config_file       = false\n}\n</code></pre> <p>Setting Up pgadmin4 Locally Using Helm Charts</p> <p>pgadmin4 can be installed using various methods. Here I will walk through downloading the pgadmin4 Helmchart source code from a Git repository and running it locally from Visual studio code to install it on an AKS cluster.</p> <p>Downloading pgadmin4 Helmcharts source code</p> <ol> <li> <p>Clone the Repository: Clone the Helmchart repository from the following Git repository: rowanruseler/helm-charts. This repository contains the necessary Helm chart for deploying pgadmin4.</p> </li> <li> <p>Access pgadmin4 Charts: Navigate to the <code>pgadmin4</code> folder within the Helmcharts repository.  URL: pgadmin4 Helm Chart.  This folder will contain all pgadmin4 source code and configurations.</p> </li> <li> <p>Configuration Files: Within the <code>pgadmin4</code> folder, you will notice the <code>values.yaml</code> - this file stores configuration settings for pgadmin4.  either create a separate file named <code>pgadmin4.yaml</code> or use <code>values.yaml</code> to store configuration settings.</p> </li> <li> <p>Customizing pgadmin4 Configuration: Once the files are set up, customize pgadmin4 according to your requirements:</p> </li> </ol> <p>Change-1: Configure Extra Secret Mounts, see the difference before and after:</p> <pre><code>extraSecretMounts:\n  - name: config-local\n    secret: pgadmin4-config\n    subPath: config_local.py\n    mountPath: \"/pgadmin4/config_local.py\"\n    readOnly: true\n  # - name: pgpassfile\n  #   secret: pgpassfile\n  #   subPath: pgpassfile\n  #   mountPath: \"/var/lib/pgadmin/storage/pgadmin/file.pgpass\"\n  #   readOnly: true\n</code></pre> <p>Change-2: Update Email &amp; Password</p> <pre><code>env:\n  # can be email or nickname\n  email: &lt;your-emailaddress&gt;\n  password: &lt;your-password&gt;\n</code></pre> <p>Change-3: Update config_local.py File</p> <p>Edit the <code>config_local.py</code> file to configure Single Sign-On (SSO) settings for later use:</p> <pre><code>MASTER_PASSWORD_REQUIRED = True\nAUTHENTICATION_SOURCES = ['oauth2', 'internal']\nOAUTH2_AUTO_CREATE_USER = True\nOAUTH2_CONFIG = [\n  {\n      'OAUTH2_NAME': 'microsoft',\n      'OAUTH2_DISPLAY_NAME': 'Microsoft',\n      'OAUTH2_CLIENT_ID': 'your-client-id-here',\n      'OAUTH2_CLIENT_SECRET': 'your-client-secret-here',\n      'OAUTH2_TOKEN_URL': 'https://login.microsoftonline.com/your-tenant-id-here/oauth2/v2.0/token',\n      'OAUTH2_AUTHORIZATION_URL': 'https://login.microsoftonline.com/your-tenant-id-here/oauth2/v2.0/authorize',\n      'OAUTH2_API_BASE_URL': 'https://graph.microsoft.com/oidc/',\n      'OAUTH2_USERINFO_ENDPOINT': 'userinfo',\n      'OAUTH2_ICON': 'fa-microsoft',\n      'OAUTH2_BUTTON_COLOR': '#0000ff',\n      'OAUTH2_SCOPE': 'openid email profile'\n  }\n]\n</code></pre> <p>Change-4: Update Chart.yaml</p> <p>If required, update the <code>appVersion</code> field in the <code>Chart.yaml</code> file to reflect any changes made to the application version.</p> <p>Final folder structure</p> <p>After completing the setup, your folder structure should resemble the following:</p> <pre><code>terraform_project/\n\u2514\u2500\u2500 pgadmin4/\n    \u251c\u2500\u2500 values.yaml\n    \u251c\u2500\u2500 config_local.py\n    \u251c\u2500\u2500 Chart.yaml\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-2-create-namespace-for-pgadmin4","title":"Step-2: Create namespace for pgadmin4","text":"<p>Create a separate namespace for pgadmin4 where all pgadmin4 related resources will be created. let's create a new file called pgadmin4.tf and copy the following configuration.</p> pgadmin4.tf<pre><code>resource \"kubernetes_namespace\" \"pgadmin4\" {  \n  metadata {\n    name = \"pgadmin4\"\n  }\n}\n</code></pre> <p>run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> <p></p> <p>output</p> <p><pre><code>Plan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> run terraform apply</p> <pre><code>terraform apply dev-plan\n</code></pre> <pre><code>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-3-install-pgadmin4-helmchart-using-terraform","title":"Step-3: Install pgadmin4 helmchart using terraform","text":"<p>Visit the official pgadmin4 Helm chart on the ArtifactHUB website: pgadmin4 Helm Chart.</p> <p>Click on the \"Install\" button to retrieve the necessary details for the pgadmin4 Helmchart installation.</p> <pre><code>repository = \"https://helm.runix.net\"\nchart      = \"pgadmin4\"\nversion    = \"1.23.3\"\n</code></pre> pgadmin4.tf<pre><code># Install pgadmin4 helm chart using terraform\nresource \"helm_release\" \"pgadmin4\" {\n  name       = \"pgadmin4\"\n  repository = \"https://helm.runix.net\"\n  chart      = \"pgadmin4\"\n  version    = \"1.23.3\"  \n  namespace = kubernetes_namespace.pgadmin4.metadata.0.name\n  values = [\n    \"${file(\"pgadmin4/pgadmin4.yaml\")}\"\n  ]\n\n  lifecycle {\n    ignore_changes = [\n      # values,\n      version\n    ]\n  }\n  depends_on = [\n    kubernetes_namespace.pgadmin4\n  ]\n}\n</code></pre> <p>run terraform plan</p> <p><pre><code>terraform validate\nterraform fmt\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output <pre><code>Plan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> </p> <p>run terraform apply</p> <p><pre><code>terraform apply dev-plan\n</code></pre> <pre><code>helm_release.pgadmin4: Creating...\nhelm_release.pgadmin4: Still creating... [10s elapsed]\nhelm_release.pgadmin4: Still creating... [20s elapsed]\nhelm_release.pgadmin4: Still creating... [30s elapsed]\nhelm_release.pgadmin4: Still creating... [40s elapsed]\nhelm_release.pgadmin4: Still creating... [50s elapsed]\nhelm_release.pgadmin4: Still creating... [1m0s elapsed]\nhelm_release.pgadmin4: Creation complete after 1m5s [id=pgadmin4]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs\n</code></pre></p> <p>Note</p> <p>Even though the Helm install was completed successfully, I noticed that the pod was not visible in the AKS cluster. I had to manually execute the following command to view the pod in the AKS cluster.</p> <pre><code>helm upgrade pgadmin4 runix/pgadmin4 -n pgadmin4\n</code></pre> <p>output <pre><code>Release \"pgadmin4\" has been upgraded. Happy Helming!\nNAME: pgadmin4\nLAST DEPLOYED: Fri Feb 16 12:10:15 2024\nNAMESPACE: pgadmin4\nSTATUS: deployed\nREVISION: 7\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace pgadmin4 -l \"app.kubernetes.io/name=pgadmin4,app.kubernetes.io/instance=pgadmin4\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl port-forward $POD_NAME 8080:80\n</code></pre></p> <p>commands for direct install:</p> <pre><code>helm install my-release runix/pgadmin4 --set env.password=SuperSecret\nhelm install my-release runix/pgadmin4 -f values.yaml\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-4-verify-pgadmin4-resources-in-aks","title":"Step 4. Verify pgadmin4 resources in AKS","text":"<p>Validate to make sure all the deployments / services created and running as expected. </p> <p>Run the following <code>kubectl</code> commands to verify the pgadmin4 installation in the AKS cluster.</p> <pre><code>kubectl get all -n pgadmin4\n# or\nkubectl get all,configmaps,secrets -n pgadmin4\n</code></pre> <p>or</p> <pre><code>kubectl get namespace pgadmin4\nkubectl get deployments -n pgadmin4\nkubectl get pods -n pgadmin4\nkubectl get services -n pgadmin4\n</code></pre> <p>expected output</p> <pre><code>NAME                            READY   STATUS    RESTARTS   AGE\npod/pgadmin4-765c7ffd6d-6jmr9   1/1     Running   0          5h11m        \n\nNAME               TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE  \nservice/pgadmin4   ClusterIP   10.25.44.90   &lt;none&gt;        80/TCP    5h24m\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/pgadmin4   1/1     1            1           5h24m\n\nNAME                                  DESIRED   CURRENT   READY   AGE     \nreplicaset.apps/pgadmin4-57d4c6955d   0         0         0       5h24m   \nreplicaset.apps/pgadmin4-765c7ffd6d   1         1         1       5h14m \n</code></pre> <pre><code>kubectl get configmaps -n pgadmin4\n</code></pre> <pre><code>NAME               DATA   AGE\nkube-root-ca.crt   1      6h48m\n</code></pre> <pre><code>kubectl get secrets -n pgadmin4\n</code></pre> <pre><code>pgadmin4                         Opaque               1      5h26m\npgadmin4-config                  Opaque               1      5h42m\nsh.helm.release.v1.pgadmin4.v1   helm.sh/release.v1   1      6h20m\nsh.helm.release.v1.pgadmin4.v2   helm.sh/release.v1   1      6h10m\nsh.helm.release.v1.pgadmin4.v3   helm.sh/release.v1   1      5h47m\nsh.helm.release.v1.pgadmin4.v4   helm.sh/release.v1   1      5h35m\nsh.helm.release.v1.pgadmin4.v5   helm.sh/release.v1   1      5h34m\nsh.helm.release.v1.pgadmin4.v6   helm.sh/release.v1   1      5h27m\nsh.helm.release.v1.pgadmin4.v7   helm.sh/release.v1   1      5h26m\n</code></pre> <pre><code>kubectl get ingress -n pgadmin4\n</code></pre> <pre><code>No resources found in argocd namespace.\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-6-access-pgadmin4-locally-port-forwarding","title":"Step-6. Access pgadmin4 locally - port forwarding","text":"<p>To perform local login testing for pgadmin4, you can use port forwarding. Here are the steps:</p> <pre><code>kubectl port-forward service/pgadmin4 -n pgadmin4 8080:80\n</code></pre> <p><pre><code>Forwarding from 127.0.0.1:8080 -&gt; 8080\nForwarding from [::1]:8080 -&gt; 8080\nHandling connection for 8080\n</code></pre> Access the pgadmin4 web interface by visiting http://localhost:8080 in your web browser.</p> <p></p> <p>After Login &gt; Dashboard</p> <p></p> <p>The port forwarding can be stopped by cancelling the command with CTRL + C.</p> <p>login with following credentials</p> <pre><code>username: anjkeesari@gmail.com\nPassword: StrongPassword@\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#step-7-configure-ingress-for-pgadmin4","title":"Step-7. Configure Ingress for pgadmin4","text":"<p>Create an Ingress resource to expose pgAdmin 4 URL use the following yaml manifest  (pgadmin4-ingress.yaml):</p> pgadmin4-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: pgadmin4\n  annotations:\n    kubernetes.io/ingress.class: azure/application-gateway\n    appgw.ingress.kubernetes.io/health-probe-status-codes: \"200-499\"\n    cert-manager.io/cluster-issuer: letsencrypt\n    cert-manager.io/acme-challenge-type: http01\nspec:\n  tls:\n  - hosts:\n    - dev.pgadmin.mydomain.net\n    secretName: tls-secret\n  rules:\n    - host: dev.pgadmin.mydomain.net\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: pgadmin4\n                port:   \n                  number: 80\nstatus:\n  loadBalancer:\n    ingress:\n      - ip: 20.125.213.106\n\n# kubectl apply -f pgadmin4-ingress.yaml -n pgadmin4\n</code></pre> <p>Deploy Ingress YAML file using terraform</p> <p><pre><code># Create pgadmin4 ingress file\ndata \"kubectl_file_documents\" \"pgadmin4_ingress\" {\n  content = file(\"pgadmin4/pgadmin4-ingress.yaml\")\n}\n\n# Apply pgadmin4 ingress file\nresource \"kubectl_manifest\" \"pgadmin4_ingress\" {\n  for_each  = data.kubectl_file_documents.pgadmin4_ingress.manifests\n  yaml_body = each.value\n  depends_on = [\n    data.kubectl_file_documents.pgadmin4_ingress\n  ]\n}\n</code></pre> Apply the Ingress resource by running terraform plan &amp; apply </p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Ensure all resources are correctly deployed and running:</p> <pre><code>kubectl get deployments,services,ingress -n pgadmin4\n</code></pre> <p>Update DNS Records</p> <p>After setting up Ingress, you need to ensure that the domain name you've chosen for pgAdmin 4 points to your Ingress controller's IP address. Here's how to add a new record set in your public DNS zone:</p> <p>Look for a service that exposes an external IP address. This is the IP address you'll use in your DNS configuration.</p> <p>Add DNS record &amp; create a new record set</p> <p>Verify DNS Propagation</p> <p>Before accessing pgAdmin 4, ensure that the DNS changes have propagated and that your domain name resolves to the correct IP address. You can use tools like <code>nslookup</code> or <code>dig</code> for this purpose.</p> <pre><code>nslookup pgadmin.yourdomain.com\n</code></pre> <pre><code>Server:  UnKnown\nAddress:  192.168.1.1\n\nNon-authoritative answer:\nName:    dev.pgadmin.mydomain.com\nAddress:  20.125.213.200\n</code></pre> <pre><code>dig pgadmin.yourdomain.com\n</code></pre> <p>Check the output to ensure that the domain resolves to the external IP address of your Ingress controller.</p>"},{"location":"helmchart/5-install-pgadmin4/#step-8-troubleshooting","title":"Step-8. Troubleshooting","text":"<p>If you're unable to access pgAdmin4 or face issues:</p> <ul> <li>Check Ingress Rules: Ensure your Ingress resource is correctly configured to route traffic to the pgAdmin 4 service.</li> <li>Verify Pod &amp; Service: Ensure the pgAdmin4 pod and service is running.</li> <li>Logs: Check the logs of your pgAdmin4 pod and Ingress controller for any errors or warnings.</li> </ul> <p>Following these steps should help you test and verify that your pgAdmin4 URL is set up correctly and that the application is accessible over the internet.</p> <p>Helm chart troubleshooting details</p> <pre><code>helm repo list\nhelm ls -aA\nhelm list --namespace pgadmin4\n</code></pre> <p>helm upgrade</p> <pre><code>helm install pgadmin4 runix/pgadmin4 --set env.email=&lt;your-email&gt; --set env.password=&lt;your-password&gt; --namespace=&lt;your-namespace&gt; --create-namespace\nhelm upgrade pgadmin4 runix/pgadmin4 -f .\\pgadmin4\\pgadmin4.yaml --create-namespace -n pgadmin4\nhelm upgrade pgadmin4 runix/pgadmin4 -f .\\pgadmin4\\pgadmin4.yaml -n pgadmin4\n</code></pre> <pre><code>Release \"pgadmin4\" has been upgraded. Happy Helming!\nNAME: pgadmin4\nLAST DEPLOYED: Mon Dec  5 07:09:32 2022\nNAMESPACE: pgadmin4\nSTATUS: deployed\nREVISION: 11\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace pgadmin4 -l \"app.kubernetes.io/name=pgadmin4,app.kubernetes.io/instance=pgadmin4\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl port-forward $POD_NAME 8080:80\n</code></pre> <pre><code>helm history pgadmin4 -n pgadmin4\n</code></pre> <p>helm rollback</p> <pre><code>helm rollback pgadmin4 10 \n</code></pre> <pre><code>Rollback was a success! Happy Helming!\n</code></pre> <p>helm uninstall</p> <pre><code>helm uninstall pgadmin4 -n pgadmin4\n</code></pre> <p>pod logs</p> <pre><code>kubectl logs pods/pgadmin4-765c7ffd6d-6jmr9 -n pgadmin4\n</code></pre> <pre><code>postfix/postlog: starting the Postfix mail system\n[2024-02-16 20:23:15 +0000] [1] [INFO] Starting gunicorn 20.1.0\n[2024-02-16 20:23:15 +0000] [1] [INFO] Listening at: http://[::]:80 (1)\n[2024-02-16 20:23:15 +0000] [1] [INFO] Using worker: gthread\n[2024-02-16 20:23:15 +0000] [88] [INFO] Booting worker with pid: 88\n::ffff:10.64.4.10 - - [16/Feb/2024:20:24:09 +0000] \"GET /misc/ping HTTP/1.1\" 200 4 \"-\" \"kube-probe/1.27\"\n::ffff:10.64.4.10 - - [16/Feb/2024:20:24:09 +0000] \"GET /misc/ping HTTP/1.1\" 200 4 \"-\" \"kube-probe/1.27\"\n::ffff:10.64.4.10 - - [16/Feb/2024:20:25:09 +0000] \"GET /misc/ping HTTP/1.1\" 200 4 \"-\" \"kube-probe/1.27\"\n::ffff:10.64.4.10 - - [16/Feb/2024:20:28:09 +0000] \"GET /misc/ping HTTP/1.1\" 200 4 \"-\" \"kube-probe/1.27\"\n</code></pre>"},{"location":"helmchart/5-install-pgadmin4/#reference","title":"Reference","text":"<p>Here is the list of all the resources used while working on this setup </p> <ul> <li>pgAdmin home page</li> <li>Helm Provider for Terraform</li> <li>Explore &gt; dpage/pgadmin4 - get the latest Docker image from here.</li> <li>pgadmin4 Helm Chart</li> <li>pgadmin4 Helm Chart - Github - Helpful for SSO setup</li> <li>pgadmin4 Documentation {:target='_blank'}</li> </ul>"},{"location":"helmchart/6-install-grafana/","title":"Install Grafana Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/6-install-grafana/#introduction","title":"Introduction","text":"<p>Grafana is an open-source analytics and visualization platform that allows you to monitor, analyze, and understand your metrics. It provides a flexible and powerful tool for observing your systems and applications. In this guide, I will walk you through the process of installing Grafana using Helmchart in Azure Kubernetes Services (AKS). This tutorial is designed for beginners, providing step-by-step instructions along with the necessary <code>kubectl</code> commands and outputs.</p> <p>Azure Kubernetes Service (AKS) is a managed Kubernetes service provided by Microsoft Azure, allowing you to deploy, manage, and scale containerized applications using Kubernetes. Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. Grafana Helmchart is a Helm chart that simplifies the deployment of Grafana in Kubernetes clusters.</p> <p>In this tutorial, I will guide you through the process of installing Grafana using Helmchart in an AKS cluster.</p>"},{"location":"helmchart/6-install-grafana/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Login into Azure</li> <li>Step 2. Connect to AKS Cluster</li> <li>Step 3. Add Grafana Helm Repository</li> <li>Step 4. Install Grafana Helmchart</li> <li>Step 5. Verify Grafana Resources in AKS</li> <li>Step 6. Access Grafana Dashboard Locally - port forwarding</li> <li>Step 7. Configure Ingress for Grafana</li> </ul>"},{"location":"helmchart/6-install-grafana/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An Azure account with access to AKS.</li> <li><code>Azure CLI</code> installed on your local machine.</li> <li><code>kubectl</code> installed on your local machine.</li> <li><code>Helm</code> installed on your local machine.</li> </ul>"},{"location":"helmchart/6-install-grafana/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/6-install-grafana/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/6-install-grafana/#step-3-add-grafana-helm-repository","title":"Step 3: Add Grafana Helm Repository","text":"<p>Before installing Grafana, you need to add the Helm repository for Grafana:</p> <pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\n</code></pre> <pre><code>helm list -aA\nhelm list --namespace grafana\n</code></pre>"},{"location":"helmchart/6-install-grafana/#step-4-install-grafana-helmchart","title":"Step 4: Install Grafana Helmchart","text":"<p>Now, you can install Grafana using Helmchart. Execute the following command:</p> <pre><code># use this command if you need to create namespace along with helm install\nhelm install grafana grafana/grafana --namespace grafana --create-namespace --version 7.3.4\n\n# use this command if you already have namespace created\nhelm install grafana grafana/grafana --version 7.3.4 -n grafana\n</code></pre> <p>This command installs Grafana in your AKS cluster. You can customize the installation by providing values files or using Helm chart options.</p>"},{"location":"helmchart/6-install-grafana/#step-5-verify-grafana-resources-in-aks","title":"Step 5: Verify Grafana Resources in AKS","text":"<p>To verify that Grafana has been successfully installed, you can list the Kubernetes resources:</p> <pre><code>kubectl get pods -n grafana\nkubectl get all -n grafana\nkubectl get ConfigMap,Secret,Ingress -n grafana\nkubectl logs deployments/grafana -n grafana\n</code></pre> <p>You should see Grafana pods and services running in your cluster.</p>"},{"location":"helmchart/6-install-grafana/#step-6-access-grafana-dashboard-locally","title":"Step 6: Access Grafana Dashboard Locally","text":"<p>To access the Grafana dashboard locally, you need to create a port forward:</p> <pre><code>kubectl port-forward service/grafana 3000:80 --namespace grafana\n</code></pre> <pre><code>http://localhost:3000/login\n</code></pre> <p>Now, you can access Grafana by opening your web browser and navigating to <code>http://localhost:3000/login</code>. Use the default username (admin) to log in.</p> <p>You need to retrieve the <code>admin</code> password by running the following commands:</p> <pre><code># bash\nkubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n# pwsh\nkubectl get secret --namespace grafana grafana -o jsonpath=\"{.data.admin-password}\" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }\n</code></pre> <p>After running the above commands, you should be able to access Grafana at <code>http://localhost:3000</code> in your web browser. Log in using the username admin and the password obtained from the previous command.</p> <p>Grafana &gt; Login page </p> <p>Grafana &gt; Welcome page </p> <p>Grafana &gt; Connection page </p> <p>Grafana &gt; Dashboard page </p>"},{"location":"helmchart/6-install-grafana/#step-7-configure-ingress-for-grafana","title":"Step 7: Configure Ingress for Grafana","text":"<p>If you want to access Grafana externally, you can configure Ingress. First, create an Ingress resource:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\nspec:\n  rules:\n  - host: yourdomain.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 80\n</code></pre> <p>Apply the Ingress configuration:</p> <pre><code>kubectl apply -f ingress.yaml\n</code></pre> <p>Replace <code>yourdomain.com</code> with your actual domain name.</p>"},{"location":"helmchart/6-install-grafana/#conclusion","title":"Conclusion","text":"<p>You have successfully installed Grafana using Helmchart in Azure Kubernetes Services. Grafana provides powerful visualization and monitoring capabilities for your applications running in AKS. You can now explore and visualize your metrics to gain insights into your system's performance.</p>"},{"location":"helmchart/6-install-grafana/#reference","title":"Reference","text":"<ul> <li>Azure Kubernetes Service Documentation</li> <li>Helm Documentation</li> <li>Grafana Helm Chart Repository</li> <li>Kubernetes Documentation</li> <li>Azure CLI Documentation</li> </ul>"},{"location":"helmchart/7-install-grafana-loki-stack/","title":"Install Grafana Loki-Stack Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/7-install-grafana-loki-stack/#introduction","title":"Introduction","text":"<p>Grafana Loki is a powerful log aggregation system that allows users to collect, store, and query logs in a highly efficient and scalable manner. When integrated with Kubernetes, Grafana Loki provides a seamless solution for monitoring and analyzing logs within AKS clusters. By leveraging Helm charts, the deployment process becomes streamlined and reproducible, ensuring consistent setups across different environments.</p> <p>In this tutorial, I will guide you through the process of installing Grafana Loki-Stack using Helmchart in an AKS cluster.</p>"},{"location":"helmchart/7-install-grafana-loki-stack/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Login into Azure</li> <li>Step 2. Connect to AKS Cluster</li> <li>Step 3. Add Grafana Loki-Stack Helm Repository</li> <li>Step 4. Install Grafana Loki-Stack Helmchart</li> <li>Step 5. Verify Grafana Loki-Stack Resources in AKS</li> <li>Step 6. Access Grafana Loki-Stack Dashboard Locally - port forwarding</li> <li>Step 7. Configure Ingress for Grafana Loki-Stack</li> </ul>"},{"location":"helmchart/7-install-grafana-loki-stack/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An Azure account with access to AKS.</li> <li><code>Azure CLI</code> installed on your local machine.</li> <li><code>kubectl</code> installed on your local machine.</li> <li><code>Helm</code> installed on your local machine.</li> </ul>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-3-add-grafana-loki-stack-helm-repository","title":"Step 3: Add Grafana Loki-Stack Helm Repository","text":"<p>Before installing Grafana Loki-Stack, you need to add the Helm repository for Grafana Loki-Stack:</p> <pre><code>helm repo list\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm search repo grafana\n</code></pre> <p></p> <p></p> <p></p> <pre><code>helm list -aA\nhelm list --namespace grafana\n</code></pre> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\nloki-stack      loki-stack      1               2024-03-16 17:22:22.273156 -0700 PDT    deployed        loki-stack-2.10.2       v2.9.3  \n</code></pre>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-4-install-grafana-loki-stack-helmchart","title":"Step 4: Install Grafana Loki-Stack Helmchart","text":"<p>Now, you can install Grafana Loki-Stack using Helmchart. Execute the following command:</p> <p>Let's first see the values: <pre><code>helm show values grafana/loki-stack &gt; C:\\Source\\Repos\\IaC\\terraform\\loki-stack-values.yaml\n</code></pre></p> <p></p> <p><pre><code># use this command if you need to create namespace along with helm install\nhelm upgrade --install loki-stack --values loki-stack-values.yaml grafana/loki-stack --namespace loki-stack --create-namespace --version 2.10.2\n\n# use this command if you already have namespace created\nhelm upgrade --install loki-stack --values loki-stack-values.yaml grafana/loki-stack --version 2.10.2\n</code></pre> </p> <p>This command installs Grafana Loki-Stack in your AKS cluster. You can customize the installation by providing values files or using Helm chart options.</p>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-5-verify-grafana-loki-stack-resources-in-aks","title":"Step 5: Verify Grafana Loki-Stack Resources in AKS","text":"<p>To verify that Grafana Loki-Stack has been successfully installed, you can list the Kubernetes resources:</p> <pre><code>kubectl get all -n loki-stack\nkubectl get all,configmap,secret -n loki-stack\nkubectl get pods -n loki-stack\n</code></pre> <p></p> <p>You should see Grafana Loki-Stack pods and services running in your cluster.</p>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-6-access-grafana-loki-stack-dashboard-locally","title":"Step 6: Access Grafana Loki-Stack Dashboard Locally","text":"<p>To access the Grafana Loki-Stack dashboard locally, you need to create a port forward:</p> <pre><code>kubectl port-forward svc/loki-stack -n loki-stack 3100:3100\n</code></pre> <pre><code>http://localhost:3100/ready\n</code></pre> <p>To access the Grafana dashboard locally, you need to create a port forward:</p> <pre><code>kubectl port-forward svc/loki-stack-grafana -n loki-stack 3000:3000\n</code></pre> <pre><code>http://localhost:3000/login\n</code></pre> <p>Now, you can access Grafana &amp; Loki-Stack by opening your web browser. Use the default username (admin) to log in.</p> <p>You need to retrieve the <code>admin</code> password by running the following commands:</p> <pre><code># bash\nkubectl get secret --namespace loki-stack grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n# pwsh\nkubectl get secret --namespace loki-stack grafana -o jsonpath=\"{.data.admin-password}\" | ForEach-Object { [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($_)) }\n</code></pre> <p>After running the above commands, you should be able to access Grafana Loki-Stack at <code>http://localhost:3000</code> in your web browser. Log in using the username admin and the password obtained from the previous command.</p> <p>Grafana &gt; Login page </p> <p>Grafana &gt; Welcome page </p> <p>Grafana &gt; Connection page </p> <p>Grafana &gt; Dashboard page </p> <p>Loki &gt; ready state</p> <p></p> <p>Loki &gt; Metrics</p> <p></p> <p>Loki &gt; Config</p> <p></p> <p>Loki &gt; Labels</p> <p></p> <p>Grafana &gt; Loki data source - added by default</p> <p></p> <p>Loki &gt; access pod logs</p> <p></p> <p>Other Loki endpoints for testing:</p> <ul> <li>http://localhost:3100/ready</li> <li>http://localhost:3100/log_level</li> <li>http://localhost:3100/metrics</li> <li>http://localhost:3100/config</li> <li>http://localhost:3100/services</li> <li>http://localhost:3100/loki/api/v1/status/buildinfo</li> <li>http://localhost:3100/loki/api/v1/format_query</li> <li>http://localhost:3100/loki/api/v1/labels</li> <li>http://localhost:3100/loki/api/v1/push</li> </ul>"},{"location":"helmchart/7-install-grafana-loki-stack/#step-7-configure-ingress-for-grafana-loki-stack","title":"Step 7: Configure Ingress for Grafana Loki-Stack","text":"<p>If you want to access Grafana externally, you can configure Ingress. First, create an Ingress resource:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana-ingress\nspec:\n  rules:\n  - host: yourdomain.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: loki-stack-grafana\n            port:\n              number: 80\n</code></pre> <p>Apply the Ingress configuration:</p> <pre><code>kubectl apply -f grafana-ingress.yaml\n</code></pre> <p>loki Ingress</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: loki-ingress\nspec:\n  rules:\n  - host: yourdomain.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: loki-stack\n            port:\n              number: 3100\n</code></pre> <p>Apply the Ingress configuration:</p> <p><pre><code>kubectl apply -f loki-ingress.yaml\n</code></pre> Replace <code>yourdomain.com</code> with your actual domain name.</p> <p>clean up resources:</p> <pre><code>helm delete loki-stack -n loki-stack\nhelm delete grafana -n grafana\n</code></pre>"},{"location":"helmchart/7-install-grafana-loki-stack/#conclusion","title":"Conclusion","text":"<p>You have successfully installed Grafana Loki-Stack using Helmchart in Azure Kubernetes Services. The integration of Grafana Loki into Azure Kubernetes Services using Helm charts simplifies the deployment process and empowers users with powerful log monitoring capabilities, enhancing observability within Kubernetes clusters.</p>"},{"location":"helmchart/7-install-grafana-loki-stack/#reference","title":"Reference","text":"<ul> <li>Grafana Loki-Stack Helm Chart Repository</li> <li>Getting started with Azure Kubernetes Service and Loki</li> </ul>"},{"location":"helmchart/8-install-redis/","title":"Install Redis Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/8-install-redis/#introduction","title":"Introduction","text":"<p>In the microservices and cloud-native applications, efficient data caching is crucial for maintaining performance of the application. Redis, an open-source, in-memory data structure store, is commonly used for data caching. Azure Kubernetes Service (AKS) offers a robust platform for deploying and managing containerized applications, including Redis caches.</p> <p>This article will guide you through the process of installing Redis instances using Helm charts in Azure Kubernetes Services (AKS). Helm is a package manager for Kubernetes that simplifies the deployment and management of applications.</p>"},{"location":"helmchart/8-install-redis/#objective","title":"Objective","text":"<p>The objective of this tutorial is to demonstrate how to deploy Redis Cache in an AKS cluster using Helm charts. By the end of this tutorial, you will have a fully functional Redis cache running in your AKS environment. In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Login into Azure</li> <li>Step 2. Connect to AKS Cluster</li> <li>Step 3. Add Redis Helm Repository</li> <li>Step 4. Install Redis  Helmchart</li> <li>Step 5. Verify Redis Resources in AKS</li> <li>Step 6. Get Redis password</li> <li>Step 7. Connect using  redis-cli tool</li> <li>Step 8. Uninstalling the Chart</li> </ul>"},{"location":"helmchart/8-install-redis/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An Azure account with permissions to create resources.</li> <li><code>Azure CLI</code> installed on your local machine.</li> <li><code>kubectl</code> installed on your local machine.</li> <li><code>Helm</code> installed on your local machine.</li> </ul>"},{"location":"helmchart/8-install-redis/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/8-install-redis/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/8-install-redis/#step-3-add-redis-helm-repository","title":"Step 3: Add Redis Helm Repository","text":"<p>Before installing Redis Cache, you need to add the Helm repository for Redis Cache, here I am using bitnami <code>Redis Helm Chart</code>.</p> <p>There are two available Helm Charts: the <code>Redis Helm Chart</code> and the <code>Redis Cluster Helm Chart</code>. You can find more details here and choose whichever is the right choice for you.</p> <pre><code>helm repo list\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\nhelm search repo redis\n</code></pre> <pre><code>helm list -aA\nhelm list --namespace redis\n</code></pre> <pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\n</code></pre>"},{"location":"helmchart/8-install-redis/#step-4-install-redis-helmchart","title":"Step 4: Install Redis Helmchart","text":"<p>Now, you can install Redis using Helmchart. Execute the following command:</p> <p>Let's first see the values, this will allow you to see what is getting installed in your AKS cluster.</p> <pre><code>helm show values bitnami/redis &gt; C:\\Source\\Repos\\IaC\\terraform\\redis-values.yaml\n</code></pre> <p>Install Helm Chart:</p> <pre><code># use this command if you need to create namespace along with helm install\nhelm install redis bitnami/redis -n redis --create-namespace --version 19.0.1\n\n# use this command if you already have namespace created\nhelm upgrade --install redis bitnami/redis -n redis --version 19.0.1\n</code></pre> <p>This command installs Redis Cache in your AKS cluster. You can customize the installation by providing values files or using Helm chart options.</p> <p>output:</p> <p><pre><code>NAME: redis\nLAST DEPLOYED: Sat Mar 23 20:17:17 2024\nNAMESPACE: redis\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: redis\nCHART VERSION: 19.0.1\nAPP VERSION: 7.2.4\n\n** Please be patient while the chart is being deployed **\n\nRedis&amp;reg; can be accessed on the following DNS names from within your cluster:\n\n    redis-master.redis.svc.cluster.local for read/write operations (port 6379)\n    redis-replicas.redis.svc.cluster.local for read-only operations (port 6379)\n\nTo get your password run:\n\n    export REDIS_PASSWORD=$(kubectl get secret --namespace redis redis -o jsonpath=\"{.data.redis-password}\" | base64 -d)\n\nTo connect to your Redis&amp;reg; server:\n\n1. Run a Redis&amp;reg; pod that you can use as a client:\n\n   kubectl run --namespace redis redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.2.4-debian-12-r9 --command -- sleep infinity\n\n   Use the following command to attach to the pod:\n\n   kubectl exec --tty -i redis-client \\\n   --namespace redis -- bash\n\n2. Connect using the Redis&amp;reg; CLI:\n   REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-master\n   REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-replicas\n\n\n    kubectl port-forward --namespace redis svc/redis-master 6379:6379 &amp;\n    REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h 127.0.0.1 -p 6379\n\nWARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n  - master.resources\n  - replica.resources\n+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n</code></pre> </p> <p>Install Redis Helmchart using Terraform</p> <p>If you are looking for a fully automated way to set up Redis using Terraform, you can use the following piece of code. Make sure you have set up your Terraform environment correctly on your system.</p> <pre><code># create a new namespace for redis\nresource \"kubernetes_namespace\" \"redis\" {\n  metadata {\n    name = \"redis\"\n  }\n}\n\n# Install REDIS (bitnami) helm chart using terraform\nresource \"helm_release\" \"redis\" {\n  name       = \"redis\"\n  namespace  = kubernetes_namespace.redis.metadata.0.name\n  chart      = \"redis\"\n  repository = \"https://charts.bitnami.com/bitnami\"\n  version    = \"18.0.0\"\n  depends_on = [\n    kubernetes_namespace.redis,\n    azurerm_kubernetes_cluster.aks\n  ]\n}\n</code></pre> <p>This Terraform configuration installs the Redis (Bitnami) Helm chart in the \"redis\" namespace. Adjust the configuration according to your specific requirements and environment setup.</p>"},{"location":"helmchart/8-install-redis/#step-5-verify-redis-resources-in-aks","title":"Step 5: Verify Redis Resources in AKS","text":"<p>To verify that Redis has been successfully installed, you can list the Kubernetes resources:</p> <pre><code>helm list -aA\nhelm list --namespace redis\n</code></pre> <pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nredis   redis           1               2024-03-23 20:17:17.1864465 -0700 PDT   deployed        redis-19.0.1    7.2.4  \n</code></pre> <pre><code>kubectl get all -n redis\nkubectl get all,configmap,secret -n redis\nkubectl get pods -n redis\n</code></pre> <p>output <pre><code>NAME                            DATA   AGE\nconfigmap/kube-root-ca.crt      1      4m37s\nconfigmap/redis-configuration   3      4m36s\nconfigmap/redis-health          6      4m36s\nconfigmap/redis-scripts         2      4m36s\n\nNAME                                 TYPE                 DATA   AGE\nsecret/redis                         Opaque               1      4m36s\nsecret/sh.helm.release.v1.redis.v1   helm.sh/release.v1   1      4m37s\n\nNAME                   READY   STATUS    RESTARTS   AGE\npod/redis-master-0     1/1     Running   0          4m35s\nNAME                              READY   AGE\nstatefulset.apps/redis-master     1/1     4m37s\nstatefulset.apps/redis-replicas   3/3     4m37s\n</code></pre></p> <p>You should see Redis Cache pods and services running in your cluster.</p>"},{"location":"helmchart/8-install-redis/#step-6-get-redis-password","title":"Step 6: Get Redis password","text":"<p>To get your password run:</p> <pre><code># bash\nkubectl get secret --namespace redis redis -o jsonpath=\"{.data.redis-password}\" | base64 -d\nexport REDIS_PASSWORD=$(kubectl get secret --namespace redis redis -o jsonpath=\"{.data.redis-password}\" | base64 -d)\n\n# PowerShell\nkubectl get secret --namespace redis redis -o json | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty 'redis-password' | ForEach-Object { [System.Text.Encoding]::Utf8.GetString([System.Convert]::FromBase64String($_)) }\n\n# Get the Redis password secret from Kubernetes\n$redisSecret = kubectl get secret --namespace redis redis -o json | ConvertFrom-Json\n# Decode the password from base64\n$redisPassword = [System.Text.Encoding]::Utf8.GetString([System.Convert]::FromBase64String($redisSecret.data.\"redis-password\"))\n# Set the Redis password as an environment variable\n$env:REDIS_PASSWORD = $redisPassword\n</code></pre> <p>output  <pre><code>Bu0zgY9CRH\n</code></pre></p>"},{"location":"helmchart/8-install-redis/#step-7-connecting-redis-with-redis-cli","title":"Step 7: Connecting Redis with redis-cli","text":"<p>Connecting Redis Cache with <code>redis-cli</code> allows you to interact with the Redis server directly from the command line. redis-cli is a command-line interface utility provided by Redis that enables you to execute various commands against the Redis server.</p> <p>First get the password to connect to your Redis server:</p> <p><pre><code>export REDIS_PASSWORD=$(kubectl get secret --namespace redis redis -o jsonpath=\"{.data.redis-password}\" | base64 -d)\n</code></pre> - Run a Redis pod that you can use as a client:</p> <pre><code>kubectl run --namespace redis redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.2.4-debian-12-r9 --command -- sleep infinity\n</code></pre> <p>output</p> <pre><code>pod/redis-client created\n</code></pre> <p>Use the following command to attach to the pod:</p> <pre><code>kubectl exec --tty -i redis-client \\\n    --namespace redis -- bash\n</code></pre> <p>output</p> <pre><code>I have no name!@redis-client:/$ REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-master\n</code></pre> <ul> <li>Connect using the Redis; CLI:</li> </ul> <pre><code>REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-master\nREDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h redis-replicas\n</code></pre> <pre><code>kubectl port-forward --namespace redis svc/redis-master 6379:6379 &amp;\n    REDISCLI_AUTH=\"$REDIS_PASSWORD\" redis-cli -h 127.0.0.1 -p 6379\n</code></pre> <p><pre><code>PING\n</code></pre> output</p> <pre><code>PONG\n</code></pre> <p>Next run following set commands from the CLI:</p> <p><pre><code>set key1 hello\nset key2 world\nset key3 goodbye\nset key4 world\n</code></pre> The same will apply when you get the values. <pre><code>get key1\nget key2\nget key3\nget key4\n</code></pre></p> <p>You will see output that indicates the client is being redirected to multiple nodes.</p> <pre><code>info\n</code></pre> <p>output</p> <pre><code># Server\nredis_version:7.2.4\nredis_git_sha1:00000000\nredis_git_dirty:0\nredis_build_id:d7e3021cf535cf9d    \nredis_mode:standalone\nos:Linux 5.15.0-1054-azure x86_64  \narch_bits:64\nmonotonic_clock:POSIX clock_gettime\nmultiplexing_api:epoll\natomicvar_api:c11-builtin\ngcc_version:12.2.0\nprocess_id:1\nprocess_supervised:no\nrun_id:b6a2fa96ca6a784d1cd5550b45e7c6bcfc01ba4d\ntcp_port:6379\nserver_time_usec:1711251620634105\nuptime_in_seconds:1366\nuptime_in_days:0\nhz:10\nconfigured_hz:10\nlru_clock:16752804\nexecutable:/redis-server\nconfig_file:\nio_threads_active:0\nlistener0:name=tcp,bind=*,bind=-::*,port=6379\n\n# Clients\nconnected_clients:1\ncluster_connections:0\nmaxclients:10000\nclient_recent_max_input_buffer:20480\nclient_recent_max_output_buffer:20504\nblocked_clients:0\ntracking_clients:0\nclients_in_timeout_table:0\ntotal_blocking_keys:0\ntotal_blocking_keys_on_nokey:0\n\n# Memory\nused_memory:1104560\nused_memory_human:1.05M\nused_memory_rss:16007168\nused_memory_rss_human:15.27M\nused_memory_peak:1322552\nused_memory_peak_human:1.26M\nused_memory_peak_perc:83.52%\nused_memory_overhead:888676\nused_memory_startup:866048\nused_memory_dataset:215884\nused_memory_dataset_perc:90.51%\nallocator_allocated:1233272\nallocator_active:1486848\nallocator_resident:4481024\ntotal_system_memory:16767619072\ntotal_system_memory_human:15.62G\nused_memory_lua:31744\nused_memory_vm_eval:31744\nused_memory_lua_human:31.00K\nused_memory_scripts_eval:0\nnumber_of_cached_scripts:0\nnumber_of_functions:0\nnumber_of_libraries:0\nused_memory_vm_functions:32768\nused_memory_vm_total:64512\nused_memory_vm_total_human:63.00K\nused_memory_functions:184\nused_memory_scripts:184\nused_memory_scripts_human:184B\nmaxmemory:0\nmaxmemory_human:0B\nmaxmemory_policy:noeviction\nallocator_frag_ratio:1.21\nallocator_frag_bytes:253576\nallocator_rss_ratio:3.01\nallocator_rss_bytes:2994176\nrss_overhead_ratio:3.57\nrss_overhead_bytes:11526144\nmem_fragmentation_ratio:14.80\nmem_fragmentation_bytes:14925496\nmem_not_counted_for_evict:8\nmem_replication_backlog:20508\nmem_total_replication_buffers:20504\nmem_clients_slaves:0\nmem_clients_normal:1928\nmem_cluster_links:0\nmem_aof_buffer:8\nmem_allocator:jemalloc-5.3.0\nactive_defrag_running:0\nlazyfree_pending_objects:0\nlazyfreed_objects:0\n'\n'\n'\n'\n\n# CPU\nused_cpu_sys:1.345162\nused_cpu_user:1.532935\nused_cpu_sys_children:0.001488\nused_cpu_user_children:0.004844\nused_cpu_sys_main_thread:1.342549\nused_cpu_user_main_thread:1.532146\n\n# Modules\n\n# Errorstats\nerrorstat_ERR:count=3\nerrorstat_NOAUTH:count=3\n\n# Cluster\ncluster_enabled:0\n\n# Keyspac\n</code></pre>"},{"location":"helmchart/8-install-redis/#step-8-uninstalling-the-chart","title":"Step 8: Uninstalling the Chart","text":"<p>Once you're done experimenting, you can delete the Redis Cache deployment and associated resources:</p> <p>To uninstall/delete the redis helm deployment run:</p> <p><pre><code>helm list --namespace redis\nhelm delete redis -n redis\nkubectl delete namespace redis\n</code></pre> The command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"helmchart/8-install-redis/#conclusion","title":"Conclusion","text":"<p>In this tutorial, you learned how to deploy Redis Cache in Azure Kubernetes Services using Helm charts. By following these steps, you can integrate Redis Cache seamlessly into your AKS environment, enabling efficient data caching for your containerized applications.</p>"},{"location":"helmchart/8-install-redis/#reference","title":"Reference","text":"<ul> <li>Run scalable and resilient Redis with AKS</li> <li>helm/charts</li> <li>Bitnami package for Redis</li> </ul>"},{"location":"helmchart/9-install-minio/","title":"Install Minio Helmchart in Azure Kubernetes Services (AKS)","text":""},{"location":"helmchart/9-install-minio/#introduction","title":"Introduction","text":"<p>Minio is a high-performance object storage server built for cloud-native applications and Kubernetes. It is compatible with Amazon S3 APIs, making it easy to integrate with existing S3-compatible applications. </p> <p>Microservices often need to store and retrieve large volumes of data. Minio provides a scalable, distributed, and high-performance object storage solution that can handle large amounts of unstructured data. Microservices can interact with Minio via its S3-compatible APIs to store and retrieve data efficiently.</p> <p>In this guide, we will explore how to deploy Minio on Azure Kubernetes Services (AKS) using Helm, a package manager for Kubernetes.</p>"},{"location":"helmchart/9-install-minio/#objective","title":"Objective","text":"<p>The objective of this tutorial is to demonstrate how to deploy Minio on Azure Kubernetes Services using Helm charts. By the end of this tutorial, you will have Minio up and running in your AKS environment.</p> <p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Login into Azure</li> <li>Step 2. Connect to AKS Cluster</li> <li>Step 3. Add Minio Helm Repository</li> <li>Step 4. Install Minio Helmchart</li> <li>Step 5. Verify Minio Resources in AKS</li> <li>Step 6. Get Minio login Password</li> <li>Step 7. Access Minio Locally - port forwarding</li> <li>Step 8. Configure Ingress for Minio</li> </ul>"},{"location":"helmchart/9-install-minio/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following prerequisites:</p> <ul> <li>An <code>Azure account</code> with permissions to create resources.</li> <li><code>Azure CLI</code> installed on your local machine.</li> <li><code>kubectl</code> installed on your local machine.</li> <li><code>Helm</code> installed on your local machine.</li> <li>Access to an existing <code>AKS cluster</code> or create a new one.</li> </ul>"},{"location":"helmchart/9-install-minio/#step-1-login-into-azure","title":"Step 1: Login into Azure","text":"<p>Ensure that you are logged into the correct Azure subscription before proceeding.</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Follow the on-screen instructions to complete the login process.</p>"},{"location":"helmchart/9-install-minio/#step-2-connect-to-aks-cluster","title":"Step 2: Connect to AKS Cluster","text":"<p>Once logged in and set your subscription then connect to your AKS cluster. with your AKS cluster name:</p> <p>Use the <code>az aks get-credentials</code> command to connect to the AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"helmchart/9-install-minio/#step-3-add-minio-helm-repository","title":"Step 3: Add Minio Helm Repository","text":"<p>Before installing Minio, you need to add the Helm repository for Minio:</p> <p><pre><code>helm repo list\n</code></pre> output <pre><code>NAME                                    URL\nbitnami                                 https://charts.bitnami.com/bitnami\nrunix                                   https://helm.runix.net\ningress-nginx                           https://kubernetes.github.io/ingress-nginx\njetstack                                https://charts.jetstack.io\nprometheus-community                    https://prometheus-community.github.io/helm-charts\napache-solr                             https://solr.apache.org/charts\nazure-marketplace                       https://marketplace.azurecr.io/helm/v1/repo\nbitnami-azure                           https://marketplace.azurecr.io/helm/v1/repo\njaegertracing                           https://jaegertracing.github.io/helm-charts\ncsi-secrets-store-provider-azure        https://azure.github.io/secrets-store-csi-driver-provider-azure/charts\ncert-manager                            https://charts.jetstack.io\ndynatrace                               https://raw.githubusercontent.com/Dynatrace/dynatrace-operator/main/config/helm/repos/stable\nopen-telemetry                          https://open-telemetry.github.io/opentelemetry-helm-charts\nemberstack                              https://emberstack.github.io/helm-charts\ngrafana                                 https://grafana.github.io/helm-charts\n</code></pre></p> <p><pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> output</p> <pre><code>\"bitnami\" already exists with the same configuration, skipping\n</code></pre> <pre><code>helm repo update\n</code></pre> <p>output <pre><code>Hang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"csi-secrets-store-provider-azure\" chart repository\n...Successfully got an update from the \"runix\" chart repository\n...Successfully got an update from the \"dynatrace\" chart repository\n...Successfully got an update from the \"ingress-nginx\" chart repository\n...Successfully got an update from the \"emberstack\" chart repository\n...Successfully got an update from the \"jaegertracing\" chart repository\n...Successfully got an update from the \"open-telemetry\" chart repository\n...Successfully got an update from the \"cert-manager\" chart repository\n...Successfully got an update from the \"jetstack\" chart repository\n...Successfully got an update from the \"grafana\" chart repository\n...Successfully got an update from the \"apache-solr\" chart repository\n...Successfully got an update from the \"prometheus-community\" chart repository\n...Successfully got an update from the \"azure-marketplace\" chart repository\n...Successfully got an update from the \"bitnami-azure\" chart repository\n...Successfully got an update from the \"bitnami\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\n</code></pre></p> <p>helm list before install</p> <p><pre><code>helm list -aA\nhelm list --namespace minio\n</code></pre> output</p> <pre><code>NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART                   APP VERSION\n</code></pre>"},{"location":"helmchart/9-install-minio/#step-4-install-minio-helmchart","title":"Step 4: Install Minio Helmchart","text":"<p>Now, you can install Minio using Helmchart. Execute the following command:</p> <p>Let's first search the helm chart which we want to install. here I am installing bitnami helmchart.</p> <pre><code>helm search repo minio\n</code></pre> <p>output</p> <pre><code>NAME                    CHART VERSION   APP VERSION     DESCRIPTION\nazure-marketplace/minio 11.10.13        2022.10.29      MinIO(R) is an object storage server, compatibl...\nbitnami-azure/minio     11.10.13        2022.10.29      MinIO(R) is an object storage server, compatibl...\nbitnami/minio           14.1.2          2024.3.21       MinIO(R) is an object storage server, compatibl...\n</code></pre> <p>Let's first see the values: <pre><code>helm show values bitnami/minio &gt; C:\\Source\\Repos\\IaC\\terraform\\minio-values.yaml\n</code></pre></p> <pre><code># use this command if you need to create namespace along with helm install\nhelm install minio bitnami/minio -n minio --create-namespace --version 14.1.2\n\n# use this command if you already have namespace created\nhelm upgrade --install minio bitnami/minio -n minio --version 14.1.2\n</code></pre> <pre><code>NAME: minio\nLAST DEPLOYED: Sun Mar 24 09:04:43 2024\nNAMESPACE: minio\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: minio\nCHART VERSION: 14.1.2\nAPP VERSION: 2024.3.21\n\n** Please be patient while the chart is being deployed **\n\nMinIO&amp;reg; can be accessed via port  on the following DNS name from within your cluster:\n\n   minio.minio.svc.cluster.local\n\nTo get your credentials run:\n\n   export ROOT_USER=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-user}\" | base64 -d)\n   export ROOT_PASSWORD=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-password}\" | base64 -d)\n\nTo connect to your MinIO&amp;reg; server using a client:\n\n- Run a MinIO&amp;reg; Client pod and append the desired command (e.g. 'admin info'):\n\n   kubectl run --namespace minio minio-client \\\n     --rm --tty -i --restart='Never' \\\n     --env MINIO_SERVER_ROOT_USER=$ROOT_USER \\\n     --env MINIO_SERVER_ROOT_PASSWORD=$ROOT_PASSWORD \\\n     --env MINIO_SERVER_HOST=minio \\\n     --image docker.io/bitnami/minio-client:2024.3.20-debian-12-r0 -- admin info minio\n\nTo access the MinIO&amp;reg; web UI:\n\n- Get the MinIO&amp;reg; URL:\n\n   echo \"MinIO&amp;reg; web URL: http://127.0.0.1:9001/minio\"\n   kubectl port-forward --namespace minio svc/minio 9001:9001\n\nWARNING: There are \"resources\" sections in the chart not set. Using \"resourcesPreset\" is not recommended for production. For production installations, please set the following values according to your workload needs:\n  - resources\n+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n</code></pre> <p></p> <p>This command installs Minio in your AKS cluster. You can customize the installation by providing values files or using Helm chart options.</p>"},{"location":"helmchart/9-install-minio/#step-5-verify-minio-resources-in-aks","title":"Step 5: Verify Minio Resources in AKS","text":"<p>To verify that Minio has been successfully installed, you can list the Kubernetes resources:</p> <p><pre><code>helm list --namespace minio\n</code></pre> output</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nminio   minio           1               2024-03-24 09:04:43.0611222 -0700 PDT   deployed        minio-14.1.2    2024.3.21\n</code></pre> <pre><code>kubectl get pods -n minio\n# List all pods in the default namespace\nkubectl get all -n minio\nkubectl get configmap,secret,ingress,all -n minio\n</code></pre> <p>output</p> <p><pre><code>NAME                         DATA   AGE\nconfigmap/kube-root-ca.crt   1      43m\n\nNAME                                 TYPE                 DATA   AGE\nsecret/minio                         Opaque               2      43m\nsecret/sh.helm.release.v1.minio.v1   helm.sh/release.v1   1      43m\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/minio-7fbf9844c9-nn7w2   1/1     Running   0          43m\n\nNAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\nservice/minio   ClusterIP   10.25.104.246   &lt;none&gt;        9000/TCP,9001/TCP   43m\n\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/minio   1/1     1            1           43m\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/minio-7fbf9844c9   1         1         1       43m\n</code></pre> You should see Minio pods and services running in your cluster.</p> <p>Describe Minio pod for more details</p> <pre><code>kubectl describe pod/minio-7fbf9844c9-nn7w2 -n minio\n</code></pre> <p>Describe Minio pod logs for more details</p> <pre><code>kubectl logs pod/minio-7fbf9844c9-nn7w2 -n minio\n</code></pre>"},{"location":"helmchart/9-install-minio/#step-6-get-minio-password","title":"Step 6: Get Minio password","text":"<p>To get your credentials run:</p> <pre><code># bash\nexport ROOT_USER=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-user}\" | base64 -d)\nexport ROOT_PASSWORD=$(kubectl get secret --namespace minio minio -o jsonpath=\"{.data.root-password}\" | base64 -d)\n\n# PowerShell\n# get root-user\nkubectl get secret --namespace minio minio -o json | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty 'root-user' | ForEach-Object { [System.Text.Encoding]::Utf8.GetString([System.Convert]::FromBase64String($_)) }\n# admin\n\n# get root-password\nkubectl get secret --namespace minio minio -o json | ConvertFrom-Json | Select-Object -ExpandProperty data | Select-Object -ExpandProperty 'root-password' | ForEach-Object { [System.Text.Encoding]::Utf8.GetString([System.Convert]::FromBase64String($_)) }\n# WtgBylRJcJ\n</code></pre>"},{"location":"helmchart/9-install-minio/#step-7-access-minio-locally","title":"Step 7: Access Minio Locally","text":"<p>To connect to your MinIO server using a client, you need to create a port forward:</p> <pre><code>kubectl port-forward svc/minio 9001:9001 --namespace minio\n</code></pre> <p>To access the MinIO web UI:</p> <pre><code>http://localhost:9001/login\n# or\nhttp://127.0.0.1:9001/login\n</code></pre> <p>Now, you can access Minio by opening your web browser. Use the default username (admin) to log in.</p> <p>After running the above commands, you should be able to access Minio at <code>http://localhost:9001/login</code> in your web browser. Log in using the username admin and the password obtained from the previous command.</p> <p>Minio &gt; Login page</p> <p></p> <p>Minio &gt; Object Browser page</p> <p></p>"},{"location":"helmchart/9-install-minio/#step-7-configure-ingress-for-minio","title":"Step 7: Configure Ingress for Minio","text":"<p>If you want to access Minio externally, you can configure Ingress. First, create an Ingress resource:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: minio-ingress\nspec:\n  rules:\n  - host: minio.example.com  # Replace with your domain\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: nimio\n            port:\n              number: 9001\n</code></pre> <p>Apply the Ingress configuration:</p> <pre><code>kubectl apply -f minio-ingress.yaml\n</code></pre>"},{"location":"helmchart/9-install-minio/#step-8-uninstalling-the-chart","title":"Step 8: Uninstalling the Chart","text":"<p>Once you're done experimenting, you can delete the Minio deployment and associated resources from AMK:</p> <p>To uninstall/delete the minio helm deployment run:</p> <p><pre><code>helm list --namespace minio\nhelm delete minio -n minio\nkubectl delete namespace minio\n</code></pre> The command removes all the Kubernetes components associated with the chart and deletes the release.</p>"},{"location":"helmchart/9-install-minio/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we learned how to deploy Minio on Azure Kubernetes Services using Helm charts. Minio provides a scalable and high-performance object storage solution that can be easily integrated into Kubernetes environments.</p>"},{"location":"helmchart/9-install-minio/#reference","title":"Reference","text":"<ul> <li>Bitnami Object Storage based on MinIO(R)</li> <li>MinIO for Microsoft Azure Kubernetes Service</li> <li>min.io Azure Gateway</li> <li>min.io Azure Gateway</li> </ul>"},{"location":"kubernetes/0.getting-started/","title":"Getting Started with Docker Container","text":"<p>Chapter-3: Prepare Azure Kubernetes Service (AKS) for Microservices</p>"},{"location":"kubernetes/0.getting-started/#getting-started-with-docker-kubernetes","title":"Getting Started with Docker &amp; Kubernetes","text":""},{"location":"kubernetes/0.getting-started/#overview","title":"Overview","text":"<p>This is our third chapter of the book, In this chapter you will learn basic concepts of Docker like image, container, Dockerfile, docker engine, etc.. then start labs on Prepare an application for AKS, deploying an application in AKS and Working with AKS cluster and more.</p>"},{"location":"kubernetes/0.getting-started/#basic-concepts-of-docker","title":"Basic Concepts of Docker","text":"<ul> <li> <p>Docker: is a platform that enables developers to build, package, and deploy applications inside containers. </p> </li> <li> <p>Image: is a read-only template that contains the instructions for creating a Docker container. Docker images can be created using a Dockerfile, Once an image is built, it can be stored in a Docker registry, which is a centralized repository for Docker images.</p> </li> <li> <p>Containers:  is a runtime instance of a Docker image. Containers are lightweight, standalone executable packages that contain everything needed to run an application, including code, libraries, dependencies, and configuration files. </p> </li> <li> <p>Dockerfile: A text file that contains the instructions for building a Docker image. A Dockerfile specifies the base image, the software dependencies, and the application code for a Docker container.</p> </li> <li> <p>Docker Engine: The core component of Docker that runs on the host operating system and manages the lifecycle of Docker containers.</p> </li> <li> <p>Registry: A repository for storing and sharing Docker images. Docker Hub is the default public registry for Docker images, but you can also set up your own private registry.</p> </li> <li> <p>Docker Compose:  allows you to define and run multi-container Docker applications. It uses a YAML file to define the services, networks, and volumes needed for the application to run. With Docker Compose, you can start, stop, and manage multiple containers with a single command.</p> </li> </ul> <p>With Docker, developers can create and test applications in a local environment, then package them into containers that can be easily deployed to production servers or cloud-based environments.</p>"},{"location":"kubernetes/0.getting-started/#docker-and-kubernetes-relationship","title":"Docker and Kubernetes Relationship","text":"<p>Docker and Kubernetes are two distinct technologies that are often used together to deploy and manage containerized applications.</p> <p><code>Docker</code> is a containerization technology that allows developers to package their applications and dependencies into portable and self-contained containers. These containers can be run consistently across different environments, which makes it easier to develop, test, and deploy applications.</p> <p><code>Kubernetes</code> on the other hand, is a container orchestration platform that automates the deployment, scaling, and management of containerized applications. Kubernetes provides a framework for managing multiple Docker containers across a cluster of servers, with features such as load balancing, auto-scaling, and self-healing.</p> <p>In other words, Docker provides a way to package and distribute applications as containers, while Kubernetes provides a way to manage and orchestrate those containers in a scalable and efficient manner.</p>"},{"location":"kubernetes/1-prepare-app/","title":"Prepare an application for Azure Kubernetes Service (AKS)","text":""},{"location":"kubernetes/1-prepare-app/#introduction","title":"Introduction","text":"<p>Getting your application ready to run on Azure Kubernetes Service (AKS) involves a series of steps to ensure a smooth deployment. Here's an overview of the process:</p> <p>1. Create a new .NET Core Web API project: Start by creating a new .NET Core Web API project. Think of it as a foundation for your app.</p> <p>2. Containerization of your application: Start by containerizing your application. Containerization allows your application to run consistently across different environments.</p> <p>3. Pushing the containerized application to Azure Container Registry (ACR): To make your container image accessible to AKS, you need to store it in a container registry like Azure Container Registry (ACR).</p>"},{"location":"kubernetes/1-prepare-app/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Create a new .NET Core Web API project</li> <li>Step-2: Test the new .NET core Web API project</li> <li>Step-3: Add Dockerfiles to the API project</li> <li>Step-4: Build &amp; Test docker container locally</li> <li>Step-5: Publish docker container to ACR</li> <li>Step-6: Pull docker container from ACR</li> </ul>"},{"location":"kubernetes/1-prepare-app/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install the .NET Core SDK - https://dotnet.microsoft.com/en-us/download</li> <li>Create an Azure DevOps Repository</li> <li>Clone the Repository</li> <li>Azure Container Registry</li> <li>Docker desktop installed - https://www.docker.com/get-started/</li> </ul>"},{"location":"kubernetes/1-prepare-app/#implementation-details","title":"Implementation details","text":"<p>Here is the step by step implementation details of getting application ready to run on Azure Kubernetes Service (AKS)</p>"},{"location":"kubernetes/1-prepare-app/#step-1-create-a-new-net-core-web-api-project","title":"Step-1: Create a new .NET Core Web API project","text":"<p>To create a new .NET Core Web API project, you will need to have the .NET Core SDK installed on your machine. You can download the .NET Core SDK from the .NET website https://dotnet.microsoft.com/download.</p> <p>Once you have the .NET Core SDK installed, follow these steps to create a new .NET Core Web API project:</p> <ol> <li>Open a terminal window and navigate to the directory where you want to create your project.</li> <li>Run the <code>dotnet new</code> command to create a new .NET Core Web API project: Let's take a look some useful <code>dotnet</code> command before creating the project. Use this command to get the <code>dotnet</code> commands help so that your get idea on how use these commands better.  <pre><code>dotnet --help\n</code></pre> Use this command to get list of available <code>dotnet</code> project templates <pre><code>dotnet new --list\n</code></pre> output</li> </ol> <p><pre><code>These templates matched your input: \n\nTemplate Name                                 Short Name           Language    Tags\n--------------------------------------------  -------------------  ----------  -------------------------------------\nASP.NET Core Empty                            web                  [C#],F#     Web/Empty\nASP.NET Core gRPC Service                     grpc                 [C#]        Web/gRPC\nASP.NET Core Web API                          webapi               [C#],F#     Web/WebAPI\nASP.NET Core Web App                          razor,webapp         [C#]        Web/MVC/Razor Pages\nASP.NET Core Web App (Model-View-Controller)  mvc                  [C#],F#     Web/MVC\nASP.NET Core with Angular                     angular              [C#]        Web/MVC/SPA\nASP.NET Core with React.js                    react                [C#]        Web/MVC/SPA\nASP.NET Core with React.js and Redux          reactredux           [C#]        Web/MVC/SPA\nBlazor Server App                             blazorserver         [C#]        Web/Blazor\nBlazor WebAssembly App                        blazorwasm           [C#]        Web/Blazor/WebAssembly/PWA\nClass Library                                 classlib             [C#],F#,VB  Common/Library\nConsole App                                   console              [C#],F#,VB  Common/Console\n.\n.\nand more....\n</code></pre> Use this command to actually create new project <pre><code>dotnet new webapi -o aspnetapi\n\nor \ndotnet new webapi -o aspnetapi --no-https -f net7.0\n\ncd aspnetapi\n\ncode . \n\nor \ncode -r ../aspnetapi\n</code></pre> Output <pre><code>C:\\WINDOWS\\system32&gt;cd C:\\Source\\Repos\n\nC:\\Source\\Repos&gt;dotnet new webapi -o aspnetapi\nThe template \"ASP.NET Core Web API\" was created successfully.\n\nProcessing post-creation actions...\nRunning 'dotnet restore' on C:\\Source\\Repos\\aspnetapi\\aspnetapi.csproj...\n  Determining projects to restore...\n  Restored C:\\Source\\Repos\\aspnetapi\\aspnetapi.csproj (in 247 ms).\nRestore succeeded.\n\nC:\\Source\\Repos&gt;cd aspnetapi\n\nC:\\Source\\Repos\\aspnetapi&gt;code .\n</code></pre> 3. Here is the example of adding packages to .net projects. <pre><code>dotnet add package Microsoft.EntityFrameworkCore.InMemory\n</code></pre> 4. Run the following command to restore the project's dependencies: <pre><code>dotnet restore\n</code></pre></p>"},{"location":"kubernetes/1-prepare-app/#step-2-test-the-new-net-core-web-api-project","title":"Step-2: Test the new .NET core Web API project","text":"<ol> <li>Run the following command to build the project:  <code>dotnet build</code> command will look for the project or solution file in the current directory and compile the code in it. It will also restore any dependencies required by the project and create the output files in the bin directory. <pre><code>dotnet build\n</code></pre> output <pre><code>Microsoft (R) Build Engine version 17.0.1+b177f8fa7 for .NET\nCopyright (C) Microsoft Corporation. All rights reserved.\n\n  Determining projects to restore...\n  All projects are up-to-date for restore.\n  AspNetApi -&gt; C:\\Source\\Repos\\AspNetApi\\aspnet-api\\bin\\Debug\\net6.0\\AspNetApi.dll\n\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n\nTime Elapsed 00:00:01.51\n</code></pre></li> <li>Run the following command to start the development server:  <code>dotnet run</code> command will look for the project or solution file in the current directory and compile the code in it. After compiling, it will run the application and any output will be displayed in the console. <pre><code>dotnet run\n</code></pre> output <pre><code>Building...\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: https://localhost:7136\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: http://localhost:5136\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Development\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: C:\\Source\\Repos\\AspNetApi\\aspnet-api\\\n</code></pre> You will notice the URL in the output, copy the URL and paste it in your favorite browser. you will get a <code>404 error.</code> don\u2019t worry. Just type swagger at the end of the URL and press enter and you will get the following webpage.</li> </ol> <ul> <li> <p>https://localhost:7136</p> </li> <li> <p>https://localhost:7136/swagger/index.html - Swagger URL</p> </li> <li> <p>https://localhost:7136/api/aspnetapi/v1/weatherforecast - API endpoint URL</p> </li> </ul> <p>If you are able to see this swagger URL in your browser then everything is created and setup as expected.</p> <p></p> <p>Use the following command to stop the application in VS Code <pre><code>ctrl + c\n</code></pre> It is time to push your basic project template source into Azure DevOps Git repo.</p> <p>Use these git commands to push the source code.</p> <pre><code>git add .\ngit commit -am \"My fist commit - Create Web API project\"\ngit push\n</code></pre>"},{"location":"kubernetes/1-prepare-app/#step-3-add-dockerfiles-to-the-api-project","title":"Step-3: Add Dockerfiles to the API project","text":"<p>Dockerfiles will be added to the project, which provide instructions for building a container image of our Web API application.</p> <p>There are multiple way to create <code>Dockerfile</code> depending on your code editor.  Here are the step-by-step instructions for creating a <code>Dockerfile</code> in a .NET Core Web API project:</p> <ol> <li>First, open your .NET Core Web API project in Visual Studio code or your favorite code editor.</li> <li>Next, create a new file in the root directory of your project and name it Dockerfile (with no file extension).</li> <li> <p>Open the Dockerfile and add the following code to the file: <pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"AspNetApi.csproj\", \".\"]\nRUN dotnet restore \"./AspNetApi.csproj\"\nCOPY . .\nWORKDIR \"/src/.\"\nRUN dotnet build \"AspNetApi.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"AspNetApi.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"AspNetApi.dll\"]\n</code></pre> This code defines a Docker image that is based on the aspnet:6.0 image from Microsoft's container registry. The image is divided into four stages:</p> </li> <li> <p><code>base:</code> sets up the working directory and exposes port 80.</p> </li> <li><code>build:</code> restores the project dependencies, builds the project in Release mode, and copies the build output to the /app/build directory.</li> <li><code>publish:</code> publishes the project in Release mode and copies the published output to the /app/publish directory.</li> <li><code>final:</code> sets the working directory to <code>/app</code> and copies the published output from the <code>publish</code> stage to the current directory. It also specifies the entry point for the container, which is the <code>dotnet</code> command with the name of your project's DLL file.</li> </ol>"},{"location":"kubernetes/1-prepare-app/#step-4-build-test-docker-container-locally","title":"Step-4: Build &amp; Test docker container locally","text":"<p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application functions as expected.</p> <p><code>docker build</code> is a command that allows you to build a Docker image from a Dockerfile. The Dockerfile is a text file that contains instructions for Docker to build the image, including the base image to use, the files to include, the commands to run, and the ports to expose.</p> <p>To build and publish a container image for a .NET Core Web API project, you will need to have Docker installed on your machine. You can download Docker from the Docker website https://www.docker.com/get-started.</p> <p>Once you have Docker installed, follow these steps to build and publish a container for your .NET Core Web API project:</p> <ol> <li>Open a terminal window and navigate to the root of the project.</li> <li>Run the <code>docker build</code> command to build the Docker image: <pre><code>docker build -t sample/aspnet-api:20230226.1 .\n</code></pre> output <pre><code>[+] Building 9.5s (19/19) FINISHED\n =&gt; [internal] load build definition from Dockerfile                                                                                                          0.0s \n =&gt; =&gt; transferring dockerfile: 878B                                                                                                                          0.0s \n =&gt; [internal] load .dockerignore                                                                                                                             0.0s \n =&gt; =&gt; transferring context: 374B  \n..            \n..\n..\n\n =&gt; =&gt; naming to docker.io/sample/aspnet-api:20230226.1                                                                                                             \n</code></pre> Verify the new image</li> </ol> <p>if you open the docker desktop you should be able to see the newly created image there.  3. Run the <code>docker run</code> command to start a container based on the image: <pre><code>docker run --rm -p 8080:80 sample/aspnet-api:20230226.1\n</code></pre> output <pre><code>info: Microsoft.Hosting.Lifetime[14]  \n      Now listening on: http://[::]:80\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Production\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: /app/\n</code></pre> Wait for the container to start. You should see output in the terminal indicating that the container is listening on port 80. Open the docker desktop to see the newly created container in the docker desktop app </p> <p>Open a web browser and navigate to http://localhost:8080/api/values (or whatever URL corresponds to your Web API endpoint) to confirm that the Web API is running inside the Docker container.</p> <p>use these links for testing when you run docker command from vs code</p> <ul> <li>http://localhost:8080/swagger/index.html</li> <li>http://localhost:8080/api/aspnetapi/v1/heartbeat/ping</li> <li>http://localhost:8080/api/aspnetapi/v1/weatherforecast</li> </ul> <p></p> <p>Best-practice</p> <p>When working with Docker containers, it is recommended to follow a consistent naming convention to ensure clarity and organization. The following pattern is suggested for naming Docker containers:</p> <pre><code>docker build -t projectname/domainname/appname:yyyymmdd.sequence .\n\nexample:\ndocker build -t project1/sample/aspnet-api:20230226 .\n</code></pre> <p>You've successfully created a Dockerfile and built a Docker image for your .NET Core Web API project. You can now distribute the Docker image to other machines or deploy it to a cloud service like Azure or AWS.</p>"},{"location":"kubernetes/1-prepare-app/#step-5-publish-docker-container-to-acr","title":"Step-5: Publish docker container to ACR","text":"<p>Finally, we will publish the built Docker container to the Azure Container Registry (ACR), making it accessible for deployment and distribution.</p> <p>Now we've Docker Containers ready for push to Container Registry so that we can use them in future labs.</p> <p>To publish a Docker container image to Azure Container Registry (ACR), you will need to have the following:</p> <ol> <li>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI. As part of the Chapter-2 we will create this azure resource, you can come back to this steps after ACR is created.</li> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre></li> <li><code>Tag</code> your Docker container with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command: <pre><code>docker tag sample/aspnet-api:20230226.1 acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> Use this command to see a list of your current local images <pre><code>docker images\n</code></pre> output <pre><code>REPOSITORY                             TAG          IMAGE ID       CREATED         SIZE\nacr1dev.azurecr.io/sample/aspnet-api   20230226.1   1bab8ba123ca   2 hours ago     213MB\n</code></pre></li> <li>Push your Docker container to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker push acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> output <pre><code>The push refers to repository [acr1dev.azurecr.io/sample/aspnet-api]\na592c2e20b23: Pushed\n5f70bf18a086: Layer already exists\nd57ad0aaee3b: Layer already exists\naff5d88d936a: Layer already exists\nb3b2bd456a19: Layer already exists\n2540ef4bc011: Layer already exists\n94100d1041b6: Layer already exists\nbd2fe8b74db6: Layer already exists\n20230226.1: digest: sha256:026ec79d24fca0f30bcd90c7fa17e82a2347cf7bc5ac5d762a630277086ed0d1 size: 1995\n</code></pre></li> <li>Wait for the push to complete. Depending on the size of your Docker container and the speed of your internet connection, this may take a few minutes.</li> <li>Verify the newly pushed image to ACR. <pre><code># List images in registry\naz acr repository list --name acr1dev --output table\n</code></pre> output <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\n</code></pre></li> <li>Show the new tags of a image in the acr <pre><code>az acr repository show-tags --name acr1dev --repository sample/aspnet-api --output table\n</code></pre> output <pre><code>Result\n----------\n20230220.1\n20230226.1\n</code></pre></li> </ol> <p>You've successfully pushed your Docker container to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"kubernetes/1-prepare-app/#step-6-pull-docker-container-from-acr","title":"Step-6: Pull docker container from ACR","text":"<p>Pull docker container from ACR is something may be helpful during container troubleshooting.</p> <p>To pull a Docker container from Azure Container Registry (ACR), you need to perform the following steps:</p> <ol> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># Log in to the container registry\naz acr login --name acr1dev\n</code></pre></li> <li>Pull your Docker container from your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker pull acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> output <pre><code>20230226.1: Pulling from sample/aspnet-api\n01b5b2efb836: Already exists\nc4c81489d24d: Already exists\n95b82a084bc9: Already exists\nbb369c4b0f26: Already exists \nc888ac593815: Already exists\n14ce87409b2e: Already exists\n4f4fb700ef54: Already exists\nd15d1be868b7: Already exists\nDigest: sha256:026ec79d24fca0f30bcd90c7fa17e82a2347cf7bc5ac5d762a630277086ed0d1\nStatus: Downloaded newer image for acr1dev.azurecr.io/sample/aspnet-api:20230226.1\nacr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre></li> <li>Wait for the pull to complete. Depending on the size of your Docker container and the speed of your internet connection, this may take a few minutes. </li> <li>Verify the recently pulled container from ACR to make sure it running as expected <pre><code>docker run --rm -p 8080:80 acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> Test the container running following URL</li> </ol> <p></p> <p>http://localhost:8080/swagger/index.html</p> <p>You've successfully pulled your Docker container from Azure Container Registry. You can now use the Docker command-line interface to manage your container and run them locally or deploy them to other environments.</p>"},{"location":"kubernetes/2-deploy-api/","title":"Deploying an .NET Core API to Azure Kubernetes Service (AKS)","text":"<p>To deploying an application to Azure Kubernetes Service (AKS) involves several steps. Here's an overview of the process:</p> <ol> <li> <p>Create an AKS cluster: The first step is to create an AKS cluster on Azure if you haven't already. This can be done through the Azure portal or using Azure CLI or terraform. Create Azure Kubernetes Service (AKS) using terraform</p> </li> <li> <p>Prepare an application for AKS deployment: Next step is to prepare an application for AKS deployment, here you will create ACR and push the Docker container image to  Azure Container Registry (ACR). Prepare an application for Azure Kubernetes Service (AKS)</p> </li> <li> <p>Create a Kubernetes deployment manifest: After the Docker image is available in the registry, you will need to create a Kubernetes deployment file that specifies the image, the number of replicas to run, and other settings. You can use the \"kubectl create\" command to create the deployment.</p> </li> <li> <p>Create a Kubernetes service manifest: You will also need to create a Kubernetes service file that exposes your application to the cluster. A Kubernetes service manifest is a YAML file that describes how to expose your application to the outside world. The service manifest should include information such as the port that your application is listening on, and how the service should route traffic to your application.</p> </li> <li> <p>Apply the manifests to your AKS cluster: Use the <code>kubectl</code> command-line tool to apply the deployment and service manifests to your AKS cluster.</p> </li> <li> <p>Expose the service externally: By default, the Kubernetes service will only be accessible within the AKS cluster. To expose the service to the outside world, you'll need to create an Azure Load Balancer, which can be done through the Azure portal.</p> </li> <li> <p>Verify that the application is running: Once the Load Balancer is created, you should be able to access your application by navigating to the public IP address of the Load Balancer in a web browser.</p> </li> </ol> <p>By following these steps, you can deploy your application to Azure Kubernetes Service (AKS) and take advantage of the scalability and resiliency of the platform. You can also use Azure DevOps or other tools to automate the deployment process and simplify ongoing management of your application on AKS.</p>"},{"location":"kubernetes/2-deploy-api/#prerequisites","title":"Prerequisites","text":"<ul> <li>AKS cluster</li> <li>Azure Container Registry (ACR)</li> <li>Container Image</li> <li>Visual Studio Code</li> </ul>"},{"location":"kubernetes/2-deploy-api/#implementation-details","title":"Implementation details","text":"<p>Open the microservices project folder in Visual Studio code and creating new files for deploying application in Azure Kubernetes services (AKS)</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"kubernetes/2-deploy-api/#step-1-create-a-new-namespace","title":"Step-1: Create a new namespace","text":"<p>We are going to deploy our application in separate namespace in AKS instead of in <code>default</code> namespace. use this command to create new namespace in Kubernetes cluster. Let's name our namespace as <code>sample</code></p> <pre><code>kubectl create namespace sample \n</code></pre>"},{"location":"kubernetes/2-deploy-api/#step-1-create-a-kubernetes-deployment-manifest","title":"Step-1: Create a Kubernetes deployment manifest","text":"<p>Deployment manifest is a YAML file that describes the desired state of the Kubernetes objects that make up a deployment in Azure Kubernetes Service (AKS). The AKS deployment manifest includes information such as the container image to use, the number of replicas to create, the ports to expose, and any other required configuration options. </p> <p>Here is the deployment YAML file for deploying our first Microservice <code>aspnet-api</code>. we will name this file as <code>deployment.yaml</code> and store it in <code>aspnet-api</code> folder.</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n\n# kubectl apply -f deployment.yaml -n sample\n</code></pre>"},{"location":"kubernetes/2-deploy-api/#step-2-create-a-kubernetes-service-manifest","title":"Step-2: Create a Kubernetes service manifest","text":"<p>A Kubernetes service manifest is a YAML file that defines a Kubernetes service object, which provides a stable IP address and DNS name for accessing a set of pods in a Kubernetes cluster. The service manifest includes information such as the name of the service, the type of service (e.g., ClusterIP, NodePort, LoadBalancer), and the selector that determines which pods the service will forward traffic to.</p> <p>This manifest creates a service named <code>aspnet-api</code> that selects pods with the label app: <code>aspnet-api</code> and exposes them on port 80 using the ClusterIP service type. The service forwards incoming traffic to the pods on port 80.</p> <p>Here is the service YAML file for our first Microservice <code>aspnet-api</code>. we will name this file as <code>service.yaml</code> and store it in <code>aspnet-api</code> folder.</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> <p>There are 3 service types in Kubernetes:</p> <ul> <li><code>NodePort:</code> Exposes the service on a static port on each node in the cluster, allowing external traffic to access the service via any node's IP address and the specified static port.</li> <li><code>LoadBalancer:</code> Creates a cloud provider load balancer that distributes incoming traffic to the service across multiple nodes in the cluster.</li> <li><code>ExternalName:</code> Maps the service to the contents of the externalName field (e.g., a DNS name) instead of selecting pods.</li> </ul>"},{"location":"kubernetes/2-deploy-api/#step-3-apply-the-manifests-to-your-aks-cluster","title":"Step-3: Apply the manifests to your AKS cluster","text":"<p>Use these commands to apply these files in AKS.</p> <p><pre><code>kubectl apply -f deployment.yaml -n sample\n# deployment.apps/aspnet-api created\n\nkubectl apply -f service.yaml -n sample\n# service/aspnet-api created\n</code></pre> Now let's check the status of our deployment and service files in AKS by running following commands.</p> <p>get pods <pre><code>kubectl get pods -n sample\n</code></pre> output <pre><code>NAME                          READY   STATUS    RESTARTS   AGE\naspnet-api-6b7d8fbf9f-mqtnd   1/1     Running   0          20m\n</code></pre> get services <pre><code>kubectl get svc -n sample \n</code></pre> output <pre><code>NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\naspnet-api   ClusterIP   10.25.74.8   &lt;none&gt;        80/TCP    31m\n</code></pre></p> <p>Note</p> <p>EXTERNAL-IP is blank here, we'll have to update this to <code>LoadBalancer</code> type for testing.</p>"},{"location":"kubernetes/2-deploy-api/#step-4-port-forwarding","title":"Step-4: Port forwarding","text":"<p>Kubernetes port forwarding is a feature that allows you to access a Kubernetes service running inside a cluster from outside the cluster. It works by forwarding traffic from a local port on your machine to a port on a pod in the cluster.</p> <p><pre><code>kubectl port-forward svc/aspnet-api 8080:80 -n sample\n</code></pre> output <pre><code>Forwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\nHandling connection for 8080\nHandling connection for 8080\n</code></pre></p> <p>This command forwards port 8080 on your local machine to port 80 on the \"aspneet-api\" service in the AKS cluster.</p> <p>With the port forwarding established, you can now access the service using a web browser. In this example, you would access the service by opening a web browser and entering the following URL:</p> <p>http://localhost:8080/swagger/index.html</p> <p></p> <p>Port forwarding can be useful for testing and debugging applications running inside a Kubernetes cluster, allowing you to interact with them as if they were running locally on your machine. </p>"},{"location":"kubernetes/2-deploy-api/#step-5-expose-the-service-externally","title":"Step-5: Expose the service externally","text":"<p>To expose a Kubernetes service externally in Azure Kubernetes Service (AKS), you can use the <code>LoadBalancer</code> service type to create a public IP address and a load balancer in front of the service.</p> <p>Here's an example AKS service manifest that exposes a service externally: <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> In this example, the type field is set to <code>LoadBalancer</code>, which tells AKS to create a public IP address and a load balancer in front of the service. </p> <p>run the <code>kubectl apply</code> again to create the service.</p> <p><pre><code>kubectl apply -f service.yaml -n sample\n# service/aspnet-api configured\n</code></pre> Once the service is created, you can use the <code>kubectl get services</code> command to retrieve the external IP address assigned to the service:</p> <p><pre><code>kubectl get services aspnet-api -n sample\n</code></pre> The output should include the external IP address: <pre><code>NAME         TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE  \naspnet-api   LoadBalancer   10.25.74.8   20.246.168.64   80:30363/TCP   7h21m\n</code></pre></p> <p>Note</p> <p>EXTERNAL-IP is updated here with new public IP address, now you should be able to access your service externally.</p>"},{"location":"kubernetes/2-deploy-api/#step-6-verify-that-the-application-is-running","title":"Step-6: Verify that the application is running","text":"<p>You can now use the external IP address to access the service from outside the cluster.</p> <p>http://20.246.168.64/swagger/index.html</p> <p></p> <p>That's it! Your application is now running in a container in your AKS cluster, and is accessible from the public internet.</p>"},{"location":"kubernetes/2-deploy-api/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/aks/tutorial-kubernetes-deploy-application?tabs=azure-cli</li> </ul>"},{"location":"kubernetes/2-deploy-app/","title":"Deploying an ASP.NET Core MVC to Azure Kubernetes Service (AKS)","text":""},{"location":"kubernetes/2-deploy-app/#introduction","title":"Introduction","text":"<p>These step-by-step instructions guide you through the process of deploying an ASP.NET Core MVC application in Azure Kubernetes Service (AKS). In this tutorial, you will learn how to create Kubernetes YAML manifest files essential for application deployment, and you'll deploy the application using the <code>kubectl</code> tool. Additionally, we'll ensure the application is accessible over the internet by the end of the process.</p>"},{"location":"kubernetes/2-deploy-app/#prerequisites","title":"Prerequisites:","text":"<p>Before you begin, make sure you have the following prerequisites:</p> <ol> <li> <p>Azure Account: You need an active Azure account.</p> </li> <li> <p>Azure CLI: Install the Azure Command-Line Interface (CLI) on your local machine.</p> </li> <li> <p>Kubectl: Install <code>kubectl</code>, the Kubernetes command-line tool, on your local machine.</p> </li> <li> <p>Docker: You'll need Docker installed to build container images for your application.</p> </li> <li>Azure Container Registry (ACR) Create Azure Container Registry (ACR) using terraform</li> <li>Azure Kubernetes Service (AKS) Create Azure Kubernetes Service (AKS) using terraform</li> </ol>"},{"location":"kubernetes/2-deploy-app/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Create Your ASP.NET Core MVC Application</li> <li>Step 2: Dockerize Your Application</li> <li>Step 3: Push the Docker Image to Azure Container Registry (ACR)</li> <li>Step 4: Create Kubernetes YAML Manifests</li> <li>Step 5: Apply the manifests to your AKS cluster</li> <li>Step-6: Port forwarding</li> <li>Step-6: Expose the service externally</li> <li>Step-7: Verify that the application is running</li> </ul>"},{"location":"kubernetes/2-deploy-app/#implementation-details","title":"Implementation details","text":"<p>Below are the step-by-step implementation details. Before you begin this lab, please ensure that you are already connected to your Azure subscription and have access to your AKS cluster.</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"kubernetes/2-deploy-app/#step-1-create-your-aspnet-core-mvc-application","title":"Step 1: Create Your ASP.NET Core MVC Application","text":"<p>If you haven't already, create an ASP.NET Core MVC application. You can use Visual Studio code. Ensure your application runs locally before containerizing it.</p> <p>Check this for more information - Create your first website using .NET Core MVC</p>"},{"location":"kubernetes/2-deploy-app/#step-2-dockerize-your-application","title":"Step 2: Dockerize Your Application","text":"<ul> <li>Create a Dockerfile in your application's root directory to containerize your ASP.NET Core application.</li> <li>Build the Docker image:</li> <li>Test the image locally:</li> </ul> <p>Check this for more information - Create your first website using .NET Core MVC</p>"},{"location":"kubernetes/2-deploy-app/#step-3-push-the-docker-image-to-azure-container-registry-acr","title":"Step 3: Push the Docker Image to Azure Container Registry (ACR)","text":"<p>To deploy your image to AKS, you need to store it in a container registry like Azure Container Registry (ACR). </p> <ol> <li>Create an ACR in your Azure subscription using the Azure Portal or Azure CLI.</li> <li>Push your Docker image to the ACR:</li> </ol> <p>Check this for more information - Create your first website using .NET Core MVC</p>"},{"location":"kubernetes/2-deploy-app/#step-4-create-kubernetes-yaml-manifests","title":"Step 4: Create Kubernetes YAML Manifests","text":"<p>Now, create YAML manifests for your ASP.NET Core MVC application. You'll typically need two files: one for the Deployment and another for the Service.</p> <p>Create a new namespace</p> <p>We are going to deploy our application in separate namespace in AKS instead of in <code>default</code> namespace. use this command to create new namespace in Kubernetes cluster. Let's name our namespace as <code>sample</code></p> <pre><code>kubectl create namespace sample\n</code></pre> <p>Deployment YAML (deployment.yaml)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-app\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-app\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-app\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-app\n          image: acr1dev.azurecr.io/sample/aspnet-app:20230312.1\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n\n# kubectl apply -f deployment.yaml -n sample\n</code></pre> <p>Service YAML (service.yaml)</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-app\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP #LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-app\n\n# kubectl apply -f service.yaml -n sample\n</code></pre>"},{"location":"kubernetes/2-deploy-app/#step-5-apply-the-manifests-to-your-aks-cluster","title":"Step 5: Apply the manifests to your AKS cluster","text":"<p>Apply the YAML manifests to deploy your ASP.NET Core MVC application:</p> <p><pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> Verify the kubernetes objects deployment</p> <pre><code>kubectl get all -n sample \n\n# output\nNAME                                      READY   STATUS    RESTARTS   AGE\npod/aspnet-api-79b4cbf4bb-5dljg           1/1     Running   0          20h\npod/aspnet-app-8654797b89-99xdq           1/1     Running   0          50m\n\nNAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/aspnet-api           ClusterIP   10.25.69.201    &lt;none&gt;        80/TCP    178d\nservice/aspnet-app           ClusterIP   10.25.243.72    &lt;none&gt;        80/TCP    50m\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/aspnet-api           1/1     1            1           12d\ndeployment.apps/aspnet-app           1/1     1            1           50m\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/aspnet-api-79b4cbf4bb           1         1         1       12d\nreplicaset.apps/aspnet-app-8654797b89           1         1         1       50m\n</code></pre>"},{"location":"kubernetes/2-deploy-app/#step-6-port-forwarding","title":"Step-6: Port forwarding","text":"<p>Kubernetes port forwarding is a feature that allows you to access a Kubernetes service running inside a cluster from outside the cluster. It works by forwarding traffic from a local port on your machine to a port on a pod in the cluster.</p> <p>Once the deployment is complete, you can access your ASP.NET Core MVC application via port-forwarding. </p> <p><pre><code>kubectl port-forward svc/aspnet-app 8080:80 -n sample\n\n#output\nForwarding from 127.0.0.1:8080 -&gt; 80\nForwarding from [::1]:8080 -&gt; 80\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\n</code></pre> </p>"},{"location":"kubernetes/2-deploy-app/#step-6-expose-the-service-externally","title":"Step-6: Expose the service externally","text":"<p>Once the deployment is complete, you can access your ASP.NET Core MVC application via the <code>LoadBalancer</code> external IP address. You can find the IP address using the following command:</p> <p>Here's an example AKS service manifest that exposes a service externally:</p> <p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-app\n  namespace: sample\n  labels: {}\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-app\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> In this example, the type field is set to <code>LoadBalancer</code>, which tells AKS to create a public IP address and a load balancer in front of the service. </p> <p>run the <code>kubectl apply</code> again to create the service.</p> <p><pre><code>kubectl apply -f service.yaml -n sample\n# service/aspnet-api configured\n</code></pre> Once the service is created, you can use the <code>kubectl get services</code> command to retrieve the external IP address assigned to the service:</p> <pre><code>kubectl get services aspnet-app -n sample\n\n# The output should include the external IP address:\nNAME         TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)        AGE\naspnet-app   LoadBalancer   10.25.243.72   20.85.190.92   80:30227/TCP   71m\n</code></pre> <p>Note</p> <p>EXTERNAL-IP is updated here with new public IP address, now you should be able to access your service externally.</p>"},{"location":"kubernetes/2-deploy-app/#step-7-verify-that-the-application-is-running","title":"Step-7: Verify that the application is running","text":"<p>You can now use the external IP address to access the ASP.NET Core MVC application from outside the cluster.</p> <p>http://20.85.190.92/</p> <p></p>"},{"location":"kubernetes/2-deploy-app/#conclusion","title":"Conclusion","text":"<p>Your ASP.NET Core MVC application should now be running on Azure Kubernetes Service (AKS).</p>"},{"location":"kubernetes/3-working-with-aks/","title":"Working with AKS cluster using kubectl","text":"<p>Now it is time to learn some basic commands of <code>kubectl</code> to interacting with our AKS cluster so that performing actions in  next labs will easer.</p>"},{"location":"kubernetes/3-working-with-aks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Download &amp; Install Azure CLI - https://learn.microsoft.com/en-us/cli/azure/install-azure-cli</li> <li>Existing AKS cluster</li> <li>Visual studio code</li> <li>kubectl</li> </ul>"},{"location":"kubernetes/3-working-with-aks/#install-kubectl","title":"Install kubectl","text":"<p>Here are the steps to use the <code>kubectl</code> command to manage an AKS cluster:</p> <p>First, you'll need to install kubectl on your local machine. You can download the latest version of kubectl from the Kubernetes official website. https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/</p> <p>Windows OS users.</p> <pre><code>choco install kubernetes-cli\n</code></pre> <p>Mac OS users.</p> <p>look into this for more info - Install and Set Up kubectl on macOS</p> <pre><code>brew install kubectl\n\n# verify the installation\nkubectl version\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#login-to-azure-set-subscription","title":"Login to azure &amp; set subscription","text":"<pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#connect-to-aks-cluster","title":"Connect to AKS cluster","text":"<p>Before using <code>kubectl</code> commands, make sure that you connect to Kubernetes cluster first using following commands.</p> <pre><code># azure CLI\naz aks get-credentials --resource-group &lt;resource-group-name&gt; --name &lt;aks-cluster-name&gt;\n\n# Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#test-workloads-in-aks","title":"Test workloads in AKS","text":"<p>Once you've connected to the cluster successfully then try running following commands to see everything is working as expected in your cluster.</p> <pre><code>kubectl get all --namespace &lt;namespace-name&gt;\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#cluster-info","title":"Cluster-info","text":"<p>Listing and inspecting your cluster...helpful for knowing which cluster is your current context</p> <p><pre><code>kubectl cluster-info\n</code></pre> output</p> <pre><code>Kubernetes control plane is running at https://cluster1-dns-73957b84.hcp.eastus.azmk8s.io:443\nCoreDNS is running at https://cluster1-dns-73957b84.hcp.eastus.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nMetrics-server is running at https://cluster1-dns-73957b84.hcp.eastus.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#namespaces","title":"Namespaces","text":"<p><pre><code>kubectl get namespaces\n</code></pre> output</p> <pre><code>NAME                STATUS   AGE\ndefault             Active   6d4h\ngatekeeper-system   Active   6d4h\nkube-node-lease     Active   6d4h\nkube-public         Active   6d4h\nkube-system         Active   6d4h\nsample              Active   8h\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#nodes","title":"Nodes","text":"<pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                STATUS   ROLES   AGE     VERSION\naks-agentpool-25316841-vmss000000   Ready    agent   4h37m   v1.23.12\naks-agentpool-25316841-vmss000001   Ready    agent   4h37m   v1.23.12\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#pods","title":"Pods","text":"<pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE           NAME                                     READY   STATUS    RESTARTS   AGE\ngatekeeper-system   gatekeeper-audit-75cc4b59d7-7rm7m        1/1     Running   0          4h38m\ngatekeeper-system   gatekeeper-controller-78bdb459dd-2fqkm   1/1     Running   0          4h38m\ngatekeeper-system   gatekeeper-controller-78bdb459dd-9kcmt   1/1     Running   0          4h38m\nkube-system         aks-secrets-store-csi-driver-6fmxb       3/3     Running   0          4h38m\nkube-system         aks-secrets-store-csi-driver-nkwls       3/3     Running   0          4h38m\nkube-system         aks-secrets-store-provider-azure-glh4r   1/1     Running   0          4h38m\nkube-system         aks-secrets-store-provider-azure-v2vrl   1/1     Running   0          4h38m\nkube-system         ama-logs-gm6b6                           2/2     Running   0          4h38m\nkube-system         ama-logs-rs-76b7c448-l54gj               1/1     Running   0          4h38m\nkube-system         ama-logs-sld4q                           2/2     Running   0          4h38m\nkube-system         azure-ip-masq-agent-gjz65                1/1     Running   0          4h38m\nkube-system         azure-ip-masq-agent-lf45m                1/1     Running   0          4h38m\nkube-system         azure-npm-fktnn                          1/1     Running   0          4h37m\nkube-system         azure-npm-n4qmq                          1/1     Running   0          4h37m\nkube-system         azure-policy-699bff6d75-txw8w            1/1     Running   0          4h38m\nkube-system         azure-policy-webhook-854ffcbfdb-6q2gc    1/1     Running   0          4h38m\nkube-system         cloud-node-manager-7lr4k                 1/1     Running   0          4h38m\nkube-system         cloud-node-manager-rpt2q                 1/1     Running   0          4h38m\nkube-system         coredns-autoscaler-5589fb5654-dbpcz      1/1     Running   0          4h38m\nkube-system         coredns-b4854dd98-jrcf2                  1/1     Running   0          4h38m\nkube-system         coredns-b4854dd98-kr5f8                  1/1     Running   0          4h37m\nkube-system         csi-azuredisk-node-jj9xf                 3/3     Running   0          4h38m\nkube-system         csi-azuredisk-node-nwfls                 3/3     Running   0          4h38m\nkube-system         csi-azurefile-node-qrvls                 3/3     Running   0          4h38m\nkube-system         csi-azurefile-node-wzt2j                 3/3     Running   0          4h38m\nkube-system         konnectivity-agent-cd99df756-l6gqp       1/1     Running   0          4h12m\nkube-system         konnectivity-agent-cd99df756-pm4xs       1/1     Running   0          4h12m\nkube-system         kube-proxy-4kx9d                         1/1     Running   0          4h38m\nkube-system         kube-proxy-5dm98                         1/1     Running   0          4h38m\nkube-system         metrics-server-f77b4cd8-pmsk4            1/1     Running   0          4h38m\nkube-system         metrics-server-f77b4cd8-wsmll            1/1     Running   0          4h38m\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#deployments","title":"Deployments","text":"<p><pre><code>kubectl get deployments -A\n</code></pre> output</p> <pre><code>NAMESPACE           NAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ngatekeeper-system   gatekeeper-audit        1/1     1            1           4h39m\ngatekeeper-system   gatekeeper-controller   2/2     2            2           4h39m\nkube-system         ama-logs-rs             1/1     1            1           4h39m\nkube-system         azure-policy            1/1     1            1           4h39m\nkube-system         azure-policy-webhook    1/1     1            1           4h39m\nkube-system         coredns                 2/2     2            2           4h39m\nkube-system         coredns-autoscaler      1/1     1            1           4h39m\nkube-system         konnectivity-agent      2/2     2            2           4h39m\nkube-system         metrics-server          2/2     2            2           4h39m\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#services","title":"Services","text":"<p><pre><code>kubectl get svc -A\n</code></pre> output  <pre><code>NAMESPACE           NAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE\ndefault             kubernetes                     ClusterIP   10.25.0.1       &lt;none&gt;        443/TCP         4h41m\ngatekeeper-system   gatekeeper-webhook-service     ClusterIP   10.25.97.0      &lt;none&gt;        443/TCP         4h40m\nkube-system         azure-policy-webhook-service   ClusterIP   10.25.58.191    &lt;none&gt;        443/TCP         4h40m\nkube-system         kube-dns                       ClusterIP   10.25.0.10      &lt;none&gt;        53/UDP,53/TCP   4h40m\nkube-system         metrics-server                 ClusterIP   10.25.122.164   &lt;none&gt;        443/TCP         4h40m\nkube-system         npm-metrics-cluster-service    ClusterIP   10.25.231.109   &lt;none&gt;        9000/TCP        4h40m\n</code></pre></p>"},{"location":"kubernetes/3-working-with-aks/#configmaps","title":"ConfigMaps","text":"<pre><code>kubectl get configmaps -A\n</code></pre> <pre><code>NAMESPACE           NAME                                    DATA   AGE\ndefault             kube-root-ca.crt                        1      4h41m\ngatekeeper-system   kube-root-ca.crt                        1      4h41m\nkube-node-lease     kube-root-ca.crt                        1      4h41m\nkube-public         kube-root-ca.crt                        1      4h41m\nkube-system         ama-logs-rs-config                      1      4h41m\nkube-system         azure-ip-masq-agent-config-reconciled   1      4h41m\nkube-system         azure-npm-config                        1      4h41m\nkube-system         cluster-autoscaler-status               1      4h40m\nkube-system         container-azm-ms-aks-k8scluster         1      4h41m\nkube-system         coredns                                 1      4h41m\nkube-system         coredns-autoscaler                      1      4h40m\nkube-system         coredns-custom                          0      4h41m\nkube-system         extension-apiserver-authentication      6      4h41m\nkube-system         kube-root-ca.crt                        1      4h41m\nkube-system         overlay-upgrade-data                    4      4h41m\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#secrets","title":"Secrets","text":"<pre><code>kubectl get secrets -A\n</code></pre> <pre><code>NAMESPACE           NAME                                             TYPE                                  DATA   AGE\ndefault             default-token-h2svh                              kubernetes.io/service-account-token   3      4h41m\ngatekeeper-system   default-token-fjx97                              kubernetes.io/service-account-token   3      4h41m\ngatekeeper-system   gatekeeper-admin-token-jhhbk                     kubernetes.io/service-account-token   3      4h41m\ngatekeeper-system   gatekeeper-webhook-server-cert                   Opaque                                3      4h41m\nkube-node-lease     default-token-gwjtc                              kubernetes.io/service-account-token   3      4h41m\nkube-public         default-token-lmnr7                              kubernetes.io/service-account-token   3      4h41m\nkube-system         aks-secrets-store-csi-driver-token-n9wg2         kubernetes.io/service-account-token   3      4h41m\nkube-system         aks-secrets-store-provider-azure-token-zrswn     kubernetes.io/service-account-token   3      4h41m\nkube-system         ama-logs-secret                                  Opaque                                2      4h41m\nkube-system         ama-logs-token-ff7zz                             kubernetes.io/service-account-token   3      4h41m\nkube-system         attachdetach-controller-token-km8wr              kubernetes.io/service-account-token   3      4h41m\nkube-system         azure-cloud-provider-token-krcx6                 kubernetes.io/service-account-token   3      4h41m\nkube-system         azure-npm-token-vx74r                            kubernetes.io/service-account-token   3      4h41m\nkube-system         azure-policy-token-kk8hf                         kubernetes.io/service-account-token   3      4h41m\nkube-system         azure-policy-webhook-account-token-jf62w         kubernetes.io/service-account-token   3      4h41m\nkube-system         azure-policy-webhook-cert                        Opaque                                3      4h41m\nkube-system         bootstrap-signer-token-9nkl5                     kubernetes.io/service-account-token   3      4h41m\nkube-system         bootstrap-token-0f44ke                           bootstrap.kubernetes.io/token         4      4h41m\nkube-system         certificate-controller-token-hdq7d               kubernetes.io/service-account-token   3      4h41m\nkube-system         cloud-node-manager-token-vd9l2                   kubernetes.io/service-account-token   3      4h41m\nkube-system         clusterrole-aggregation-controller-token-5sscz   kubernetes.io/service-account-token   3      4h41m\nkube-system         coredns-autoscaler-token-ktffp                   kubernetes.io/service-account-token   3      4h41m\nkube-system         coredns-token-v4rm5                              kubernetes.io/service-account-token   3      4h41m\nkube-system         cronjob-controller-token-kp2w4                   kubernetes.io/service-account-token   3      4h41m\nkube-system         csi-azuredisk-node-sa-token-fm6mv                kubernetes.io/service-account-token   3      4h41m\nkube-system         csi-azurefile-node-sa-token-dj2d8                kubernetes.io/service-account-token   3      4h41m\nkube-system         daemon-set-controller-token-j28q7                kubernetes.io/service-account-token   3      4h41m\nkube-system         default-token-4wvg4                              kubernetes.io/service-account-token   3      4h41m\nkube-system         deployment-controller-token-4tmbj                kubernetes.io/service-account-token   3      4h41m\nkube-system         disruption-controller-token-p48hj                kubernetes.io/service-account-token   3      4h41m\nkube-system         endpoint-controller-token-7g4qt                  kubernetes.io/service-account-token   3      4h41m\nkube-system         endpointslice-controller-token-j8jkm             kubernetes.io/service-account-token   3      4h41m\nkube-system         endpointslicemirroring-controller-token-csssp    kubernetes.io/service-account-token   3      4h41m\nkube-system         ephemeral-volume-controller-token-6qf9d          kubernetes.io/service-account-token   3      4h41m\nkube-system         expand-controller-token-rrk2h                    kubernetes.io/service-account-token   3      4h41m\nkube-system         generic-garbage-collector-token-56vgh            kubernetes.io/service-account-token   3      4h41m\nkube-system         horizontal-pod-autoscaler-token-shkvl            kubernetes.io/service-account-token   3      4h41m\nkube-system         job-controller-token-5wn2v                       kubernetes.io/service-account-token   3      4h41m\nkube-system         konnectivity-agent-token-vgtm8                   kubernetes.io/service-account-token   3      4h41m\nkube-system         konnectivity-certs                               Opaque                                3      4h41m\nkube-system         kube-proxy-token-zrfjt                           kubernetes.io/service-account-token   3      4h41m\nkube-system         metrics-server-token-92vvw                       kubernetes.io/service-account-token   3      4h41m\nkube-system         namespace-controller-token-f7g8t                 kubernetes.io/service-account-token   3      4h41m\nkube-system         node-controller-token-jz4h4                      kubernetes.io/service-account-token   3      4h41m\nkube-system         persistent-volume-binder-token-pfqqx             kubernetes.io/service-account-token   3      4h41m\nkube-system         pod-garbage-collector-token-sxfld                kubernetes.io/service-account-token   3      4h41m\nkube-system         pv-protection-controller-token-swj5m             kubernetes.io/service-account-token   3      4h41m\nkube-system         pvc-protection-controller-token-tqdv7            kubernetes.io/service-account-token   3      4h41m\nkube-system         replicaset-controller-token-q8chf                kubernetes.io/service-account-token   3      4h41m\nkube-system         replication-controller-token-7d4zq               kubernetes.io/service-account-token   3      4h41m\nkube-system         resourcequota-controller-token-5thvs             kubernetes.io/service-account-token   3      4h41m\nkube-system         root-ca-cert-publisher-token-5nc9n               kubernetes.io/service-account-token   3      4h41m\nkube-system         service-account-controller-token-4bg7v           kubernetes.io/service-account-token   3      4h41m\nkube-system         statefulset-controller-token-kjgh9               kubernetes.io/service-account-token   3      4h41m\nkube-system         token-cleaner-token-qbzs2                        kubernetes.io/service-account-token   3      4h41m\nkube-system         ttl-after-finished-controller-token-7zgnt        kubernetes.io/service-account-token   3      4h41m\nkube-system         ttl-controller-token-9252z                       kubernetes.io/service-account-token   3      4h41m\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#cluster-details","title":"Cluster details","text":"<p><pre><code>az aks show -g 'rg-aks-dev' -n 'aks-cluster1-dev'\n</code></pre> <pre><code>{\n  \"aadProfile\": {\n    \"adminGroupObjectIDs\": [\n    ...\n    ...\n</code></pre></p> <pre><code>kubectl get roles -A\nkubectl get rolebindings -A\nkubectl get clusterroles -A\nkubectl get clusterrolebindings -A\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#more-commands","title":"more commands","text":"<pre><code>#We can easily filter using group\nkubectl api-resources | grep pod\n\n#Explain an indivdual resource in detail\nkubectl explain pod | more \nkubectl explain pod.spec | more \nkubectl explain pod.spec.containers | more \nkubectl explain pod --recursive | more \n\n#Let's take a closer look at our nodes using Describe\n#Check out Name, Taints, Conditions, Addresses, System Info, Non-Terminated Pods, and Events\nkubectl describe nodes c1-cp1 | more \nkubectl describe nodes c1-node1 | more\n\n#Use -h or --help to find help\nkubectl -h | more\nkubectl get -h | more\nkubectl create -h | more\n\n#Ok, so now that we're tired of typing commands out, let's enable bash auto-complete of our kubectl commands\nsudo apt-get install -y bash-completion\necho \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nkubectl g[tab][tab] po[tab][tab] --all[tab][tab]\n</code></pre>"},{"location":"kubernetes/3-working-with-aks/#troubleshooting-errors","title":"Troubleshooting errors","text":"<p>In case if you are getting following error while running <code>kubectl</code> commands than that means you need to convert or switch to Azure kubelogin.</p> <p><pre><code>\u276f az aks browse  -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\nKubernetes resources view on https://portal.azure.com/#resource/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-aks-dev/providers/Microsoft.ContainerService/managedClusters/aks-cluster1-dev/workloads\n\"Kubernetes resources view on https://portal.azure.com/#resource/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-aks-dev/providers/Microsoft.ContainerService/managedClusters/aks-cluster1-dev/workloads\"\n\n~ on \u2601\ufe0f\n\u276f kubectl get no\nerror: The azure auth plugin has been removed.\nPlease use the https://github.com/Azure/kubelogin kubectl/client-go credential plugin instead.\nSee https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins for further details\n</code></pre> or</p> <pre><code>error: unknown flag: --environment\nE0226 12:20:52.660790  116560 memcache.go:238] couldn't get current server API group list: Get \"https://cluster1-dns-73957b84.hcp.eastus.azmk8s.io:443/api?timeout=32s\": getting credentials: exec: executable kubelogin failed with exit code 1\n</code></pre> <p>reference for more reading- https://aptakube.com/blog/how-to-use-azure-kubelogin</p>"},{"location":"kubernetes/3-working-with-aks/#install-kubelogin","title":"Install kubelogin","text":"<pre><code>brew install Azure/kubelogin/kubelogin\n</code></pre> <pre><code>==&gt; Downloading https://formulae.brew.sh/api/formula.json\nRunning `brew update --auto-update`...\n\n==&gt; Downloading https://formulae.brew.sh/api/cask.json\n\n==&gt; Tapping azure/kubelogin\nCloning into '/opt/homebrew/Library/Taps/azure/homebrew-kubelogin'...\nremote: Enumerating objects: 111, done.\nremote: Counting objects: 100% (12/12), done.\nremote: Compressing objects: 100% (9/9), done.\nremote: Total 111 (delta 6), reused 6 (delta 3), pack-reused 99\nReceiving objects: 100% (111/111), 26.54 KiB | 274.00 KiB/s, done.\nResolving deltas: 100% (54/54), done.\nTapped 2 formulae (19 files, 49.0KB).\n==&gt; Fetching azure/kubelogin/kubelogin\n==&gt; Downloading https://github.com/Azure/kubelogin/releases/download/v0.0.26/kubelogin-darwin-arm6\n==&gt; Downloading from https://objects.githubusercontent.com/github-production-release-asset-2e65be/\n######################################################################## 100.0%\n==&gt; Installing kubelogin from azure/kubelogin\n\ud83c\udf7a  /opt/homebrew/Cellar/kubelogin/0.0.26: 3 files, 43.4MB, built in 1 second\n==&gt; Running `brew cleanup kubelogin`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n</code></pre> <p>To convert your kubectl authentication method to Azure CLI, all you need to do is run:</p> <pre><code>kubelogin convert-kubeconfig -l azurecli\n</code></pre> <p>That's all we need for now to start working with our AKS cluster.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/","title":"Setup NGINX ingress controller in AKS using Terraform","text":""},{"location":"kubernetes/4-ingress-controller-nginx/#introduction","title":"Introduction","text":"<p>NGINX Ingress Controller is a type of ingress controller, it is a piece of software (configuration) that manages incoming traffic to a Kubernetes cluster. It works as a reverse proxy and load balancer, routing incoming traffic to the appropriate Kubernetes services based on the rules defined in the ingress resources. When you use an ingress controller and ingress rules, a single IP address can be used to route traffic to multiple services in a Kubernetes cluster.</p> <p>Ingress resources are Kubernetes objects that define rules for routing incoming traffic to specific services. The NGINX Ingress Controller is responsible for reading and interpreting these rules and routing the traffic accordingly.</p> <p>The NGINX Ingress Controller is a popular choice for Kubernetes users because it is lightweight, highly scalable, and provides advanced features like SSL termination, rate limiting, and WebSockets support.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#what-is-ingress","title":"What is Ingress?","text":"<p>Ingress serves as the gateway for HTTP and HTTPS traffic from external sources into your cluster. It efficiently directs this traffic to specific services within the cluster based on predefined rules set on the Ingress resource.</p> <p></p> <p>Reference diagram from Kubernetes documentation.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code> you've been asked to setup the NGINX ingress controller in an Azure Kubernetes Service (AKS) cluster. also deploy couple of applications in the AKS cluster, each of which is accessible over the single IP address.</p> <p>Install ingress-nginx controller helm chart using terraform</p> <p>Another requirement here is to make sure that installation of Nginx ingress controller in AKS is completely automated, to fulfill this requirement we are going to use terraform configuration to install the ingress-nginx controller in our AKS.</p> <p>Note</p> <p>There are two open source ingress controllers for Kubernetes based on Nginx: </p> <ul> <li>One is maintained by the Kubernetes community (kubernetes/ingress-nginx), </li> <li>Second one is maintained by NGINX, Inc. (nginxinc/kubernetes-ingress). </li> </ul> <p>Here we will be using the Kubernetes community ingress controller.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have a Kubernetes cluster up and running along with following:</p> <ul> <li>Azure subscription - https://azure.microsoft.com/en-us/free/</li> <li>Install and configure Terraform - https://www.terraform.io/downloads</li> <li>Define Terraform providers for Helm Install<ul> <li>helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider</li> </ul> </li> <li>Install azure CLI - https://learn.microsoft.com/en-us/cli/azure/install-azure-cli</li> <li>Install and setup kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/</li> <li>Install Helm client</li> <li>Azure Container Registry (ACR) Create Azure Container Registry (ACR) using terraform</li> <li>Azure Kubernetes Service (AKS) Create Azure Kubernetes Service (AKS) using terraform</li> </ul>"},{"location":"kubernetes/4-ingress-controller-nginx/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1. Create a new namespace for Nginx ingress Controller</li> <li>Step 2: Install ingress nginx controller using terraform </li> <li>Step 3: Verify ingress-nginx resources in AKS</li> <li>Step 4: Deploy sample applications for Ingress testing</li> <li>Step 5: Create an ingress route</li> <li>Step-6: Test the ingress controller (Browse website URLs)</li> <li>Step-7: Add DNS recordset in DNS Zone</li> <li>Step-8: Create Ingress YAML file (Apply Let's Encrypt changes)</li> </ul>"},{"location":"kubernetes/4-ingress-controller-nginx/#architecture-diagram","title":"Architecture diagram","text":"<p>Here is the high level architecture diagram of ingress nginx controller components.</p> <p></p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"kubernetes/4-ingress-controller-nginx/#implementation-details","title":"Implementation Details","text":"<p>The steps given below will guide you through the process of setting up Nginx Ingress Controller in your AKS cluster. By the end of this exercise, you'll have a functional environment for ingress route working for applications within Kubernetes.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-1-configure-terraform-providers","title":"Step-1: Configure Terraform providers","text":"<p>Launch Visual Studio Code and open your current Terraform repository to begin working on your Terraform configuration.</p> <p>In order to install any Helmcharts using terraform configuration we need to have following terraform providers.</p> <ul> <li>helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider </li> </ul> <p>The Helm Provider allows you to manage your Helm charts and releases as part of your Terraform-managed infrastructure. With the Helm Provider, you can define your charts as Terraform resources and manage their installation and updates through Terraform.</p> <p>With Terraform, you can manage the installation, upgrades, and deletion of your Helm charts in a repeatable, version-controlled manner. This can help simplify your infrastructure management, ensure consistency and repeatability, and reduce the chance of manual errors.</p> <p>terraform providers</p> <p>You can install the necessary providers by adding the following code in your Terraform configuration file:</p> <p>Let's update our existing <code>provider.tf</code> file with new kubernetes, helm and kubectl providers:</p> provider.tf<pre><code>terraform {\n\n  required_version = \"&gt;=0.12\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;2.0\"\n    }\n\n    azuread = {\n      version = \"&gt;= 2.26.0\" // https://github.com/terraform-providers/terraform-provider-azuread/releases\n    }\n     kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"&gt;= 2.0.3\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"&gt;= 2.1.0\"\n    }\n\n     kubectl = {\n      source  = \"gavinbunney/kubectl\"\n      version = \"&gt;= 1.7.0\"\n    }\n  }\n}\n\nprovider \"kubernetes\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  #load_config_file       = false\n}\n\nprovider \"helm\" {\n  debug = true\n  kubernetes {\n    host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n\n  }\n}\nprovider \"kubectl\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  load_config_file       = false\n}\n</code></pre> <p>If you want to know more information about these provides, you can use following links for further reading.</p> <p>helm provider </p> <p>more info -  https://registry.terraform.io/providers/hashicorp/helm/latest/docs</p> <p>Kubernetes provider</p> <p>more info -  https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs</p> <p>Kubectl provider </p> <p>more info - https://registry.terraform.io/providers/gavinbunney/kubectl/latest/docs</p> <p>terraform init </p> <p>Since we added new providers in the terraform providers list, it is necessary to run the terraform init once again.</p> <p><pre><code>terraform init\n</code></pre> output <pre><code>Initializing the backend...\n\nInitializing provider plugins...\n- Reusing previous version of hashicorp/azurerm from the dependency lock file\n- Reusing previous version of hashicorp/azuread from the dependency lock file\n- Finding hashicorp/kubernetes versions matching \"&gt;= 2.0.3\"...\n- Reusing previous version of hashicorp/random from the dependency lock file\n- Finding hashicorp/helm versions matching \"&gt;= 2.1.0\"...\n- Finding gavinbunney/kubectl versions matching \"&gt;= 1.7.0\"...\n- Installing hashicorp/kubernetes v2.18.1...\n- Installed hashicorp/kubernetes v2.18.1 (signed by HashiCorp)\n- Using previously-installed hashicorp/random v3.4.3\n- Installing hashicorp/helm v2.9.0...\n- Installed hashicorp/helm v2.9.0 (signed by HashiCorp)\n- Installing gavinbunney/kubectl v1.14.0...\n- Installed gavinbunney/kubectl v1.14.0 (self-signed, key ID AD64217B5ADD572F)\n- Using previously-installed hashicorp/azurerm v3.31.0\n- Using previously-installed hashicorp/azuread v2.33.0\n\n\nTerraform has been successfully initialized!\n</code></pre></p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-2-create-a-new-namespace-for-ingress","title":"Step-2: Create a new namespace for ingress","text":"<p>Create a separate namespace for nginx-ingress so that ingress specific AKS resources together in one single namespace and logically isolated from other Kubernetes resources. let's create a file called <code>nginx-ingress.tf</code> and copy following terraform configuration.</p> <p>nginx-ingress.tf<pre><code>resource \"kubernetes_namespace\" \"ingress\" {  \n  metadata {\n    name = \"ingress\"\n  }\n}\n</code></pre> run terraform validate &amp; format</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>run terraform plan</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output</p> <p><pre><code> + create\n\nTerraform will perform the following actions:\n\n# kubernetes_namespace.ingress will be created\n+ resource \"kubernetes_namespace\" \"ingress\" {\n    + id = (known after apply)\n    + metadata {\n        + generation       = (known after apply)\n        + name             = \"ingress\"\n        + resource_version = (known after apply)\n        + uid              = (known after apply)\n      }\n  }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> run terraform apply <pre><code>terraform apply dev-plan\n</code></pre> <pre><code>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre></p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-2-install-ingress-resources-with-helm-chart-using-terraform","title":"Step-2: Install ingress resources with helm-chart using terraform","text":"<p>Now it is time to install ingress helm chart in AKS cluster.</p> <p>We are going to get the ingress helm chart details from official Nginx ingress ArtifactHUB website.</p> <p>https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx - click on Install button and get following helm-chart details</p> <pre><code>repository = \"https://kubernetes.github.io/ingress-nginx\"\nchart      = \"ingress-nginx\"\nversion    = \"4.5.2\"\n</code></pre> <p>we are going to use the <code>helm_release</code> terraform resource to install the Helm Chart:</p> nginx-ingress.tf<pre><code># Install ingress helm chart using terraform\nresource \"helm_release\" \"ingress\" {\n  name       = \"ingress\"\n  repository = \"https://kubernetes.github.io/ingress-nginx\"\n  chart      = \"ingress-nginx\"\n  version    = \"4.5.2\"\n  namespace  = kubernetes_namespace.ingress.metadata.0.name\n  depends_on = [\n    kubernetes_namespace.ingress\n  ]\n}\n</code></pre> <p>Let's run terraform plan</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> output <pre><code># helm_release.ingress will be created\n  + resource \"helm_release\" \"ingress\" {\n      + atomic                     = false\n      + chart                      = \"ingress-nginx\"\n      + cleanup_on_fail            = false\n      + create_namespace           = false\n      + dependency_update          = false\n      + disable_crd_hooks          = false\n      + disable_openapi_validation = false\n      + disable_webhooks           = false\n      + force_update               = false\n      + id                         = (known after apply)\n      + lint                       = false\n      + manifest                   = (known after apply)\n      + max_history                = 0\n      + metadata                   = (known after apply)\n      + name                       = \"ingress\"\n      + namespace                  = \"ingress\"\n      + pass_credentials           = false\n      + recreate_pods              = false\n      + render_subchart_notes      = true\n      + replace                    = false\n      + repository                 = \"https://kubernetes.github.io/ingress-nginx\"\n      + reset_values               = false\n      + reuse_values               = false\n      + skip_crds                  = false\n      + status                     = \"deployed\"\n      + timeout                    = 300\n      + verify                     = false\n      + version                    = \"4.5.2\"\n      + wait                       = true\n      + wait_for_jobs              = false\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre></p> <p>run terraform apply</p> <p><pre><code>terraform apply dev-plan\n</code></pre> <pre><code>helm_release.ingress: Creating...\nhelm_release.ingress: Still creating... [10s elapsed]\nhelm_release.ingress: Still creating... [20s elapsed]\nhelm_release.ingress: Still creating... [30s elapsed]\nhelm_release.ingress: Still creating... [40s elapsed]\nhelm_release.ingress: Still creating... [50s elapsed]\nhelm_release.ingress: Still creating... [1m0s elapsed]\nhelm_release.ingress: Creation complete after 1m7s [id=ingress]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre></p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-3-verify-ingress-nginx-resources-in-aks","title":"Step 3. Verify Ingress-Nginx resources in AKS.","text":"<p>Step-2 terraform output shows that helm chart installation is successful therefore in this step we can validate Ingress-Nginx installation by running following commands.</p> <p>Connect to AKS cluster</p> <pre><code># azure CLI\n\n# Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n</code></pre> <p>Get pods and services related to ingress.</p> <pre><code>kubectl get namespace ingress\nkubectl get deployments -n ingress\nkubectl get pods -n ingress\nkubectl get services -n ingress\nkubectl get configmaps -n ingress\nkubectl get secrets -n ingress\n</code></pre> <p>Get all resources related to ingress.</p> <p><pre><code>kubectl get all -n ingress\n</code></pre> <pre><code>NAME                                                   READY   STATUS    RESTARTS   AGE\npod/ingress-ingress-nginx-controller-f9c49b469-rjdlw   1/1     Running   0          4m32s\n\nNAME                                                 TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)                      AGE\nservice/ingress-ingress-nginx-controller             LoadBalancer   10.25.221.77    20.121.154.157   80:30515/TCP,443:31485/TCP   4m33s\nservice/ingress-ingress-nginx-controller-admission   ClusterIP      10.25.113.145   &lt;none&gt;           443/TCP                      4m33s\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ingress-ingress-nginx-controller   1/1     1            1           4m32s\n\nNAME                                                         DESIRED   CURRENT   READY   AGE\nreplicaset.apps/ingress-ingress-nginx-controller-f9c49b469   1         1         1       4m32s\n</code></pre></p> <p>configmaps details <pre><code>kubectl get configmaps -n ingress\n</code></pre></p> <pre><code>NAME                               DATA   AGE\ningress-ingress-nginx-controller   1      18m\nkube-root-ca.crt                   1      18m\n</code></pre> <p>Ingress secrets </p> <p><pre><code>kubectl get secrets -n ingress\n</code></pre> <pre><code>NAME                                TYPE                                  DATA   AGE\ndefault-token-p2kjg                 kubernetes.io/service-account-token   3      19m\ningress-ingress-nginx-admission     Opaque                                3      19m\ningress-ingress-nginx-token-vnpw8   kubernetes.io/service-account-token   3      19m\nsh.helm.release.v1.ingress.v1       helm.sh/release.v1                    1      19m\n</code></pre></p> <pre><code>kubectl get ingress -n ingress\n</code></pre> <p>output</p> <pre><code>No resources found in ingress namespace.\n</code></pre> <p>CRDs</p> <pre><code>kubectl get clusterrole -n ingress\nkubectl get clusterrolebinding -n ingress\nkubectl get CustomResourceDefinition -n ingress\nor\nkubectl get crd -l app.kubernetes.io/name=cert-manager\n</code></pre> <p>outputs from above commands shows that Nginx ingress controller is installed correctly in our AKS cluster and ready to use by deploying a application in AKS.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-4-deploy-sample-applications-for-ingress-testing","title":"Step 4: Deploy sample applications for Ingress testing","text":"<p>Since we confirmed that Nginx ingress controller is successfully install in our AKS, we can now deploy few Microservices which we built in Microservices chapter and test the Ingress controller to make sure it is routing the request to our backend AKS cluster services. </p> <p>Create a new namespace</p> <p>We are going to deploy our application in separate namespace in AKS instead of in <code>default</code> namespace. use this command to create new namespace in Kubernetes cluster. Let's name our namespace as <code>sample</code></p> <p><pre><code>kubectl create namespace sample\n</code></pre> Here I am going to re-use or create Kubernetes YAML Manifests files for <code>aspnet-api</code> </p> <p>Deployment YAML (deployment.yaml)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230323.15\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n\n# kubectl apply -f deployment.yaml -n sample\n</code></pre> <p>Service YAML (service.yaml)</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP #LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> <p>Apply the YAML manifests to deploy your ASP.NET Core MVC application:</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> <p>Create Kubernetes YAML Manifests files for <code>aspnet-app</code></p> <p>Deployment YAML (deployment.yaml)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-app\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-app\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-app\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-app\n          image: acr1dev.azurecr.io/sample/aspnet-app:20230312.1\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n\n# kubectl apply -f deployment.yaml -n sample\n</code></pre> <p>Service YAML (service.yaml)</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-app\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP #LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-app\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> <p>Apply the YAML manifests to deploy your ASP.NET Core MVC application:</p> <pre><code>kubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n</code></pre> <p>Verify the kubernetes objects deployment</p> <pre><code>kubectl get all -n sample \n\n# output\nNAME                                      READY   STATUS    RESTARTS   AGE\npod/aspnet-api-79b4cbf4bb-5dljg           1/1     Running   0          20h\npod/aspnet-app-8654797b89-99xdq           1/1     Running   0          50m\n\nNAME                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/aspnet-api           ClusterIP   10.25.69.201    &lt;none&gt;        80/TCP    178d\nservice/aspnet-app           ClusterIP   10.25.243.72    &lt;none&gt;        80/TCP    50m\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/aspnet-api           1/1     1            1           12d\ndeployment.apps/aspnet-app           1/1     1            1           50m\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/aspnet-api-79b4cbf4bb           1         1         1       12d\nreplicaset.apps/aspnet-app-8654797b89           1         1         1       50m\n</code></pre>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-5-create-an-ingress-route","title":"Step-5: Create an ingress route","text":"<p>Create a new file named <code>ingress.yaml</code> and copy in the following YAML.</p> <p>ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: aspnet-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      - path: /aspnet-app(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: aspnet-app\n            port:\n              number: 80\n      - path: /aspnet-api(/|$)(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: aspnet-api\n            port:\n              number: 80\n      - path: /(.*)\n        pathType: Prefix\n        backend:\n          service:\n            name: aspnet-app\n            port:\n              number: 80\n</code></pre> kubectl apply</p> <pre><code>kubectl apply -f ingress.yaml -n sample\n\n# output\ningress.networking.k8s.io/aspnet-ingress created\n</code></pre> <p>Get Ingress </p> <pre><code>kubectl get ingress -n sample\n\n# output \nNAME             CLASS   HOSTS   ADDRESS          PORTS   AGE\naspnet-ingress   nginx   *       20.121.154.157   80      58s\n</code></pre> <p>Note</p> <p>Verify same static IP (20.121.154.157) address assigned to both applications here.</p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-6-test-the-ingress-controller","title":"Step-6: Test the ingress controller","text":"<p>To test the routes for the ingress controller, browse two applications with path prefix mentioned in the ingress.</p> <p>for example </p> <p>ASP.NET MVC applications</p> <p>http://20.121.154.157/ or</p> <p>http://20.121.154.157/aspnet-app/</p> <p></p> <p>aspnet-api Swagger URL</p> <p>http://20.121.154.157/aspnet-api/swagger/index.html</p> <p></p>"},{"location":"kubernetes/4-ingress-controller-nginx/#step-7-add-dns-recordset-in-dns-zone","title":"Step-7: Add DNS Recordset in DNS Zone","text":"<p>There are two types of DNS zones in Azure:</p> <ul> <li> <p>Public DNS Zones: These DNS zones are publicly accessible on the internet and can be used to map domain names to IP addresses. You can use public DNS zones to host your website publicly accessible.</p> </li> <li> <p>Private DNS Zones: These DNS zones are not publicly accessible on the internet and are used to map domain names to private IP addresses within your virtual network. You can use private DNS zones to enable DNS name resolution between resources in your virtual network.</p> </li> </ul> <p>If you want to browse the URL with your custom domain instead of IP address like above you need create A recordset in one of the DNS zones depending on how you want to access your URL. </p> <p>An A record (Address Record) maps a domain name to an IPv4 address. Here we will map A record in DNS Zone.</p> <p>You can create A recordset in DNS zone using the Azure portal, Azure CLI, or Azure PowerShell. </p> <p>To create A record set in a DNS zone in Azure portal, follow these steps:</p> <ul> <li>Sign in to the Azure portal (https://portal.azure.com).</li> <li>In the left-hand menu, click on \"All services\" and type \"DNS zones\" in the search box. Click on \"DNS zones\" when it appears in the results.</li> <li>Select the DNS zone in which you want to create the record set.</li> <li>Click on the \"Record set\" button at the top of the page.</li> <li>In the \"Add record set\" blade, enter the following information:<ul> <li>Name: The name of the record set</li> <li>Type: The type of DNS record you want to create, such as A, AAAA, CNAME, MX, NS, PTR, SRV, or TXT.</li> <li>TTL: The time-to-live value for the DNS record, in seconds.</li> <li>IP address : The value or values associated with the DNS record, such as an IP address for an A record</li> </ul> </li> <li>Click the \"Add\" button to create the record set.</li> </ul> <p>Once the record set is created, it may take a few minutes for the changes to propagate across the internet. You can view and manage your DNS record sets in the Azure portal or by using Azure CLI or PowerShell commands.</p> <p>Reference:</p> <p>Implementation</p> <ul> <li>https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx - get helm chart details from here</li> <li>https://learn.microsoft.com/en-us/azure/aks/ingress-basic?tabs=azure-powershell - get the sample application from here</li> <li>https://kubernetes.github.io/ingress-nginx/ - NGINX Ingress Controller official documentation</li> <li>https://kubernetes.github.io/ingress-nginx/deploy/#azure Installation Guide</li> </ul>"},{"location":"kubernetes/4.1-ingress-controller-agic/","title":"Setup Application Gateway ingress controller (AGIC) in AKS using Terraform","text":""},{"location":"kubernetes/4.1-ingress-controller-agic/#introduction","title":"Introduction","text":"<p>There are several ways to setup the Ingress in AKS. In a previous session, we explored the setup using the Nginx Ingress controller. Now, we will into the Application Gateway Ingress Controller (AGIC).</p> <p>There are two distinct methods to setup an ingress controller in AKS:</p> <ul> <li>Setup NGINX ingress controller in AKS using Terraform</li> <li>Setup Application Gateway ingress controller (AGIC) in AKS using Terraform</li> </ul>"},{"location":"kubernetes/4.1-ingress-controller-agic/#what-is-agic","title":"What is AGIC?","text":"<p>The Application Gateway Ingress Controller allows Azure Application Gateway to be used as the ingress for an Azure Kubernetes Service aka AKS cluster. AGIC monitors the Kubernetes cluster it's hosted on and continuously updates an Application Gateway, so that selected services are exposed to the Internet. </p> <p>Note</p> <p>In order to setup Application Gateway Ingress Controller, we must need AKS and Application Gateway azure resources. </p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#benefits-of-agic","title":"Benefits of AGIC","text":"<p>Let's explore some of the advantages of AGIC when compared to other ingress controllers.</p> <ol> <li> <p>Simplified Architecture: AGIC eliminates the need for an additional load balancer or public IP address in front of your AKS cluster. This streamlines your architecture and reduces the complexity of your network setup.</p> </li> <li> <p>Efficient Data Routing: Unlike some ingress controllers that involve multiple hops for requests to reach the AKS cluster, AGIC communicates directly with pods using their private IP addresses. This minimizes data traversal and leads to better overall performance.</p> </li> <li> <p>Resource Efficiency: AGIC operates exclusively with Standard_v2 and WAF_v2 SKUs, which offer autoscaling capabilities. This means that AGIC can dynamically adjust its resources in response to changes in traffic load without impacting your AKS cluster's resources.</p> </li> <li> <p>Enhanced Security: AGIC, when used in conjunction with Application Gateway, provides additional security features such as TLS policy enforcement and Web Application Firewall (WAF) functionality. This helps safeguard your AKS cluster and applications from threats and vulnerabilities.</p> </li> </ol>"},{"location":"kubernetes/4.1-ingress-controller-agic/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code> you've been asked to setup the AGIC ingress controller in an Azure Kubernetes Service (AKS) cluster. also deploy couple of applications in the AKS cluster, each of which is accessible over the single IP address.</p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#architecture-diagram","title":"Architecture diagram","text":"<p>Here is the reference architecture diagram of AGIC components from MSDN documentation.</p> <p></p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have a Kubernetes cluster &amp; Application Gateway up and running along with following:</p> <ul> <li>Azure subscription - https://azure.microsoft.com/en-us/free/</li> <li>Install and configure Terraform - https://www.terraform.io/downloads</li> <li>Setup Terraform Foundation <ul> <li>Part-1 - http://k8s.anjikeesari.com/azure/6-tf-foundation-1/</li> <li>Part-2 - http://k8s.anjikeesari.com/azure/6-tf-foundation-2/ </li> </ul> </li> <li>Install azure CLI - https://learn.microsoft.com/en-us/cli/azure/install-azure-cli</li> <li>Install and setup kubectl - https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/</li> <li>AKS cluster was created with Azure CNI and managed identities </li> <li>Application Gateway v2 in the same virtual network as AKS</li> </ul>"},{"location":"kubernetes/4.1-ingress-controller-agic/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step-1: Enable AGIC add-on in AKS cluster</li> <li>Step-2: Create role assignment for AGIC Pod</li> <li>Step-3: Verify the installation</li> <li>Step-4: Deploy an application</li> <li>Step-5: Verify Changes in Application Gateway</li> <li>Step-6: Access Your Application</li> </ul> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show\n\n# Lists all available Azure subscriptions\naz account list\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#implementation-details","title":"Implementation Details","text":"<p>The steps given below will guide you through the process of setting up Application Gateway Ingress Controller (AGIC) in your AKS cluster. By the end of this exercise, you'll have a functional environment for ingress route working for applications within Kubernetes.</p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-1-enable-agic-add-on-in-aks-cluster","title":"Step-1: Enable AGIC add-on in AKS cluster","text":"<p>Setting up AGIC in an existing AKS cluster can be done in various ways. In this guide, we'll utilize a Terraform script for configuration.</p> <p>Use this Terraform script to enable the Application Gateway Ingress Controller (AGIC) add-on for your Azure Kubernetes Services (AKS) cluster:</p> <pre><code>ingress_application_gateway {\n  gateway_id = azurerm_application_gateway.appgtw.id\n}\n</code></pre> <p>Add this piece of code to your existing AKS Terraform configuration, this was created as part of AKS setup. After that, run the following commands:</p> <p>Run Terraform Plan &amp; Apply:</p> <p>Execute these commands to see your changes in action:</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> <p>Verify the AGIC Add-on in AKS: </p> <p>Once the Terraform apply has completed successfully, go to your AKS cluster's networking tab. Ensure that AGIC is selected, and you'll also find the application gateway name there.</p> <p></p> <p>Check the AGIC Pod</p> <p>You'll notice a new pod created in the <code>kube-system</code> namespace within your AKS resource group.</p> <p></p> <p>Check the AGIC Managed Service Identity (MSI):</p> <p>Verify the existence of the following managed identity in the MC_rg_xxx resource group.</p> <p></p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-2-create-role-assignment-for-agic-pod","title":"Step-2: Create role assignment for AGIC Pod","text":"<p>The Application Gateway Ingress Controller (AGIC) pod requires specific permissions on the Azure Application Gateway to function correctly. These permissions include:</p> <ol> <li> <p>Read Permission: AGIC needs to retrieve the current configuration of the Azure Application Gateway, including the list of listeners, rules, and backend pools.</p> </li> <li> <p>Write Permission: AGIC needs to configure the Azure Application Gateway to route traffic to applications on the AKS cluster. This involves creating and updating listeners, rules, and backend pools.</p> </li> <li> <p>Delete Permission: AGIC needs to delete resources that are no longer needed, like listeners and rules that are no longer in use.</p> </li> </ol> <p>These permissions enable AGIC to manage load balancing rules effectively for AKS cluster applications. The specific Azure RBAC role providing these permissions may vary based on your Azure environment's configuration. Typically, the Contributor role is sufficient.</p> <ul> <li>reference - https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-install-existing</li> </ul> <p>Follow these steps:</p> <p>1. Get the Managed Identity ID of the AGIC Pod in AKS Cluster</p> <p>When you create an AKS cluster, it generates an additional resource group that starts with \"MC_\" followed by the cluster name and location. This resource group is used for managing various resources related to the AKS cluster. When you enable the AGIC (Application Gateway Ingress Controller) add-on, AGIC pod's managed identity is automatically created within this resource group.</p> <p>Here's a Terraform configuration to retrieve the resource ID of this managed identity from the Azure Managed Cloud resource group:</p> <p>aks-agic-pod-settings.tf<pre><code>data \"azurerm_user_assigned_identity\" \"pod_identity_appgw\" {\n  name                = \"ingressapplicationgateway-${azurerm_kubernetes_cluster.aks.name}\"\n  resource_group_name = \"MC_${data.azurerm_resource_group.rg.name}_${azurerm_kubernetes_cluster.aks.name}_${var.aks_rg_suffix}\"\n  depends_on = [\n    data.azurerm_resource_group.rg,\n    azurerm_kubernetes_cluster.aks,\n  ]\n}\n</code></pre> This Terraform data block searches for the user-assigned identity with the specified name within the managed resource group for your AKS cluster. The </p> <p>2. Contributor role to the AGIC pod on the Application Gateway</p> <p>Create a Terraform configuration to grant the Contributor role to the AGIC pod on the Application Gateway.</p> aks-agic-pod-settings.tf<pre><code>resource \"azurerm_role_assignment\" \"identity_appgw_contributor_ra\" {\n  scope                = azurerm_application_gateway.appgtw.id\n  role_definition_name = \"Contributor\"\n  principal_id         = data.azurerm_user_assigned_identity.pod_identity_appgw.principal_id\n  # skip_service_principal_aad_check = true\n  lifecycle {\n    ignore_changes = [\n      skip_service_principal_aad_check,\n    ]\n  }\n  depends_on = [\n    azurerm_kubernetes_cluster.aks,\n    azurerm_application_gateway.appgtw\n  ]\n}\n</code></pre> <p>3. Reader access to the Application Gateway resource group</p> <p>Provide the identity with Reader access to the Application Gateway resource group by adding the following Terraform configuration:</p> aks-agic-pod-settings.tf<pre><code># Give the identity Reader access to the Application Gateway resource group.\nresource \"azurerm_role_assignment\" \"identity_appgw_reader_ra\" {\n  scope                = data.azurerm_resource_group.rg.id\n  role_definition_name = \"Reader\"\n  principal_id         = data.azurerm_user_assigned_identity.pod_identity_appgw.principal_id\n  # skip_service_principal_aad_check = true\n  lifecycle {\n    ignore_changes = [\n      skip_service_principal_aad_check,\n    ]\n  }\n  depends_on = [\n    data.azurerm_resource_group.rg,\n    azurerm_kubernetes_cluster.aks,\n    azurerm_application_gateway.appgtw\n  ]\n}\n</code></pre> <p>4. Network Contributor access</p> <p>Give the Network Contributor access to the Application Gateway subnet's Virtual network or vnet's resource group.</p> <p>aks-agic-pod-settings.tf<pre><code>resource \"azurerm_role_assignment\" \"identity_appgw_network_contributor_ra\" {\n  # scope                = data.azurerm_virtual_network.hub_vnet.id\n  scope                = data.azurerm_resource_group.hub_vnet_rg.id\n  role_definition_name = \"Network Contributor\"\n  principal_id         = data.azurerm_user_assigned_identity.pod_identity_appgw.principal_id\n  # skip_service_principal_aad_check = true\n  lifecycle {\n    ignore_changes = [\n      skip_service_principal_aad_check,\n    ]\n  }\n  depends_on = [\n    azurerm_kubernetes_cluster.aks,\n    azurerm_application_gateway.appgtw\n  ]\n}\n</code></pre> https://learn.microsoft.com/en-us/azure/application-gateway/configuration-infrastructure#virtual-network-permission</p> <p>5. Validate Role Assignments</p> <p>These configurations ensure that the AGIC pod has the necessary permissions to interact with the Application Gateway.</p> <p>Application Gateway Access Control (IAM)</p> <p></p> <p>Virtual Network Access Control (IAM)</p> <p></p> <p>Please note that these permissions should be carefully managed to maintain security and access control in your environment.</p> <p>6. Validate AGIC pod logs</p> <p><pre><code>kubectl get pods -n kube-system\n</code></pre> output</p> <pre><code>NAME                                        READY   STATUS             RESTARTS          AGE\ningress-appgw-deployment-7d5cc48645-64j8x   1/1     Running            0                 45h\n</code></pre> <p><pre><code>kubectl logs --since=5m pod/ingress-appgw-deployment-7d5cc48645-64j8x -n kube-system\n</code></pre> output</p> <pre><code>I0902 16:13:40.683379       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:13:40.779898       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:13:41.493144       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:13:53.403917       1 reflector.go:530] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: Watch close - *v1.Endpoints total 7 items received\nI0902 16:13:58.672502       1 reflector.go:530] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: Watch close - *v1.Pod total 13 items received\nI0902 16:14:10.581990       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:14:10.640850       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:14:10.641353       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:14:10.683622       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:14:10.780401       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\nI0902 16:14:11.495856       1 reflector.go:381] pkg/mod/k8s.io/client-go@v0.21.2/tools/cache/reflector.go:167: forcing resync\n</code></pre> <p>Step-3:  Deploy a sample application by using AGIC for ingress on the AKS cluster.</p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-3-verify-the-installation","title":"Step-3: Verify the Installation","text":"<p>Check the status of the Application Gateway Ingress Controller pods to ensure it's running:</p> <pre><code>kubectl get pods -n kube-system | grep azure-ingress-controller\n</code></pre>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-3-deploy-an-application","title":"Step-3: Deploy an application","text":"<p>Deploy a sample application by using AGIC for ingress on the AKS cluster.</p> <p>To verify that the Application Gateway + AKS + AGIC installation is set up correctly, deploy the simplest possible app:</p> asp-net.yaml<pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-agic-app-pod\n  labels:\n    app: test-agic-app\nspec:\n  containers:\n  - image: \"mcr.microsoft.com/dotnet/samples:aspnetapp\"\n    name: aspnetapp-image\n    ports:\n    - containerPort: 80\n      protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: test-agic-app-service\nspec:\n  selector:\n    app: test-agic-app\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-agic-app-ingress\n  annotations:\n    kubernetes.io/ingress.class: azure/application-gateway\nspec:\n  rules:\n    - host: test.agic.contoso.com\n      http:\n        paths:\n          - path: /\n            backend:\n              serviceName: test-agic-app-service\n              servicePort: 80\nEOF\n</code></pre> <p>reference- https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-troubleshoot</p> <p>After a successful deployment of the app above your AKS cluster will have a new Pod, Service and an Ingress.</p> <p>Reference:</p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-4-verify-changes-in-application-gateway","title":"Step-4: Verify Changes in Application Gateway","text":"<p>Verifying changes in your Azure Application Gateway is an essential part of ensuring that your configuration and routing updates are functioning as expected. Here are some steps on how to verify changes in your Application Gateway:</p> <ul> <li>Access the Azure Portal</li> <li>Navigate to Your Application Gateway</li> <li>Check Backend Health</li> <li>Review Listener Configuration</li> <li>Inspect HTTP Settings</li> </ul>"},{"location":"kubernetes/4.1-ingress-controller-agic/#step-5-check-that-the-application-is-reachable-through-application-gateway","title":"Step-5: Check that the application is reachable through application gateway.","text":"<p>Wait for the Application Gateway Ingress Controller to configure routing rules. Once configured, you can access your application via the Application Gateway's public IP or DNS name.</p>"},{"location":"kubernetes/4.1-ingress-controller-agic/#conclusion","title":"Conclusion","text":"<p>You've successfully set up the Application Gateway Ingress Controller in Azure AKS, allowing you to route traffic to your Kubernetes services using Azure Application Gateway.</p>"},{"location":"kubernetes/5-cert-manager/","title":"Setup Cert-Manager in AKS using Terraform","text":""},{"location":"kubernetes/5-cert-manager/#introduction","title":"Introduction","text":"<p>Cert-Manager</p> <p>Cert-Manager is an open-source tool that automates the management and issuance of TLS certificates in Kubernetes. It is used to manage and automate the process of obtaining, renewing, and revoking X.509 certificates from various certificate authorities. Cert-Manager integrates with Kubernetes by creating custom resource definitions (CRDs) to define and manage certificate issuance, renewal, and revocation processes. This helps simplify and automate certificate management, reducing the chances of manual errors and security vulnerabilities. Cert-Manager is compatible with various certificate authorities, including Let's Encrypt, which provides free and open-source certificates.</p> <p>Let's Encrypt</p> <p>Let's Encrypt is a free, automated, and open certificate authority (CA). It provides domain-validated X.509 certificates for Transport Layer Security (TLS) encryption, which is used to secure web traffic. These certificates can be obtained and managed automatically, making it easier and more cost-effective for website owners to encrypt their sites. Let's Encrypt is a non-profit organization and its goal is to make encryption accessible and affordable for all websites. By providing free, easy-to-use certificates, Let's Encrypt aims to encourage widespread adoption of encryption, helping to create a safer and more secure internet. Certificates issued by Let's Encrypt are trusted by all major browsers and are renewable every 90 days.</p> <p>Key Features</p> <ol> <li>Automated issuance and renewal of certificates to secure Ingress with TLS.</li> <li>Fully integrated issuers from recognized public and private Certificate Authorities.</li> <li>Secure pod-to-pod communication with mTLS using private PKI issuers.</li> <li>Supports certificate use cases for web-facing and internal workloads.</li> <li>Open-source add-ons for enhanced cloud-native service mesh security.</li> <li>Backed by major cloud service providers and distributions.</li> </ol>"},{"location":"kubernetes/5-cert-manager/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>  you have been asked to provision certificates for your applications deployed in Kubernetes, another requirement is that it should be Cloud native certificate management. Here we are going to use cert-Manager as a Kubernetes operator, that can provision certificates from certificate authorities like Let's Encrypt automatically.</p> <p>Install Cert-Manager helm chart using terraform</p> <p>Another requirement here is to make sure that installation of Cert-Manager in AKS cluster is completely automated, to fulfill this requirement we are going to use terraform configuration to install the Cert-Manager in our AKS.</p>"},{"location":"kubernetes/5-cert-manager/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this exercise, ensure you have the following in place:</p> <ul> <li>A functioning Kubernetes cluster</li> <li>An active Azure subscription Link</li> <li>Terraform installed and configured Guide</li> <li>Defined Terraform providers for Helm installation, including:<ul> <li>Helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider</li> </ul> </li> <li>Azure CLI installed Guide</li> <li>Kubectl installed and set up Guide</li> <li>Helm client installed</li> <li>An AKS (Azure Kubernetes Service) cluster</li> </ul>"},{"location":"kubernetes/5-cert-manager/#objective","title":"Objective","text":"<p>In this exercise, we will cover the following steps to implement Cert-Manager in your AKS cluster:</p> <ol> <li> <p>Step-1: Define Terraform Providers for Helm Install (Setup Terraform)</p> </li> <li> <p>Step 2: Create a New Namespace for Cert-Manager</p> </li> <li> <p>Step 3: Install Cert-Manager Helm Chart Using Terraform</p> </li> <li> <p>Step 4: Verify Cert-Manager Resources in AKS</p> </li> <li> <p>Step 5: Create ClusterIssuer YAML File</p> <ul> <li>Create ClusterIssuer using Terraform</li> <li>Initialize Terraform</li> <li>Create a Terraform execution plan</li> <li>Apply a Terraform execution plan</li> </ul> </li> <li> <p>Step 6: Test the Application with HTTPS URL</p> </li> </ol>"},{"location":"kubernetes/5-cert-manager/#architecture-diagram","title":"Architecture Diagram","text":"<p>Below is a high-level architecture diagram depicting the components involved in Cert-Manager:</p> <p>[Diagram Placeholder - Working on it...]</p> <p></p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show --output table\n\n# Lists all available Azure subscriptions\naz account list --output table\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre></p>"},{"location":"kubernetes/5-cert-manager/#implementation-details","title":"Implementation Details","text":"<p>The steps given below will guide you through the process of setting up and using Cert-Manager in your AKS cluster. By the end of this exercise, you'll have a functional environment for automating the management of TLS certificates within Kubernetes.</p>"},{"location":"kubernetes/5-cert-manager/#step-1-configure-terraform-providers","title":"Step-1: Configure Terraform providers","text":"<p>Launch Visual Studio Code and open your current Terraform repository to start working on your Terraform configuration.</p> <p>To install Helm charts using Terraform configuration, we need the following Terraform providers:</p> <ul> <li>Helm provider</li> <li>Kubernetes provider</li> <li>Kubectl provider</li> </ul> <p>The Helm Provider allows you to manage your Helm charts and releases as part of your Terraform-managed infrastructure. It lets you define your charts as Terraform resources, simplifying their installation and updates.</p> <p>With Terraform, you can handle the installation, upgrades, and removal of your Helm charts in a consistent, version-controlled way. This helps streamline infrastructure management, ensuring uniformity and reducing the chance of errors.</p> <p>Terraform Providers</p> <p>You can install these providers by adding the following code to your Terraform configuration file. Update the existing provider.tf file with these new providers:</p> provider.tf<pre><code>terraform {\n\n  required_version = \"&gt;=0.12\"\n\n  required_providers {\n    azurerm = {\n      source  = \"hashicorp/azurerm\"\n      version = \"~&gt;3.69.0\" //\"~&gt;2.0\"\n    }\n\n    azuread = {\n      version = \"&gt;= 2.26.0\" // https://github.com/terraform-providers/terraform-provider-azuread/releases\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"&gt;= 2.0.3\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"&gt;= 2.1.0\"\n    }\n\n    kubectl = {\n      source  = \"gavinbunney/kubectl\"\n      version = \"&gt;= 1.7.0\"\n    }\n  }\n}\n\nprovider \"random\" {}\n\nprovider \"azurerm\" {\n  features {}\n  skip_provider_registration = true\n  subscription_id            = var.sp-subscription-id\n  client_id                  = var.sp-client-id\n  client_secret              = var.sp-client-secret\n  tenant_id                  = var.sp-tenant-id\n}\n\nprovider \"kubernetes\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  #load_config_file       = false\n}\n\nprovider \"helm\" {\n  debug = true\n  kubernetes {\n    host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n\n  }\n}\nprovider \"kubectl\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  load_config_file       = false\n}\n</code></pre> <p>You can find more information about these providers by referring to their respective documentation:</p> <ul> <li>Helm Provider: More info</li> <li>Kubernetes Provider: More info</li> <li>Kubectl Provider: More info</li> </ul> <p>Run terraform init</p> <p>After adding these new providers to the Terraform providers list, you need to run <code>terraform init</code> to initialize them:</p> <pre><code>terraform init\n</code></pre> <p>output <pre><code>Initializing the backend...\n\nInitializing provider plugins...\n- Reusing previous version of hashicorp/azurerm from the dependency lock file\n- Reusing previous version of hashicorp/azuread from the dependency lock file\n- Finding hashicorp/kubernetes versions matching \"&gt;= 2.0.3\"...\n- Reusing previous version of hashicorp/random from the dependency lock file\n- Finding hashicorp/helm versions matching \"&gt;= 2.1.0\"...\n- Finding gavinbunney/kubectl versions matching \"&gt;= 1.7.0\"...\n- Installing hashicorp/kubernetes v2.18.1...\n- Installed hashicorp/kubernetes v2.18.1 (signed by HashiCorp)\n- Using previously-installed hashicorp/random v3.4.3\n- Installing hashicorp/helm v2.9.0...\n- Installed hashicorp/helm v2.9.0 (signed by HashiCorp)\n- Installing gavinbunney/kubectl v1.14.0...\n- Installed gavinbunney/kubectl v1.14.0 (self-signed, key ID AD64217B5ADD572F)\n- Using previously-installed hashicorp/azurerm v3.31.0\n- Using previously-installed hashicorp/azuread v2.33.0\n\n\nTerraform has been successfully initialized!\n</code></pre></p> <p>This command initializes the provider plugins and ensures they are ready for use in your Terraform project.</p> <p>Once the initialization process is complete, your Terraform project will be set up with the necessary providers for managing Helm charts and Kubernetes resources.</p>"},{"location":"kubernetes/5-cert-manager/#step-2-create-a-new-namespace-for-cert-mananger","title":"Step-2: Create a new namespace for Cert Mananger","text":"<p>Creating a separate namespace for cert-manager is important to keep all cert-manager related AKS resources organized in a single space, logically distinct from other Kubernetes resources. Here's how to achieve that:</p> <p>Create the Namespace</p> <p>Let's start by creating a file named cert-manager.tf and paste the following Terraform configuration:</p> <p>cert-manager.tf<pre><code># create namespace for cert mananger\nresource \"kubernetes_namespace\" \"cert_manager\" {\n  metadata {\n    labels = {\n      \"name\" = \"cert-manager\"\n    }\n    name = \"cert-manager\"\n  }\n}\n</code></pre> Validate and Format</p> <p>Run these commands to validate and format your Terraform configuration:</p> <pre><code>terraform validate\nterraform fmt\n</code></pre> <p>Plan the Changes</p> <p><pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> The output will list the changes Terraform is going to make, including creating the cert-manager namespace:</p> <p><pre><code> + create\n\nTerraform will perform the following actions:\n\n# kubernetes_namespace.cert_manager will be created\n+ resource \"kubernetes_namespace\" \"cert_manager\" {\n    + id                               = (known after apply)\n    + wait_for_default_service_account = false\n\n    + metadata {\n        + generation       = (known after apply)\n        + labels           = {\n            + \"name\" = \"cert-manager\"\n          }\n        + name             = \"cert-manager\"\n        + resource_version = (known after apply)\n        + uid              = (known after apply)\n      }\n  }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> Apply the Changes</p> <p>Apply the planned changes using the following command:</p> <p><pre><code>terraform apply dev-plan\n</code></pre> <pre><code>Apply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre></p> <p>After applying, you'll see that Terraform has added the cert-manager namespace. This isolation helps keep your cert-manager resources organized and distinguishable from others within your Kubernetes cluster.</p>"},{"location":"kubernetes/5-cert-manager/#step-3-install-cert-manager-helm-chart-using-terraform","title":"Step-3: Install cert-manager helm chart using terraform","text":"<p>Now, let's look into installing the cert-manager Helm chart into your AKS cluster using Terraform. Here's the step-by-step process:</p> <p>1. Add the Helm repository</p> <p>First, you need to add the Helm repository for cert-manager. The official source for cert-manager charts is the Helm repository provided by Jetstack. Adding the repository is essential for accessing the chart securely:</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre> <p>2. Update your local Helm chart repository cache:</p> <p>Make sure to update your local Helm chart repository cache to get the latest information.</p> <pre><code>helm repo update\n</code></pre> <p>3. Install the Cert-Manager Helm Chart</p> <p>To install the cert-manager Helm chart, you'll need its details. These details can be found on the ArtifactHUB website. Navigate to the cert-manager chart page and click the Install button to get the required information.</p> <p>https://artifacthub.io/packages/helm/cert-manager/cert-manager - click on Install button and get following helm-chart details</p> <pre><code>repository = \"https://charts.jetstack.io\"\nchart      = \"cert-manager\"\nversion    = \"1.12.3\"\n</code></pre> <p>In your Terraform configuration, create a new resource using the <code>helm_release</code> block:</p> cert-manager.tf<pre><code># Install cert-manager helm chart using terraform\nresource \"helm_release\" \"cert_manager\" {\n  name       = \"cert-manager\"\n  repository = \"https://charts.jetstack.io\"\n  chart      = \"cert-manager\"\n  version    = \"v1.12.3\"\n  namespace  = kubernetes_namespace.cert_manager.metadata.0.name\n\n  set {\n    name  = \"installCRDs\"\n    value = \"true\"\n  }\n  depends_on = [\n    kubernetes_namespace.cert_manager    \n  ]\n}\n</code></pre> <p>Plan and Apply</p> <p>Run the following commands to validate, plan, and apply the installation of the cert-manager Helm chart using Terraform:</p> <p><pre><code>terraform validate\nterraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\nterraform apply dev-plan\n</code></pre> output <pre><code> + create\n\nTerraform will perform the following actions:\n\n# helm_release.cert_manager will be created\n+ resource \"helm_release\" \"cert_manager\" {\n    + atomic                     = false\n    + chart                      = \"cert-manager\"\n    + cleanup_on_fail            = false\n    + create_namespace           = false\n    + dependency_update          = false\n    + disable_crd_hooks          = false\n    + disable_openapi_validation = false\n    + disable_webhooks           = false\n    + force_update               = false\n    + id                         = (known after apply)\n    + lint                       = false\n    + manifest                   = (known after apply)\n    + max_history                = 0\n    + metadata                   = (known after apply)\n    + name                       = \"cert-manager\"\n    + namespace                  = \"cert-manager\"\n    + pass_credentials           = false\n    + recreate_pods              = false\n    + render_subchart_notes      = true\n    + replace                    = false\n    + repository                 = \"https://charts.jetstack.io\"\n    + reset_values               = false\n    + reuse_values               = false\n    + skip_crds                  = false\n    + status                     = \"deployed\"\n    + timeout                    = 300\n    + verify                     = false\n    + version                    = \"v1.12.3\"\n    + wait                       = true\n    + wait_for_jobs              = false\n\n    + set {\n        + name  = \"installCRDs\"\n        + value = \"true\"\n      }\n  }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre></p> <p>Check the plan output to ensure everything is as expected.</p> <p>Apply the plan to actually install the cert-manager Helm chart:</p> <p><pre><code>terraform apply dev-plan\n</code></pre> <pre><code>helm_release.cert_manager: Creating...\nhelm_release.cert_manager: Still creating... [10s elapsed]\nhelm_release.cert_manager: Still creating... [20s elapsed]\nhelm_release.cert_manager: Still creating... [30s elapsed]\nhelm_release.cert_manager: Still creating... [40s elapsed]\nhelm_release.cert_manager: Still creating... [50s elapsed]\nhelm_release.cert_manager: Creation complete after 59s [id=cert-manager]\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre></p> <p>Terraform will take care of installing the cert-manager Helm chart with the specified version into your AKS cluster. This process will provide the necessary infrastructure to manage and issue TLS certificates within your Kubernetes environment.</p>"},{"location":"kubernetes/5-cert-manager/#step-4-verify-cert-manager-resources-in-aks","title":"Step 4. Verify cert-manager resources in AKS.","text":"<p>After successfully installing the cert-manager Helm chart in your AKS cluster, the next step is to verify the installation. This involves checking the deployed resources and ensuring everything is running as expected. Here's how you can verify the cert-manager installation:</p> <p>Connect to AKS cluster</p> <p>Use the Azure CLI to connect to your AKS cluster:</p> <pre><code># azure CLI\n\n# Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n</code></pre> <p>Get Cert-Manager Resources</p> <p>Run the following commands to get information about the cert-manager resources:</p> <pre><code>kubectl get namespace cert-manager\nkubectl get deployments -n cert-manager\nkubectl get pods -n cert-manager\nkubectl get services -n cert-manager\nkubectl get configmaps -n cert-manager\nkubectl get secrets -n cert-manager\n</code></pre> <p>Get all resources related to cert-manager.</p> <p>Run the following command to get a comprehensive list of all resources related to cert-manager:</p> <p><pre><code>kubectl get all -n cert-manager\n</code></pre> <pre><code>NAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-5674b9b755-vgpxw              1/1     Running   0          15m\npod/cert-manager-cainjector-557c547f54-n748g   1/1     Running   0          15m\npod/cert-manager-webhook-86868b95db-lgg8d      1/1     Running   0          15m\n\nNAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/cert-manager           ClusterIP   10.25.117.167   &lt;none&gt;        9402/TCP   16m\nservice/cert-manager-webhook   ClusterIP   10.25.63.104    &lt;none&gt;        443/TCP    16m\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           16m\ndeployment.apps/cert-manager-cainjector   1/1     1            1           16m\ndeployment.apps/cert-manager-webhook      1/1     1            1           16m\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-5674b9b755              1         1         1       16m\nreplicaset.apps/cert-manager-cainjector-557c547f54   1         1         1       16m\nreplicaset.apps/cert-manager-webhook-86868b95db      1         1         1       16m\n</code></pre></p> <p>Get Configmaps Details</p> <pre><code>kubectl get configmaps -n cert-manager\n</code></pre> <pre><code>NAME                   DATA   AGE\ncert-manager-webhook   0      16m\nkube-root-ca.crt       1      30m\n</code></pre> <p>Get Secrets</p> <p><pre><code>kubectl get secrets -n cert-manager\n</code></pre> <pre><code>NAME                                 TYPE                 DATA   AGE\ncert-manager-webhook-ca              Opaque               3      17m\nsh.helm.release.v1.cert-manager.v1   helm.sh/release.v1   1      17m\n</code></pre></p> <p>Get Ingress</p> <pre><code>kubectl get ingress -n cert-manager\n</code></pre> <p>output</p> <pre><code>No resources found in cert-manager namespace.\n</code></pre> <p>get CRDs</p> <pre><code>kubectl get clusterrole -n cert-manager\nkubectl get clusterrolebinding -n cert-manager\nkubectl get CustomResourceDefinition -n cert-manager\n\nor\nkubectl get crd -l app.kubernetes.io/name=cert-manager\n</code></pre> <pre><code>NAME                                  CREATED AT\ncertificaterequests.cert-manager.io   2023-08-27T15:11:41Z\ncertificates.cert-manager.io          2023-08-27T15:11:41Z\nchallenges.acme.cert-manager.io       2023-08-27T15:11:41Z\nclusterissuers.cert-manager.io        2023-08-27T15:11:41Z\nissuers.cert-manager.io               2023-08-27T15:11:42Z\norders.acme.cert-manager.io           2023-08-27T15:11:41Z\n</code></pre> <p>The output from these commands will provide you with a detailed overview of the cert-manager resources that are deployed in your AKS cluster. If all the resources are shown as \"Running,\" \"Ready,\" or \"Available,\" it indicates that the cert-manager installation was successful. This verification step ensures that cert-manager is ready for use, and you can proceed with deploying applications in your AKS cluster while enjoying the benefits of automatic certificate management.</p>"},{"location":"kubernetes/5-cert-manager/#step-5-create-clusterissuer-yaml-file","title":"Step-5: Create clusterissuer YAML file","text":"<p>The purpose of a ClusterIssuer in AKS (Azure Kubernetes Service) is to manage the issuance and renewal of TLS certificates for an AKS cluster. It defines the certificate authority that will be used to issue and sign the certificates, such as Let's Encrypt, and it can be used to configure additional properties such as the email address for certificate expiration notifications. By using a ClusterIssuer, you can automate the process of obtaining and renewing certificates for your AKS cluster, ensuring that your applications continue to use valid certificates without manual intervention.</p> clusterissuer.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: anjkeesari@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n          podTemplate:\n            spec:\n              nodeSelector:\n                \"kubernetes.io/os\": linux\n\n#kubectl apply -f cert-manager/clusterissuer-nginx.yaml -n sample\n</code></pre> <p>loading cluster issuer file using terraform</p> cert_manager.tf<pre><code>locals {\n  clusterissuer = \"cert-manager/clusterissuer-nginx.yaml\"\n}\n\n# Create clusterissuer for nginx YAML file\ndata \"kubectl_file_documents\" \"clusterissuer\" {\n  content = file(local.clusterissuer)\n}\n</code></pre> <p>Create ClusterIssuer using terraform</p> <p>cert_manager.tf<pre><code># Apply clusterissuer for nginx YAML file\nresource \"kubectl_manifest\" \"clusterissuer\" {\n  for_each  = data.kubectl_file_documents.clusterissuer.manifests\n  yaml_body = each.value\n  depends_on = [\n    data.kubectl_file_documents.clusterissuer\n  ]\n}\n</code></pre> Testing</p> <p>describe clusterissuer </p> <pre><code>kubectl describe clusterissuer letsencrypt\n</code></pre> <p>Output</p> <pre><code>Name:         letsencrypt\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n .\n .\n .\nStatus:\n  Acme:\n    Last Private Key Hash:  XyQWbNfKPyMk77tcC5HPrjudqf/Fnl7YbHCPqniQz0Y=\n    Last Registered Email:  anjkeesari@gmail.com\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/1278195356\n  Conditions:\n    Last Transition Time:  2023-08-27T19:25:54Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/5-cert-manager/#step-6-test-the-application-with-https-urls","title":"Step-6: Test the application with https URLs","text":"<p>In this step, we'll test our application using HTTPS URLs to ensure that the certificate management provided by cert-manager and Let's Encrypt is functioning correctly. Follow these steps to test the application:</p> <p>Use the <code>deployment.yaml</code> file for your aspnet-api Microservice deployment. Apply it to the 'sample' namespace:</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n\n# kubectl apply -f deployment.yaml -n sample\n</code></pre> <p>Modify the <code>service.yaml</code> file for the aspnet-api Microservice by changing the <code>ClusterIP</code> to <code>LoadBalancer</code>. Apply the changes to the 'sample' namespace:</p> <p>service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector: \n    app: aspnet-api\n\n# kubectl apply -f service.yaml -n sample\n</code></pre> After applying the modified service.yaml file, you'll be able to access the Swagger API URL from the internet. Currently, this URL is not secure (HTTP). We'll make this URL secure (HTTPS) using cert-manager and Let's Encrypt.</p> <p></p> <p>Note</p> <p>I'm skipping the DNS configuration in this lab, assuming you've already configured the custom domain in your DNS settings, and it's ready to be used for this exercise.</p> <p>Add the following annotations to the ingress.yaml file for the sample Ingress resource:</p> <p>annotations</p> <p><pre><code>  cert-manager.io/cluster-issuer: letsencrypt\n  cert-manager.io/acme-challenge-type: http01\n</code></pre> spec-&gt;tls</p> <pre><code>  tls:\n  - hosts:\n    - dev.k8s.anjikeesari.com\n    secretName: tls-secret\n</code></pre> <p>Here is the complete ingress YAML file</p> ingress.yaml<pre><code># This ingress used for api calls.\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: sample\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    # appgw.ingress.kubernetes.io/backend-path-prefix: \"/\"\n    cert-manager.io/cluster-issuer: letsencrypt\n    appgw.ingress.kubernetes.io/ssl-redirect: \"true\"\n    cert-manager.io/acme-challenge-type: http01\n    appgw.ingress.kubernetes.io/health-probe-status-codes: \"200-499\"\nspec:\n  tls:\n  - hosts:\n    - dev.k8s.anjikeesari.com\n    secretName: tls-secret\n  rules:\n    - host: dev.k8s.anjikeesari.com\n      http:\n        paths:\n          - path: /aspnet/api/* # aspnet svc rest api calls\n            pathType: Prefix\n            backend:\n              service:\n                name: aspnet-api\n                port:\n                  number: 80\n# status:\n#   loadBalancer:\n#     ingress:\n#       - ip: 20.221.242.197\n# kubectl apply -f ingress.yaml -n sample\n</code></pre> <p>Test the application using custom domain URLs with the HTTPS protocol. Access the following URL in your browser:</p> <p>https://dev.k8s.anjikeesari.com/swagger/index.html</p> <p>You'll notice that the certificate used is provided by Let's Encrypt. This indicates that the cert-manager and Let's Encrypt integration is working correctly.</p> <p>By following these steps, you've successfully configured your application to use HTTPS URLs and demonstrated that the certificate issuance and management process with cert-manager and Let's Encrypt is functioning as intended.</p> <p></p>"},{"location":"kubernetes/5-cert-manager/#reference","title":"Reference:","text":"<p>Here is a list of resources that were used as references during the development of this technical scenario:</p> <ol> <li>Cert-Manager Documentation - Installing with Helm</li> <li>Artifact Hub - Cert-Manager Helm Chart</li> <li>Azure AKS Ingress TLS Documentation</li> <li>Setup Let's Encrypt ClusterIssuer with Terraform</li> </ol>"},{"location":"kubernetes/5.1-issue-cert/","title":"Issue the Let's Encrypt SSL Certificate to the Website","text":""},{"location":"kubernetes/5.1-issue-cert/#introduction","title":"Introduction","text":"<p>Ensuring the security of your website is crucial, and implementing HTTPS through a valid SSL/TLS certificate is a important step. In this lab, we'll use Let's Encrypt, a trusted Certificate Authority (CA) known for providing free certificates.</p> <p>This tutorial will show you how to obtain a Cert-Manager Certificate using Let's Encrypt for our Argocd website's domain URL. We'll use Terraform, a powerful infrastructure-as-code tool, to simplify and automate the HTTPS implementation.</p>"},{"location":"kubernetes/5.1-issue-cert/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Cloud Engineer</code>, your task is to secure the domain URL of your ArgoCD deployment in a Kubernetes environment. The challenge is not only to obtain certificates from reputable authorities like Let's Encrypt but also to ensure their fully automated renewal every three months.</p>"},{"location":"kubernetes/5.1-issue-cert/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding with this exercise, ensure you have the following in place:</p> <ul> <li>An active Azure subscription Link</li> <li>Terraform installed and configured Guide</li> <li>Azure CLI installed Guide</li> <li>Kubectl installed and set up Guide</li> <li>A Functioning Kubernetes cluster</li> <li>Setup Cert-Manager in AKS using Terraform</li> <li>Installed ArgoCD in AKS cluster</li> </ul>"},{"location":"kubernetes/5.1-issue-cert/#objective","title":"Objective","text":"<p>In this exercise, we will cover the following steps to implement issue the Let's Encrypt SSL Certificate to the Website:</p> <ol> <li> <p>Step-1: Install Cert-Manager</p> </li> <li> <p>Step-2: Create Clusterissuer Issuer YAML file</p> </li> <li> <p>Step 3: Veryfiy ArgoCD Pods status in AKS cluster</p> </li> <li> <p>Step 4: Configure Ingress for ArgoCD</p> </li> <li> <p>Step 5: Add new DNS zone record set for Argocd custom domain</p> </li> <li> <p>Step 6: Verify Certificates</p> </li> <li> <p>Step 7: Access ArgoCD via HTTPS</p> </li> </ol> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show --output table\n\n# Lists all available Azure subscriptions\naz account list --output table\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>To interact with your Azure Kubernetes Service (AKS) cluster, you need to establish a connection. Depending on your role, you can use either the User or Admin credentials:</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# verify the aks connection by running following commands\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#implementation-details","title":"Implementation Details","text":"<p>The steps given below will guide you through the process of setting up and using Cert-Manager in our AKS cluster. By the end of this exercise, you'll have a functional environment for automating the management of TLS certificates within Kubernetes.</p>"},{"location":"kubernetes/5.1-issue-cert/#step-1-install-cert-manager","title":"Step-1: Install Cert-Manager","text":"<p>Cert-Manager is a Kubernetes add-on that helps with the management of certificates. In our previous lab, we've already created cert-manager using terraform. Setup Cert-Manager in AKS using Terraform</p> <pre><code>kubectl get all -n cert-manager\n\n#output\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/cert-manager-5674b9b755-97hvg              1/1     Running   0          32d\npod/cert-manager-cainjector-557c547f54-7hh2r   1/1     Running   0          32d\npod/cert-manager-webhook-86868b95db-sqddc      1/1     Running   0          32d\n\nNAME                           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/cert-manager           ClusterIP   10.25.117.167   &lt;none&gt;        9402/TCP   88d\nservice/cert-manager-webhook   ClusterIP   10.25.63.104    &lt;none&gt;        443/TCP    88d\n\nNAME                                      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/cert-manager              1/1     1            1           88d\ndeployment.apps/cert-manager-cainjector   1/1     1            1           88d\ndeployment.apps/cert-manager-webhook      1/1     1            1           88d\n\nNAME                                                 DESIRED   CURRENT   READY   AGE\nreplicaset.apps/cert-manager-5674b9b755              1         1         1       88d\nreplicaset.apps/cert-manager-cainjector-557c547f54   1         1         1       88d\nreplicaset.apps/cert-manager-webhook-86868b95db      1         1         1       88d\n</code></pre> <p>Now, let's streamline the process further by Issue the Let's Encrypt SSL Certificate to one our ArgoCD Website .</p>"},{"location":"kubernetes/5.1-issue-cert/#step-2-create-clusterissuer-yaml-file","title":"Step-2: Create Clusterissuer YAML file","text":"<p>The purpose of a ClusterIssuer in AKS (Azure Kubernetes Service) is to manage the issuance and renewal of TLS certificates for an AKS cluster.</p> <p>Create a file `clusterissuer-nginx.yaml`` with the following content:</p> clusterissuer-nginx.yaml<pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: anjkeesari@gmail.com\n    privateKeySecretRef:\n      name: letsencrypt\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n          podTemplate:\n            spec:\n              nodeSelector:\n                \"kubernetes.io/os\": linux\n\n#kubectl apply -f cert-manager/clusterissuer.yaml -n sample\n</code></pre> <p>loading cluster issuer file using terraform</p> cert_manager.tf<pre><code>locals {\n  clusterissuer = \"cert-manager/clusterissuer-nginx.yaml\"\n}\n\n# Create clusterissuer for nginx YAML file\n\ndata \"kubectl_file_documents\" \"clusterissuer\" {\n  content = file(local.clusterissuer)\n}\n</code></pre> <p>Create ClusterIssuer using terraform</p> <p>cert_manager.tf<pre><code># Apply clusterissuer for nginx YAML file\n\nresource \"kubectl_manifest\" \"clusterissuer\" {\n  for_each  = data.kubectl_file_documents.clusterissuer.manifests\n  yaml_body = each.value\n  depends_on = [\n    data.kubectl_file_documents.clusterissuer\n  ]\n}\n</code></pre> Testing</p> <p>Get more information about clusterissuer:</p> <pre><code>kubectl describe clusterissuer letsencrypt\n</code></pre> <p>Output</p> <pre><code>Name:         letsencrypt\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n .\n .\n .\nStatus:\n  Acme:\n    Last Private Key Hash:  XyQWbNfKPyMk77tcC5HPrjudqf/Fnl7YbHCPqniQz0Y=\n    Last Registered Email:  anjkeesari@gmail.com\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/1278195356\n  Conditions:\n    Last Transition Time:  2023-08-27T19:25:54Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#step-3-veryfiy-argocd-pods-status-in-aks-cluster","title":"Step-3: Veryfiy ArgoCD Pods status in AKS cluster","text":"<p>The installation of ArgoCD in our AKS environment involves distinct steps, and the process has different step. Installing ArgoCD in AKS is done in this lab - Install ArgoCD CLI</p> <p>Now, let's ensure the smooth functioning of ArgoCD by verifying the status of its pods within the AKS cluster before issuing the let's encrypt SSL certificate to the website.</p> <pre><code>kubectl get all -n argocd\n\n# output\nNAME                                                    READY   STATUS    RESTARTS   AGE\npod/argocd-application-controller-0                     1/1     Running   0          31d\npod/argocd-applicationset-controller-848fc4dcfb-8bxvb   1/1     Running   0          32d\npod/argocd-dex-server-56888697cd-652xv                  1/1     Running   0          32d\npod/argocd-notifications-controller-5cd6fc4886-7ds87    1/1     Running   0          32d\npod/argocd-redis-b54b4ccd8-pgtd5                        1/1     Running   0          32d\npod/argocd-repo-server-78998f9d78-gfskb                 1/1     Running   0          32d\npod/argocd-server-c799cf854-v8tjn                       1/1     Running   0          32d\n\nNAME                                       TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE\nservice/argocd-applicationset-controller   ClusterIP      10.25.250.229   &lt;none&gt;          7000/TCP                     263d\nservice/argocd-dex-server                  ClusterIP      10.25.247.199   &lt;none&gt;          5556/TCP,5557/TCP            263d\nservice/argocd-redis                       ClusterIP      10.25.211.159   &lt;none&gt;          6379/TCP                     263d\nservice/argocd-repo-server                 ClusterIP      10.25.233.23    &lt;none&gt;          8081/TCP                     263d\nservice/argocd-server                      LoadBalancer   10.25.115.123   20.124.172.79   80:30119/TCP,443:30064/TCP   263d\n\nNAME                                               READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/argocd-applicationset-controller   1/1     1            1           263d\ndeployment.apps/argocd-dex-server                  1/1     1            1           263d\ndeployment.apps/argocd-notifications-controller    1/1     1            1           263d\ndeployment.apps/argocd-redis                       1/1     1            1           263d\ndeployment.apps/argocd-repo-server                 1/1     1            1           263d\ndeployment.apps/argocd-server                      1/1     1            1           263d\n\nNAME                                                          DESIRED   CURRENT   READY   AGE\nreplicaset.apps/argocd-applicationset-controller-848fc4dcfb   1         1         1       263d\nreplicaset.apps/argocd-dex-server-56888697cd                  1         1         1       263d\nreplicaset.apps/argocd-notifications-controller-5cd6fc4886    1         1         1       263d\nreplicaset.apps/argocd-redis-b54b4ccd8                        1         1         1       263d\nreplicaset.apps/argocd-repo-server-78998f9d78                 1         1         1       263d\nreplicaset.apps/argocd-server-c799cf854                       1         1         1       263d\n\nNAME                                             READY   AGE\nstatefulset.apps/argocd-application-controller   1/1     263d\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#step-4-configure-ingress-for-argocd","title":"Step-4: Configure Ingress for ArgoCD","text":"<p>Ensure that ArgoCD is accessible via an Ingress. Create a file <code>argocd-ingress.yaml</code>:</p> <p>Added the following annotations to the regular ingress file to make it HTTPS.</p> <p>annotations</p> <p><pre><code>  appgw.ingress.kubernetes.io/ssl-redirect: \"true\"\n  cert-manager.io/cluster-issuer: letsencrypt\n  cert-manager.io/acme-challenge-type: http01\n</code></pre> spec-&gt;tls</p> <pre><code>  tls:\n  - hosts:\n    - argocd.yourdomain.com\n    secretName: tls-secret\n</code></pre> <p>Here is the sample Application Gateway Ingress controller YAML file with SSL enabled:</p> argocd-ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: argocd\n  namespace: argocd\n  annotations:\n    kubernetes.io/ingress.class: azure/application-gateway\n    appgw.ingress.kubernetes.io/backend-path-prefix: \"/\"\n    appgw.ingress.kubernetes.io/health-probe-status-codes: \"200-499\"\n    appgw.ingress.kubernetes.io/ssl-redirect: \"true\"\n    cert-manager.io/cluster-issuer: letsencrypt\n    cert-manager.io/acme-challenge-type: http01\nspec:\n  tls:\n  - hosts:\n    - argocd.yourdomain.com\n    secretName: tls-secret\n  rules:\n    - host: argocd.yourdomain.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:   \n                  number: 80\n          - path: /argocd/*\n            pathType: Prefix\n            backend:\n              service:\n                name: argocd-server\n                port:\n                 number: 80\nstatus:\n  loadBalancer:\n    ingress:\n      - ip: 20.100.100.100\n\n# kubectl apply -f argocd/argocd-ingress.yaml -n argocd\n</code></pre> <p>loading argocd-ingress file using terraform</p> helm_argocd.tf<pre><code>locals {\n  argocd_ingress = \"argocd/argocd-ingress.yaml\"\n}\n\n# Create argocd ingress file\ndata \"kubectl_file_documents\" \"argocd_ingress\" {\n  content = file(local.argocd_ingress)\n}\n</code></pre> <p>Create argocd ingress using terraform</p> helm_argocd.tf<pre><code># Apply argocd ingress file\nresource \"kubectl_manifest\" \"argocd_ingress\" {\n  for_each  = data.kubectl_file_documents.argocd_ingress.manifests\n  yaml_body = each.value\n  depends_on = [\n    data.kubectl_file_documents.argocd_ingress\n  ]\n}\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#step-5-add-new-record-in-azure-dns-zone-for-argocd-custom-domain","title":"Step-5: Add new record in Azure DNS Zone for Argocd custom domain","text":"<p>Follow these steps to add Records sets from Azure portal</p> <ul> <li>Inside the Azure DNS Zone, click on \"+Records sets\" button.</li> <li>Configure the new record set in \"Add record set\"</li> <li>Name: Enter the subdomain or record name (e.g., argocd).</li> <li>Type: Choose the record type (e.g., A for IP address of argocd).</li> <li>TTL (Time to Live): Set the TTL for the record.</li> <li>IP Address or Alias: Enter the IP address of argocd.</li> <li>Click on \"OK\" to create the new record.</li> </ul> <p>CLI commands to add records sets</p> <pre><code># add-record example\naz network dns record-set a add-record --ipv4-address \"20.100.100.100\" --record-set-name \"argocd\" --resource-group \"rg-publicdns-dev\" --zone-name \"yourdomain.com\"\n\n# delete-record example\naz network dns record-set a delete --name \"argocd\" --resource-group \"rg-publicdns-dev\" --zone-name \"yourdomain.com\" --yes\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#step-6-verify-certificates","title":"Step-6: Verify Certificates","text":"<p>Check if the certificate has been successfully obtained. It might take a few minutes:</p> <pre><code>kubectl get certificate -n argocd\n\n# output\nNAME         READY   SECRET       AGE\ntls-secret   True    tls-secret   85d\n</code></pre> <p>Describe certificate to see the details, you will also notice this message <code>Certificate is up to date and has not expired</code></p> <pre><code>kubectl describe certificate/tls-secret -n argocd\n\nName:         tls-secret\nNamespace:    argocd\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n .\n .\n .\nSpec:\n  Dns Names:\n      argocd.yourdomain.com\n  Issuer Ref:\n    Group:      cert-manager.io\n    Kind:       ClusterIssuer\n    Name:       letsencrypt\n  Secret Name:  tls-secret\n  Usages:\n    digital signature\n    key encipherment\nStatus:\n  Conditions:\n    Last Transition Time:  2023-08-29T22:33:37Z\n    Message:               Certificate is up to date and has not expired\n    Observed Generation:   1\n    Reason:                Ready\n    Status:                True\n    Type:                  Ready\n  Not After:               2024-01-26T20:34:10Z\n  Not Before:              2023-10-28T20:34:11Z\n  Renewal Time:            2023-12-27T20:34:10Z\n  Revision:                2\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/5.1-issue-cert/#step-7-access-argocd-via-https","title":"Step-7: Access ArgoCD via HTTPS","text":"<p>Once the certificate is issued, access your ArgoCD instance securely via HTTPS:</p> <p><pre><code>https://argocd.yourdomain.com\n</code></pre> Custom doamin with Valid SSL Certificate:</p> <p></p> <p>Certificate Issued By and validity period details:</p> <p></p> <p>You've successfully issued an SSL certificate to our ArgoCD website using Let's Encrypt.</p>"},{"location":"kubernetes/5.1-issue-cert/#reference","title":"Reference:","text":"<p>Here is a list of resources that were used as references during the development of this technical scenario:</p>"},{"location":"kubernetes/5.1-manifests-microservice-1/","title":"Creating YAML Manifests for Microservice-1 in AKS","text":""},{"location":"kubernetes/5.1-manifests-microservice-1/#introduction","title":"Introduction","text":"<p>In this lab, we will look into the process of creating plain YAML manifest files to deploy a microservice, specifically the \"aspnet-api,\" within an Azure Kubernetes Service (AKS) cluster. This practical exercise serves as an essential step towards understanding the fundamentals of Kubernetes configuration through YAML manifests. By manually applying these manifests using the <code>kubectl</code> command, we ensure that our microservice operates correctly within the AKS cluster. Moreover, this groundwork paves the way for future endeavors wherein we'll learn to encapsulate these configurations as Helm charts for streamlined deployment.</p>"},{"location":"kubernetes/5.1-manifests-microservice-1/#implementation-details","title":"Implementation Details","text":"<p>Throughout this exercise, we will accomplish the following objectives:</p> <ul> <li>Step-1: Create deployment manifest</li> <li>Step-2: Create service manifest</li> <li>Step-3: Create ingress manifest</li> <li>Step-4: Deploy manifest files in AKS</li> <li>Step-5: Test Microservice-1 running in AKS</li> </ul>"},{"location":"kubernetes/5.1-manifests-microservice-1/#step-1-create-deployment-manifest","title":"Step 1: Create Deployment Manifest","text":"<p>We'll begin by creating a deployment manifest for the \"aspnet-api\" microservice. This YAML file will outline the deployment's desired characteristics, such as the number of replicas, update strategy, and container specifications. By defining these attributes, we establish a clear blueprint for Kubernetes on how to deploy and manage our microservice.</p> <p>Here's an example of the deployment manifest:</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5\n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230323.15\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n</code></pre>"},{"location":"kubernetes/5.1-manifests-microservice-1/#step-2-create-service-manifest","title":"Step 2: Create Service Manifest","text":"<p>In this step, we'll craft a service manifest for the \"aspnet-api\" microservice. The service manifest specifies how external traffic should access our microservice. This YAML file defines the service's type, ports, and labels, enabling Kubernetes to manage network communication effectively.</p> <p>Here's an example of the service manifest:</p> service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: aspnet-api\n  namespace: sample\n  labels: {}\nspec:\n  type: ClusterIP\n  ports:\n    - port: 80\n      targetPort: 80\n      protocol: TCP\n      name: http\n  selector:\n    app: aspnet-api\n</code></pre>"},{"location":"kubernetes/5.1-manifests-microservice-1/#step-3-create-ingress-manifest","title":"Step 3: Create Ingress Manifest","text":"<p>This step involves creating an ingress manifest, which outlines how external traffic will be routed to our microservice within the AKS cluster. The ingress configuration includes details about the host, paths, and backend services, effectively handling the external access.</p> ingress.yaml<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n    nginx.ingress.kubernetes.io/use-regex: \"true\"\n    nginx.ingress.kubernetes.io/rewrite-target: /$2\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      - path: /aspnet/api/* # aspnet svc rest api calls\n        pathType: Prefix\n        backend:\n          service:\n            name: aspnet-api\n            port:\n              number: 80\nstatus:\n  loadBalancer:\n    ingress:\n      - ip: 20.62.213.32\n# kubectl apply -f ingress.yaml -n sample\n</code></pre>"},{"location":"kubernetes/5.1-manifests-microservice-1/#step-4-deploy-manifest-files-in-aks","title":"Step 4: Deploy Manifest Files in AKS","text":"<p>With our deployment, service, and potentially ingress manifests ready, it's time to apply these configurations to the AKS cluster. We can accomplish this by using the <code>kubectl apply</code> command along with the respective YAML files.</p> <pre><code>kubectl apply -f deployment.yaml -n sample\nkubectl apply -f service.yaml -n sample\nkubectl apply -f ingress.yaml -n sample\n</code></pre> <pre><code>NAME                                      READY   STATUS    RESTARTS   AGE\npod/aspnet-api-79b4cbf4bb-54cng           1/1     Running   0          18h\n\nNAME                         TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE\nservice/aspnet-api           LoadBalancer   10.25.69.201    20.62.213.32   80:32204/TCP   166d\n\nNAME                                 READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/aspnet-api           1/1     1            1           18h\n\nNAME                                            DESIRED   CURRENT   READY   AGE\nreplicaset.apps/aspnet-api-79b4cbf4bb           1         1         1       18h\n</code></pre>"},{"location":"kubernetes/5.1-manifests-microservice-1/#step-5-test-microservice-1-running-in-aks","title":"Step 5: Test Microservice-1 Running in AKS","text":"<p>Upon successfully applying the manifest files, we can test the functionality of our \"aspnet-api\" microservice within the AKS cluster. This step involves accessing the microservice through its designated URL and ensuring its proper operation.</p>"},{"location":"kubernetes/5.1-manifests-microservice-1/#conclusion","title":"Conclusion","text":"<p>Through this hands-on exercise, we've gained insight into defining deployments, services, and ingress using simple YAML files. This foundational knowledge forms the basis for further exploration, such as packaging these configurations as Helm charts for more efficient deployment in the future.</p>"},{"location":"kubernetes/5.2-cert-troubleshooting/","title":"Troubleshooting Problems with Let's Encrypt Certificates","text":""},{"location":"kubernetes/5.2-cert-troubleshooting/#introduction","title":"Introduction","text":"<p>In this guide, we'll explore troubleshooting techniques to identify the root cause if your Let's Encrypt certificate issuance or renewal encounters issues.</p> <p>login to Azure</p> <p>Before delving into troubleshooting, ensure you are logged into the correct Azure subscription using Visual Studio Code:</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show --output table\n\n# Lists all available Azure subscriptions\naz account list --output table\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to the AKS Cluster</p> <p>To interact with your Azure Kubernetes Service (AKS) cluster, you need to establish a connection. Depending on your role, you can use either the User or Admin credentials:</p> <p><pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# verify the aks connection by running following commands\nkubectl get no\nkubectl get namespace -A\n</code></pre> Certificate Management with cert-manager</p> <p>The troubleshooting process involves checking various resources within the cert-manager.</p> <p>There are several resources that are involved in requesting a certificate.</p> <p><pre><code>Ingress -&gt; \n    Certificate -&gt;\n                  CertificateRequest \nACME -&gt;\n    Order  -&gt; \n            Challenge  -&gt;\n</code></pre> The cert-manager flow all starts at a <code>Certificate</code> resource, you can create this yourself or your Ingress resource will to this for you</p>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-1-checking-the-certificate-resource","title":"Step-1: Checking the Certificate resource","text":"<p>Start by verifying the Certificate resource in the specified namespace (e.g., argocd):</p> <pre><code>kubectl get certificate -n argocd\n\n# output\nNAME         READY   SECRET       AGE\ntls-secret   True    tls-secret   85d\n</code></pre> <p>Look for the status of the certificates. If \"Ready\" is false, obtain more details:</p> <pre><code>kubectl describe certificate tls-secret -n argocd\n\n# output\nName:         tls-secret\nNamespace:    argocd\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         Certificate\nMetadata:\n.\n.\n.\nSpec:\n  Dns Names:\n      argocd.yourdomain.com\n  Issuer Ref:\n    Group:      cert-manager.io\n    Kind:       ClusterIssuer\n    Name:       letsencrypt\n  Secret Name:  tls-secret\n  Usages:\n    digital signature\n    key encipherment\nStatus:\n  Conditions:\n    Last Transition Time:  2023-08-29T22:33:37Z\n    Message:               Certificate is up to date and has not expired\n    Observed Generation:   1\n    Reason:                Ready\n    Status:                True\n    Type:                  Ready\n  Not After:               2024-01-26T20:34:10Z\n  Not Before:              2023-10-28T20:34:11Z\n  Renewal Time:            2023-12-27T20:34:10Z\n  Revision:                2\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-2-checking-the-certificaterequest","title":"Step-2: Checking the CertificateRequest","text":"<p>The CertificateRequest resource represents a Certificate Signing Request (CSR) in cert-manager:</p> <pre><code>kubectl get certificaterequest -n argocd\n\n# output\nNAME               APPROVED   DENIED   READY   ISSUER        REQUESTOR                                         AGE\ntls-secret-gvppb   True                True    letsencrypt   system:serviceaccount:cert-manager:cert-manager   85d\ntls-secret-px228   True                True    letsencrypt   system:serviceaccount:cert-manager:cert-manager   25d\n</code></pre> <p>Get more information about a specific CertificateRequest:</p> <pre><code>kubectl describe certificaterequest tls-secret-px228 -n argocd\n\n# output\nName:         tls-secret-px228\nNamespace:    argocd\nLabels:       &lt;none&gt;\nAnnotations:  cert-manager.io/certificate-name: tls-secret\n              cert-manager.io/certificate-revision: 2\n              cert-manager.io/private-key-secret-name: tls-secret-ctl62\nAPI Version:  cert-manager.io/v1\nKind:         CertificateRequest\nMetadata:\n .\n .\n .\nSpec:\n  Extra:\n    authentication.kubernetes.io/pod-name:\n      cert-manager-85f68f95f-928hm\n    authentication.kubernetes.io/pod-uid:\n      562560ea-32ab-4294-8eab-be3eca845dfa\n  Groups:\n    system:serviceaccounts\n    system:serviceaccounts:cert-manager\n    system:authenticated\n  Issuer Ref:\n    Group:  cert-manager.io\n    Kind:   ClusterIssuer\n    Name:   letsencrypt\n  Request:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2pUQ0NBWFVDQVFBd0FEQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUxyUQpkajJ5M3JGcUlxNGxiQXVKNCtWMCtnWHlEd1BHaUpPUXdoNG9NS00zOFJXZ2UzMHhqR1AwUnRVNEQ3Z2VxUFh5Ck5nVDR3cHphREU2UWk0S0t3endwZGV4MHovMUJQeWxtTlo3Um9GbDNJUTZkR2l2MTNLekFhK1BJbkdjK052N1EKZ0I5bDc0QTRBYWxWeTBmV3NITzNIcEI2RGJPRTUvbmRDOHJmTEhlMmE1SXdFR09pRGhoNFdrVjNmVVRvYkc3bwpOWEoydlhPZmJFekY2T3pPL1VTeEExSTBwUENkUTdKbUVQYytrTGw4WXlJczJKcFJNa0dybTF5TGpSV1hpVXFnCmVVY2NDck8yZVliUHdKbkJUckFrNjgwekVwTm5JV2QwTGlDWDIyaUFCSFY4OS9HTzlhM1crMWFaR3M0S0E5dXIKZ0VQem1ZSmVrblhzUlIxamdJTUNBd0VBQWFCSU1FWUdDU3FHU0liM0RRRUpEakU1TURjd0tBWURWUjBSQkNFdwpINElkWkdWMkxtRnlaMjlqWkM1bGQyMHpMbUZ6YzJWMGJXRnlheTV1WlhRd0N3WURWUjBQQkFRREFnV2dNQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFBL3lPcVZNdWIrazZHUzNnek5PcmQrT3ZGQ213Q2RWZ3g4NCtGT0gwVlUKb1luTTVSUWcrZmJyOUI2eDVHRzA0WG9mTVl5eUJJenZVQzd5QUg3djN4UkhOZGJVeVNteWp0NEpGVWFQcDJlcwpuSXdNMm9pSklxbEM4UEFyeG1PYlpTMkZWWTZrcTVVVXYvVHV6VTZHaUd0aTBCSmcrTFhySTQ2U2xrTHM1OEdHCm5hUVJubGRpTW51ZlN3b1VFVG1CcW5OMm0zTC94MmxBRTlTQjZFN0ZVZk9FNlI3SXBVRVpyN3doTFFwRno0VjQKQ3RmNmJaYU1tMmJiOVh6NkRQK2VaSW9obTRUc08vQ3B3ZUhkVVFoM3NCQ0l5MmNlRnk0VFhkL1UrYUptL1dXWgorelRlUHNSNnA1KzJQQ2xGWGM4bnhXR284RmxvT0xNZjVJNmRRS1FHQlpmTgotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K\n  UID:      da8c7f37-8646-44ad-a532-a780eef32c43\n  Usages:\n    digital signature\n    key encipherment\n  Username:  system:serviceaccount:cert-manager:cert-manager\nStatus:\n  Certificate:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZCekNDQSsrZ0F3SUJBZ0lTQThJZ01hMWc0eXBuaU9Ud2ZVMUh0ZW1TTUEwR0NTcUdTSWIzRFFFQkN3VUEKTURJeEN6QUpCZ05WQkFZVEFsVlRNUll3RkFZRFZRUUtFdzFNWlhRbmN5QkZibU55ZVhCME1Rc3dDUVlEVlFRRApFd0pTTXpBZUZ3MHlNekV3TWpneU1ETTBNVEZhRncweU5EQXhNall5TURNME1UQmFNQ2d4SmpBa0JnTlZCQU1UCkhXUmxkaTVoY21kdlkyUXVaWGR0TXk1aGMzTmxkRzFoY21zdWJtVjBNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUYKQUFPQ0FROEFNSUlCQ2dLQ0FRRUF1dEIyUGJMZXNXb2lyaVZzQzRuajVYVDZCZklQQThhSWs1RENIaWd3b3pmeApGYUI3ZlRHTVkvUkcxVGdQdUI2bzlmSTJCUGpDbk5vTVRwQ0xnb3JEUENsMTdIVFAvVUUvS1dZMW50R2dXWGNoCkRwMGFLL1hjck1CcjQ4aWNaejQyL3RDQUgyWHZnRGdCcVZYTFI5YXdjN2Nla0hvTnM0VG4rZDBMeXQ4c2Q3WnIKa2pBUVk2SU9HSGhhUlhkOVJPaHNidWcxY25hOWM1OXNUTVhvN003OVJMRURValNrOEoxRHNtWVE5ejZRdVh4agpJaXpZbWxFeVFhdWJYSXVORlplSlNxQjVSeHdLczdaNWhzL0FtY0ZPc0NUcnpUTVNrMmNoWjNRdUlKZmJhSUFFCmRYejM4WTcxcmRiN1Zwa2F6Z29EMjZ1QVEvT1pnbDZTZGV4RkhXT0Fnd0lEQVFBQm80SUNIekNDQWhzd0RnWUQKVlIwUEFRSC9CQVFEQWdXZ01CMEdBMVVkSlFRV01CUUdDQ3NHQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFNQmdOVgpIUk1CQWY4RUFqQUFNQjBHQTFVZERnUVdCQlROV2xpay8remRSUmFqbGtEZGxtVCtSbVJTOWpBZkJnTlZIU01FCkdEQVdnQlFVTHJNWHQxaFd5NjVRQ1VEbUg2K2RpeFRDeGpCVkJnZ3JCZ0VGQlFjQkFRUkpNRWN3SVFZSUt3WUIKQlFVSE1BR0dGV2gwZEhBNkx5OXlNeTV2TG14bGJtTnlMbTl5WnpBaUJnZ3JCZ0VGQlFjd0FvWVdhSFIwY0RvdgpMM0l6TG1rdWJHVnVZM0l1YjNKbkx6QW9CZ05WSFJFRUlUQWZnaDFrWlhZdVlYSm5iMk5rTG1WM2JUTXVZWE56ClpYUnRZWEpyTG01bGREQVRCZ05WSFNBRUREQUtNQWdHQm1lQkRBRUNBVENDQVFRR0Npc0dBUVFCMW5rQ0JBSUUKZ2ZVRWdmSUE4QUIyQUR0VGQzVStMYm1BVG9zd1d3YitRRHRuMkUvRDlNZTlBQTB0Y20vaCt0UVhBQUFCaTNnMQplQm9BQUFRREFFY3dSUUloQUpSM2hUY0ltTTRZUnZvT1FpK2NRS0tLS0tCg==\n  Conditions:\n    Last Transition Time:  2023-10-28T21:33:36Z\n    Message:               Certificate request has been approved by cert-manager.io\n    Reason:                cert-manager.io\n    Status:                True\n    Type:                  Approved\n    Last Transition Time:  2023-10-28T21:34:12Z\n    Message:               Certificate fetched from issuer successfully\n    Reason:                Issued\n    Status:                True\n    Type:                  Ready\n</code></pre>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-3-acme-troubleshooting","title":"Step-3: ACME Troubleshooting","text":"<p>ACME (e.g. Let's Encrypt) issuers have 2 additional resources inside cert-manager: <code>Orders</code> and <code>Challenges</code></p> <p>When requesting ACME certificates, cert-manager will create <code>Order</code> and <code>Challenges</code> to complete the request.</p>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-4-troubleshooting-clusterissuers","title":"Step-4: Troubleshooting (Cluster)Issuers","text":"<p>Ensure that the (Cluster)Issuer you're using is in a ready state</p> <pre><code>kubectl get clusterissuer\nkubectl get clusterissuer -n argocd \n\n# output\nNAME          READY   AGE\nletsencrypt   True    312d\n</code></pre> <p>If \"Ready\" is false, check the status:</p> <pre><code>kubectl describe clusterissuer letsencrypt\nkubectl describe clusterissuer letsencrypt -n argocd \n\n# output\nName:         letsencrypt\nNamespace:\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nAPI Version:  cert-manager.io/v1\nKind:         ClusterIssuer\nMetadata:\n .\n .\n .\nSpec:\n  Acme:\n    Email:            anji.keesari@assetmark.com\n    Preferred Chain:\n    Private Key Secret Ref:\n      Name:  letsencrypt\n    Server:  https://acme-v02.api.letsencrypt.org/directory\n    Solvers:\n      http01:\n        Ingress:\n          Class:  azure/application-gateway\n          Pod Template:\n            Metadata:\n            Spec:\n              Node Selector:\n                kubernetes.io/os:  linux\nStatus:\n  Acme:\n    Last Registered Email:  anji.keesari@assetmark.com\n    Uri:                    https://acme-v02.api.letsencrypt.org/acme/acct/819289237\n  Conditions:\n    Last Transition Time:  2023-01-14T22:43:04Z\n    Message:               The ACME account was registered with the ACME server\n    Observed Generation:   1\n    Reason:                ACMEAccountRegistered\n    Status:                True\n    Type:                  Ready\nEvents:                    &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-5-troubleshooting-orders","title":"Step-5: Troubleshooting Orders","text":"<p>When we run a describe on the <code>CertificateRequest</code> resource we see that an Order that has been created:</p> <pre><code>kubectl get certificaterequest -n argocd\n</code></pre> <p>Check the status of <code>Orders</code> created during the certificate request:</p> <pre><code>kubectl get order -A\nkubectl get order -n argocd\n\n# output\nNAME                        STATE   AGE\ntls-secret-gvppb-89069991   valid   85d\ntls-secret-px228-89069991   valid   25d\n</code></pre> <p>Orders are a request to an ACME instance to issue a certificate. By running kubectl describe order on a particular order, information can be gleaned about failures in the process:</p> <p>Once an Order is successful, you should see an event like the following:</p> <pre><code> kubectl describe order tls-secret-px228-89069991 -n argocd\n\n# output\nName:         tls-secret-px228-89069991\nNamespace:    argocd\nLabels:       &lt;none&gt;\nAnnotations:  cert-manager.io/certificate-name: tls-secret\n              cert-manager.io/certificate-revision: 2\n              cert-manager.io/private-key-secret-name: tls-secret-ctl62\nAPI Version:  acme.cert-manager.io/v1\nKind:         Order\nMetadata:\n .\n .\n .\nSpec:\n  Dns Names:\n    argocd.yourdomain.com\n  Issuer Ref:\n    Group:  cert-manager.io\n    Kind:   ClusterIssuer\n    Name:   letsencrypt\n  Request:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ2pUQ0NBWFVDQVFBd0FEQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQUxyUQpkajJ5M3JGcUlxNGxiQXVKNCtWMCtnWHlEd1BHaUpPUXdoNG9NS00zOFJXZ2UzMHhqR1AwUnRVNEQ3Z2VxUFh5Ck5nVDR3cHphREU2UWk0S0t3endwZGV4MHovMUJQeWxtTlo3Um9GbDNJUTZkR2l2MTNLekFhK1BJbkdjK052N1EKZ0I5bDc0QTRBYWxWeTBmV3NITzNIcEI2RGJPRTUvbmRDOHJmTEhlMmE1SXdFR09pRGhoNFdrVjNmVVRvYkc3bwpOWEoydlhPZmJFekY2T3pPL1VTeEExSTBwUENkUTdKbUVQYytrTGw4WXlJczJKcFJNa0dybTF5TGpSV1hpVXFnCmVVY2NDck8yZVliUHdKbkJUckFrNjgwekVwTm5JV2QwTGlDWDIyaUFCSFY4OS9HTzlhM1crMWFaR3M0S0E5dXIKZ0VQem1ZSmVrblhzUlIxamdJTUNBd0VBQWFCSU1FWUdDU3FHU0liM0RRRUpEakU1TURjd0tBWURWUjBSQkNFdwpINElkWkdWMkxtRnlaMjlqWkM1bGQyMHpMbUZ6YzJWMGJXRnlheTV1WlhRd0N3WURWUjBQQkFRREFnV2dNQTBHCkNTcUdTSWIzRFFFQkN3VUFBNElCQVFBL3lPcVZNdWIrazZHUzNnek5PcmQrT3ZGQ213Q2RWZ3g4NCtGT0gwVlUKb1luTTVSUWcrZmJyOUI2eDVHRzA0WG9mTVl5eUJJenZVQzd5QUg3djN4UkhOZGJVeVNteWp0NEpGVWFQcDJlcwpuSXdNMm9pSklxbEM4UEFyeG1PYlpTMkZWWTZrcTVVVXYvVHV6VTZHaUd0aTBCSmcrTFhySTQ2U2xrTHM1OEdHCm5hUVJubGRpTW51ZlN3b1VFVG1CcW5OMm0zTC94MmxBRTlTQjZFN0ZVZk9FNlI3SXBVRVpyN3doTFFwRno0VjQKQ3RmNmJaYU1tMmJiOVh6NkRQK2VaSW9obTRUc08vQ3B3ZUhkVVFoM3NCQ0l5MmNlRnk0VFhkL1UrYUptL1dXWgorelRlUHNSNnA1KzJQQ2xGWGM4bnhXR284RmxvT0xNZjVJNmRRS1FHQlpmTgotLS0tLUVORCBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0K\nStatus:\n  Authorizations:\n    Challenges:\n      Token:        e8t60kb3mDeO5BWHAkYg_RI1dXH57z_ZBZZeLbyUa6U\n      Type:         http-01\n      URL:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/278379837496/W_Tvfg\n      Token:        e8t60kb3mDeO5BWHAkYg_RI1dXH57z_ZBZZeLbyUa6U\n      Type:         dns-01\n      URL:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/278379837496/22QXhQ\n      Token:        e8t60kb3mDeO5BWHAkYg_RI1dXH57z_ZBZZeLbyUa6U\n      Type:         tls-alpn-01\n      URL:          https://acme-v02.api.letsencrypt.org/acme/chall-v3/278379837496/wE0I2Q\n    Identifier:     argocd.yourdomain.com\n    Initial State:  pending\n    URL:            https://acme-v02.api.letsencrypt.org/acme/authz-v3/278379837496\n    Wildcard:       false\n  Certificate:      LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUZCekNDQSsrZ0F3SUJBZ0lTQThJZ01hMWc0eXBuaU9Ud2ZVMUh0ZW1TTUEwR0NTcUdTSWIzRFFFQkN3VUEKTURJeEN6QUpCZ05WQkFZVEFsVlRNUll3RkFZRFZRUUtFdzFNWlhRbmN5QkZibU55ZVhCME1Rc3dDUVlEVlFRRApFd0pTTXpBZUZ3MHlNekV3TWpneU1ETTBNVEZhRncweU5EQXhNall5TURNME1UQmFNQ2d4SmpBa0JnTlZCQU1UCkhXUmxkaTVoY21kdlkyUXVaWGR0TXk1aGMzTmxkRzFoY21zdWJtVjBNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUYKQUFPQ0FROEFNSUlCQ2dLQ0FRRUF1dEIyUGJMZXNXb2lyaVZzQzRuajVYVDZCZklQQThhSWs1RENIaWd3b3pmeApGYUI3ZlRHTVkvUkcxVGdQdUI2bzlmSTJCUGpDbk5vTVRwQ0xnb3JEUENsMTdIVFAvVUUvS1dZMW50R2dXWGNoCkRwMGFLL1hjck1CcjQ4aWNaejQyL3RDQUgyWHZnRGdCcVZYTFI5YXdjN2Nla0hvTnM0VG4rZDBMeXQ4c2Q3WnIKa2pBUVk2SU9HSGhhUlhkOVJPaHNidWcxY25hOWM1OXNUTVhvN003OVJMRURValNrOEoxRHNtWVE5ejZRdVh4agpJaXpZbWxFeVFhdWJYSXVORlplSlNxQjVSeHdLczdaNWhzL0FtY0ZPc0NUcnpUTVNrMmNoWjNRdUlKZmJhSUFFCmRYejM4WTcxcmRiN1Zwa2F6Z29EMjZ1QVEvT1pnbDZTZGV4RkhXT0Fnd0lEQVFBQm80SUNIekNDQWhzd0RnWUQKVlIwUEFRSC9CQVFEQWdXZ01CMEdBMVVkSlFRV01CUUdDQ3NHQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFNQmdOVgpIUk1CQWY4RUFqQUFNQjBHQTFVZERnUVdCQlROV2xpay8remRSUmFqbGtEZGxtVCtSbVJTOWpBZkJnTlZIU01FCkdEQVdnQlFVTHJNWHQxaFd5NjVRQ1VEbUg2K2RpeFRDeGpCVkJnZ3JCZ0VGQlFjQkFRUkpNRWN3SVFZSUt3WUIKQlFVSE1BR0dGV2gwZEhBNkx5OXlNeTV2TG14bGJtcUdTSWIzRFFFQkN3VUFBNElCQVFBS2N3QnNsbTcvRGxMUXJ0Mk01MW9HclMrbzQ0Ky95UW9ERlZEQwo1V3hDdTIrYjlMUlB3a1NJQ0hYTTZ3ZWJGR0p1ZU43c0o3bzVYUFdpb1c1V2xIQVFVN0c3NUsvUW9zTXJBZFNXCjlNVWdOVFA1MkdFMjRIR050TGkxcW9KRmxjRHlxU01vNTlhaHkyY0kycUJETEtvYmt4L0ozdldyYVYwVDlWdUcKV0NMS1RWWGtjR2R0d2xmRlJqbEJ6NHBZZzFodG1mNVg2RFlPOEE0anF2MklsOURqWEE2VVNiVzFGelhTTHI5TwpoZThZNElXUzZ3WTdiQ2tqQ1dEY1JRSk1FaGc3NmZzTzN0eEUrRmlZcnVxOVJVV2hpRjFteXY0UTZXK0N5QkZDCkRmdnA3T09HQU42ZEVPTTQrcVI5c2Rqb1NZS0VCcHNyNkd0UEFRdzRkeTc1M2VjNQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n  Finalize URL:     https://acme-v02.api.letsencrypt.org/acme/finalize/819289237/218302940896\n  State:            valid\n  URL:              https://acme-v02.api.letsencrypt.org/acme/order/819289237/218302940896\nEvents:             &lt;none&gt;\n</code></pre> <p>In case of failure, here we can see that cert-manager has created two Challenge resources to verify we control specific domains, a requirements of the ACME order to obtain a signed certificate.</p>"},{"location":"kubernetes/5.2-cert-troubleshooting/#step-6-troubleshooting-challenges","title":"Step-6: Troubleshooting Challenges","text":"<p>In order to determine why an ACME Order is not being finished, we can debug using the <code>Challenge</code> resources that cert-manager has created.</p> <pre><code>kubectl get challenges -A\nkubectl get challenges -n argocd\n\n# output\nNAME                                 STATE     DOMAIN            REASON                                     AGE\nexample-com-2745722290-4391602865-0  pending   example.com       Waiting for dns-01 challenge propagation   22s\n</code></pre> <p>Get more information about a specific Challenge:</p> <pre><code> kubectl describe challenge example-com-2745722290-4391602865-0\n\n# output\nStatus:\n  Presented:   true\n  Processing:  true\n  Reason:      Waiting for dns-01 challenge propagation\n  State:       pending\nEvents:\n  Type    Reason     Age   From          Message\n  ----    ------     ----  ----          -------\n  Normal  Started    19s   cert-manager  Challenge scheduled for processing\n  Normal  Presented  16s   cert-manager  Presented challenge using dns-01 challenge mechanism\n</code></pre> <p>Conclusion By systematically checking these resources and their statuses, you can identify and resolve issues with Let's Encrypt certificates managed by cert-manager. Refer to cert-manager's official troubleshooting documentation for additional insights.</p>"},{"location":"kubernetes/5.2-cert-troubleshooting/#reference","title":"Reference:","text":"<ul> <li>https://cert-manager.io/docs/troubleshooting/</li> <li>https://cert-manager.io/docs/troubleshooting/acme/</li> </ul>"},{"location":"kubernetes/5.2-manifests-microservice-2/","title":"5.2 manifests microservice 2","text":"<ul> <li>Step-1: Create deployment manifest</li> <li>Step-2: Create service manifest</li> <li>Step-3: Create ingress manifest</li> <li>Step-4: Deploy manifest files in AKS</li> <li>Step-5: Test Microservice-1 running in AKS</li> </ul>"},{"location":"kubernetes/6-aks-kv-integration/","title":"Integrating Azure Key Vault with AKS using Terraform","text":""},{"location":"kubernetes/6-aks-kv-integration/#introduction","title":"Introduction","text":"<p>Managing and using secretes, certificates is something very important part of any kind of project.</p> <p>Default Kubernetes secrete management will not be sufficient for securing secretes; they are just base64 encoded strings which can be easily decoded anyone. true security is, using Azure Key vault for storing secretes.</p> <p>By using the <code>Azure Key Vault Provider for Secrets Store CSI Driver</code>, you can securely store secrets in Azure Key Vault and use them in your applications without having to manage the secrets directly in your Kubernetes cluster.</p> <p>The <code>Azure Key Vault Provider for Secrets Store CSI Driver</code> is a Container Storage Interface (CSI) driver that allows you to use Azure Key Vault as a secrets store for your Kubernetes cluster. It provides a way for your pods to securely access secrets stored in Key Vault and use them in your applications. The driver acts as a bridge between your Kubernetes cluster and Azure Key Vault.</p>"},{"location":"kubernetes/6-aks-kv-integration/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>cloud engineer</code> you've been asked to integrate Azure Key Vault service with Azure Kubernetes Service(AKS) so that all your application configuration is fully secured. we are going to use Azure Key Vault Provider for Secrets Store CSI Driver for achiving the goal.</p>"},{"location":"kubernetes/6-aks-kv-integration/#prerequisites","title":"Prerequisites","text":"<p>Before start the implementation, ensure you have the following prerequisites:</p> <ul> <li>Azure subscription</li> <li>Terraform installed and configured</li> <li>Azure CLI, Kubectl, Helm tools installed</li> <li>AKS cluster</li> <li>Azure Key Vault</li> </ul>"},{"location":"kubernetes/6-aks-kv-integration/#objective","title":"Objective","text":"<p>In this exercise, we will cover the following steps to integrating Azure key vault with AKS using terraform:</p> <ol> <li> <p>Step-1: Install the Secrets Store CSI Driver and the Azure Keyvault Provider</p> </li> <li> <p>Step 2: Craete access policy in Azure Key Vault for AKS cluster</p> </li> <li> <p>Step 3: Create Secret Provider Class</p> </li> <li> <p>Step 4: Update and Apply Deployment YAML File</p> </li> <li> <p>Step 5: Create Secret in Azure Key Vault</p> </li> <li> <p>Step 6: Validate the Secrets Store Deployment</p> </li> </ol>"},{"location":"kubernetes/6-aks-kv-integration/#architecture-diagram","title":"Architecture Diagram","text":"<p>This digram explains components used in this lab like CSI driver, Keyvault Provider &amp; AKS VMSS access in Key vault.</p> <p></p> <p>login to Azure</p> <p>Before you start this lab, ensure you are logged into the correct Azure subscription using Visual Studio Code:</p> <pre><code># Login to Azure\naz login \n\n# Shows current Azure subscription\naz account show --output table\n\n# Lists all available Azure subscriptions\naz account list --output table\n\n# Sets Azure subscription to desired subscription using ID\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>To interact with your Azure Kubernetes Service (AKS) cluster, you need to establish a connection. Depending on your role, you can use either the User or Admin credentials:</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# verify the aks connection by running following commands\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/6-aks-kv-integration/#implementation-details","title":"Implementation Details","text":"<p>This guide will take you through the steps to configure and run the Azure Key Vault Provider for Secrets Store CSI driver on Kubernetes for Integrating Azure Key Vault with AKS cluster.</p> <p>Utilize Terraform to deploy the Secrets Store CSI Driver Helm chart onto AKS.</p>"},{"location":"kubernetes/6-aks-kv-integration/#step-1-install-the-secrets-store-csi-driver-and-the-azure-keyvault-provider","title":"Step-1: Install the Secrets Store CSI Driver and the Azure Keyvault Provider","text":"<p>To install Helm charts using Terraform, you'll need to add the necessary Terraform providers. Here's an example of how to include them in your <code>provider.tf</code> file:</p> <p>Let's add following terraform providers in <code>provider.tf</code> file</p> provider.tf<pre><code>provider \"kubernetes\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  #load_config_file       = false\n}\n\nprovider \"helm\" {\n  debug = true\n\n  kubernetes {\n    host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n\n  }\n}\nprovider \"kubectl\" {\n  host                   = azurerm_kubernetes_cluster.aks.kube_admin_config.0.host\n  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_certificate)\n  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.client_key)\n  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_admin_config.0.cluster_ca_certificate)\n  load_config_file       = false\n}\n</code></pre> <p>Since we've added new providers in terraform project we need to run terraform init once again;</p> <p>Terraform init</p> <pre><code>terraform init\n</code></pre> <p>Install Helm Chart using terraform</p> <p><code>Deploy the CSI Driver:</code> You need to deploy the Azure Key Vault Provider for Secrets Store CSI Driver in your Kubernetes cluster. Here we are going to use Helm chart to deploy the driver.</p> <p>Running the this helm chart will install both the Secrets Store CSI Driver and Azure Key Vault provider.</p> aks-kv-csi-driver.tf<pre><code>locals {\n  csi_driver_settings = {\n    linux = {\n      enabled = true\n      resources = {\n        requests = {\n          cpu    = \"100m\"\n          memory = \"100Mi\"\n        }\n        limits = {\n          cpu    = \"100m\"\n          memory = \"100Mi\"\n        }\n      }\n    }\n    secrets-store-csi-driver = {\n      install = true\n      linux = {\n        enabled = true\n      }\n      logLevel = {\n        debug = true\n      }\n      syncSecret = {\n        enabled = true\n      }\n    }\n  }\n}\n\nresource \"helm_release\" \"csi_driver\" {\n  name  = \"csi-secrets-store-provider-azure\"\n  chart = \"csi-secrets-store-provider-azure\"\n  #   version      = \"0.0.8\"\n  repository   = \"https://azure.github.io/secrets-store-csi-driver-provider-azure/charts\"\n  namespace    = \"kube-system\"\n  max_history  = 4\n  atomic       = true\n  reuse_values = false\n  timeout      = 1800\n  values       = [yamlencode(local.csi_driver_settings)]\n  depends_on = [\n    azurerm_kubernetes_cluster.aks\n  ]\n}\n</code></pre> <p>Validation</p> <p>Verify the Secrets Store CSI Driver and Azure Key Vault provider are successfully installed</p> <p>To verify the <code>Secrets Store CSI Driver</code> is installed, run this command:</p> <pre><code>kubectl get pods -l app=secrets-store-csi-driver -n kube-system\n\n# You should see the driver pods running on each agent node:\nNAME                             READY   STATUS    RESTARTS   AGE\nsecrets-store-csi-driver-spbfq   3/3     Running   0          3h52m\n</code></pre> <p>To verify the Azure <code>Key Vault provider</code> is installed, run this command:</p> <pre><code>kubectl get pods -l app=csi-secrets-store-provider-azure -n kube-system\n\n# You should see the provider pods running on each agent node:\nNAME                                         READY   STATUS    RESTARTS   AGE\ncsi-secrets-store-provider-azure-tpb4j   1/1     Running   0          3h52m\n</code></pre> <p>Note</p> <p>I recently observed that the latest version of AKS clusters now includes these pods by default. Therefore, if you have created an AKS cluster with the latest Kubernetes version, you can safely skip step-1.</p>"},{"location":"kubernetes/6-aks-kv-integration/#step-2-craete-access-policy-in-azure-key-vault-for-aks-cluster","title":"Step-2: Craete access policy in Azure Key Vault for AKS cluster","text":"<p>Granting the necessary permissions for the Kubernetes cluster to access secrets in the Key Vault is a crucial step in securing your applications. There are multiple approaches to achieve this, such as using Helm charts, Azure Command-Line Interface (az) commands, or Azure Active Directory (AAD) pod identity. In this lab, we'll focus on granting access to the Azure Kubernetes Service (AKS) Virtual Machine Scale Set (VMSS) identity, when I tried with AAD pod identity I've encountered couple of issues therefore I am selecting VMSS identity here.</p> <p>Manual Steps: Enable Managed Service Identity (MSI) for the AKS VMSS.</p> <ol> <li> <p>Navigate to the AKS MC_ resource group in the Azure portal, typically named in the format <code>MC_rg-rgname-dev_aks-aksname-dev_region</code>.</p> </li> <li> <p>Select the AKS Virtual Machine Scale Set (VMSS) - often named something like <code>aks-agentpool-12345678-vmss</code>.</p> </li> <li> <p>In the left navigation pane, click on \"Identity,\" and set the Status to \"On\" in the \"System Assigned\" tab. This action creates a new MSI. Copy the generated ID; you'll need it later.</p> </li> </ol> <p>Update the Terraform script with the AKS VMSS identity.</p> <p>Once the AKS VMSS identity is enabled, proceed to update the Terraform script to incorporate this identity. Below are the steps:</p> <ul> <li>In the Terraform script, create a new file (e.g., <code>role_assignment.tf</code>) and add the following code:</li> </ul> role_assignment.tf<pre><code>  resource \"azurerm_key_vault_access_policy\" \"aks_vmss_access_policy\" {\n    key_vault_id = azurerm_key_vault.kv.id\n    tenant_id    = data.azurerm_subscription.current.tenant_id\n    object_id    = var.aks_vmss_identity // Replace with the Object (principal) ID of the VMSS\n\n    secret_permissions = [\n      \"Get\",\n    ]\n\n    depends_on = [\n      azurerm_key_vault.kv\n    ]\n  }\n</code></pre> <ul> <li>Declare a variable for AKS VMSS identity in your Terraform script:</li> </ul> variables.tf<pre><code>  variable \"aks_vmss_identity\" {\n    description = \"Manually enable the AKS VMSS MSI before running this.\"\n    type        = string\n  }\n</code></pre> <ul> <li>Copy the Object (principal) ID of the VMSS generated during the manual step and update the variable in the Terraform script:</li> </ul> variables.tf<pre><code>  aks_vmss_identity = \"ddfd8a57-316e-44ba-b5ea-dc1824d9b480\"\n</code></pre> <p>Validate Key Vault Access Policy</p> <p>The Terraform code above creates a new Key Vault access policy, granting the specified permissions to the AKS VMSS identity. Note that the <code>object_id</code> should be dynamically retrieved; however, the solution to achieve dynamic reading is pending.</p> <p>Ensure to automate the manual step in the future for a fully streamlined process.</p>"},{"location":"kubernetes/6-aks-kv-integration/#step-3-create-secret-provider-class","title":"Step-3: Create Secret Provider Class","text":"<p>To effectively utilize and configure the Secrets Store CSI Driver for your Azure Kubernetes Service (AKS) cluster, you'll need to create a <code>SecretProviderClass</code> custom resource. This is an essential step as it defines how the Secrets Store CSI Driver interacts with your Azure Key Vault and synchronizes secrets to your Kubernetes cluster.</p> <p>The creation of the <code>SecretProviderClass</code> can be seamlessly integrated into your Helm charts or YAML manifests. Below is an example YAML file (secretproviderclass.yaml) illustrating a SecretProviderClass setup using a system-assigned identity to access the Azure Key Vault:</p> secretproviderclass.yaml<pre><code># This is a SecretProviderClass example using system-assigned identity to access with key vault\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: keyvault-secret-provider\n  namespace: sample\nspec:\n  provider: azure\n  secretObjects:                              # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects\n  - secretName: secret-provider-dev                   # name of the Kubernetes secret object\n    type: Opaque                              # type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls)\n    data:\n    - objectName: sample-secret               # name of the mounted content to sync; this could be the object name or the object alias\n      key: sample-secret                      # data field to populate\n    - objectName: connectionstring-dbname     # name of the mounted content to sync; this could be the object name or the object alias\n      key: connectionstring-dbname            # data field to populate\n\n  parameters:\n    usePodIdentity: \"false\"\n    useVMManagedIdentity: \"true\"    # Set to true for using managed identity\n    userAssignedIdentityID: \"\"      # If empty, then defaults to use the system assigned identity on the VM\n    keyvaultName: kv-kvname-dev      # azure key vault service name\n    cloudName: \"AzurePublicCloud\"   # [OPTIONAL for Azure] if not provided, the Azure environment defaults to AzurePublicCloud\n    objects:  |\n      array:\n        - |\n          objectName: sample-secret\n          objectType: secret        # object types: secret, key, or cert\n          objectVersion: \"\"         # [OPTIONAL] object versions, default to latest if empty\n        - |\n          objectName: connectionstring-dbname\n          objectType: secret        # object types: secret, key, or cert\n          objectVersion: \"\"         # [OPTIONAL] object versions, default to latest if empty\n    tenantId: ee93c4a0-f87d-46ad-b4be-3ee05cefecws  # The tenant ID of the key vault\n</code></pre> <p>Explanation:</p> <ul> <li>provider: Specifies the provider, in this case, \"azure.\"</li> <li>secretObjects: Defines the desired state of synced Kubernetes secret objects.</li> <li>parameters: Configuration parameters for the Secrets Store CSI Driver.</li> <li>objects: Specifies the array of objects to sync from Azure Key Vault to the Kubernetes cluster.</li> </ul> <p>kubectl apply </p> <p>Execute the following kubectl apply command to apply the configuration:</p> <p><pre><code>kubectl apply -f secretproviderclass.yaml\n\n# output \n\n# secretproviderclass.secrets-store.csi.x-k8s.io/azure-kvname-system-msi created\n</code></pre> validation</p> <pre><code>kubectl get secretproviderclass -n sample\n\n# output\n\nNAME                       AGE\nkeyvault-secret-provider   4m12s\n</code></pre> <p>This confirms that the <code>SecretProviderClass</code> named <code>keyvault-secret-provider</code> has been successfully created in the <code>sample</code> namespace.</p> <p>For more details, you can refer to the official Microsoft documentation</p>"},{"location":"kubernetes/6-aks-kv-integration/#step-4-update-and-apply-deployment-yaml-file","title":"Step-4: Update and apply deployment YAML file","text":"<p>To seamlessly integrate the Secrets Store CSI Driver into your Azure Kubernetes Service (AKS) cluster, you must update your deployment YAML file to reference the newly created <code>SecretProviderClass</code>. This ensures that your applications can securely access and utilize the secrets stored in Azure Key Vault.</p> aspnet-api.yalm<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: tenant1\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api-image\n          image: acrname.azurecr.io/aspnet-api:20221006.2\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n          volumeMounts:\n          - name: secrets-store01-inline\n            mountPath: \"/mnt/secrets-store\"\n            readOnly: true\n\n          env:\n            - name: ASPNETCORE_ENVIRONMENT\n              value: \"Development\"\n            - name: ConnectionStrings__CoreDB\n              valueFrom:\n                secretKeyRef:\n                  name: secret-provider-dev\n                  key: connectionstring-coredb\n      volumes:\n        - name: secrets-store01-inline\n          csi:\n            driver: secrets-store.csi.k8s.io\n            readOnly: true\n            volumeAttributes:\n              secretProviderClass: \"keyvault-secret-provider\"\n</code></pre> <p>Explanation: - volumeMounts: Mounts the secrets store volume to the specified path in your container. - volumes: Specifies the volume configuration, including the CSI driver and the associated SecretProviderClass.</p> <p>Then, apply the updated deployment YAML file to the cluster:</p> <p>Once you've updated your deployment YAML file, apply the changes to the cluster using the following kubectl apply command:</p> <pre><code>kubectl apply -f aspnet-api.yaml \n</code></pre> <p>This action ensures that your deployment now incorporates the Secrets Store CSI Driver, and your application pods will securely access the secrets from Azure Key Vault.</p>"},{"location":"kubernetes/6-aks-kv-integration/#step-5-create-secret-in-azure-key-vault","title":"Step-5: Create Secret in Azure Key Vault","text":"<p>Azure Key Vault can store keys, secrets, and certificates. In the following example, a plain-text secret called ExampleSecret is configured.</p> <pre><code>az keyvault secret set --vault-name &lt;keyvault-name&gt; -n ExampleSecret --value MyAKSHCIExampleSecret\n</code></pre>"},{"location":"kubernetes/6-aks-kv-integration/#step-6-validate-the-secrets-store-deployment","title":"Step-6: Validate the Secrets Store deployment","text":"<p>To show the secrets that are held in secrets-store, run the following command:</p> <pre><code>kubectl exec busybox-secrets-store-inline --namespace kube-system -- ls /mnt/secrets-store/\n\n# output\n\nExampleSecret\n</code></pre> <p>To show the test secret held in secrets-store, run the following command:</p> <pre><code>kubectl exec busybox-secrets-store-inline --namespace kube-system -- cat /mnt/secrets-store/ExampleSecret\n\n#output\n\nMyAKSHCIExampleSecret\n</code></pre>"},{"location":"kubernetes/6-aks-kv-integration/#references","title":"References","text":"<p>Here is a comprehensive list of resources that were used in the development of this technical guide:</p> <ol> <li> <p>Use Azure Key Vault Provider for Kubernetes Secrets Store CSI Driver with AKS hybrid - Microsoft Azure's official documentation on utilizing the Azure Key Vault Provider for Kubernetes Secrets Store CSI Driver with AKS.</p> </li> <li> <p>MSDN Documentation - Refer to the MSDN documentation for detailed insights into how secrets are pulled from the Key Vault service.</p> </li> <li> <p>secrets-store-csi-driver-provider-azure GitHub Repo - Explore the GitHub repository for the Azure provider of the Secrets Store CSI Driver.</p> </li> <li> <p>Azure Key Vault Provider for Secrets Store CSI Driver Documentation - The official documentation for the Azure Key Vault Provider for Secrets Store CSI Driver.</p> </li> <li> <p>Helm Chart Installation details - Get detailed information on installing Helm Charts for the Secrets Store CSI Driver.</p> </li> </ol> <p>Troubleshooting Resources</p> <ol> <li> <p>Troubleshooting: Secret is not creating in AKS after fetching it with CSI driver - Stack Overflow discussion on troubleshooting issues with secret creation in AKS.</p> </li> <li> <p>AKS with Azure Key Vault: Env variables don't load - Server Fault thread addressing environment variable loading issues with AKS and Azure Key Vault.</p> </li> <li> <p>Azure Key Vault integration with AKS - Not clear what to do if not working - Stack Overflow discussion on integration issues with AKS and Key Vault.</p> </li> <li> <p>GitHub Issue: Azure Key Vault integration with AKS - GitHub issue addressing problems related to Azure Key Vault integration.</p> </li> <li> <p>AKS and Azure Key Vault Integration - Stack Overflow discussion on integrating Azure Key Vault with AKS.</p> </li> <li> <p>Reloading an env variable synced to a mounted Azure Key Vault secret - A guide on reloading environment variables synced with Azure Key Vault secrets.</p> </li> <li> <p>Known Limitations of Secrets Store CSI Driver - Understand the known limitations of the Secrets Store CSI Driver.</p> </li> <li> <p>Enable and Disable Autorotation - Learn about enabling and disabling autorotation with the CSI Secrets Store Driver.</p> </li> </ol> <p>Feel free to explore these resources for a more in-depth understanding and troubleshooting of the Secrets Store CSI Driver integration with Azure Kubernetes Service.</p>"},{"location":"kubernetes/aks-cluster-upgrade/","title":"Aks cluster upgrade","text":""},{"location":"kubernetes/aks-cluster-upgrade/#introduction","title":"Introduction","text":""},{"location":"kubernetes/aks-cluster-upgrade/#login-to-azure","title":"login to Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Set Azure subscription\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#connect-to-cluster","title":"Connect to Cluster","text":"<p>Use the following command to connect to your AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get no -o wide\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#command-to-list-the-kubernetes-version-and-the-available-kubernetes-version-upgrades-on-control-plane","title":"Command to list the kubernetes version and the available kubernetes version upgrades on control plane","text":"<pre><code>az aks get-upgrades \\\n   --resource-group ResourceGroupName --name 'AKSClusterName' --output table\n\n# example\naz aks get-upgrades \\\n   --resource-group 'rg-aks-dev' --name 'aks-cluster1-dev' --output table\n\n# output\nName     ResourceGroup    MasterVersion    Upgrades       \n-------  ---------------  ---------------  ---------------\ndefault  rg-aks-dev       1.24.9           1.25.6, 1.25.11\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#command-to-list-the-kubernetes-version-for-all-the-nodes-in-your-nodepools","title":"Command to list the kubernetes version for all the nodes in your nodepools","text":"<pre><code>az aks nodepool list \\  \n   --resource-group ResourceGroupName --cluster-name AKSClusterName \\\n   --query \"[].{Name:name,k8version:orchestratorVersion}\" --output table\n\n# example\naz aks nodepool list \\\n   --resource-group 'rg-aks-dev' --cluster-name 'aks-cluster1-dev' \\\n   --query \"[].{Name:name,k8version:orchestratorVersion}\" --output table\n# output\nName       K8version\n---------  -----------\nagentpool  1.24.9\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#upgrade-kubernetes-version-on-all-nodes-at-once","title":"Upgrade kubernetes version on all nodes at once","text":"<p>In order to upgrade all nodes at once - we run the following command, this will upgrade all control plane and worker nodes at once to the desired kubernetes version</p> <pre><code>az aks upgrade \\\n   --resource-group ResourceGroupName --name AKSClusterName \\\n    --no-wait \\\n   --kubernetes-version KubernetesVersion\n\n# example\naz aks upgrade \\\n   --resource-group 'rg-aks-dev' --name 'aks-cluster1-dev' \\\n    --no-wait \\\n   --kubernetes-version 1.24.9\n\n#output\nKubernetes may be unavailable during cluster upgrades.\n Are you sure you want to perform this operation? (y/N): y\nSince control-plane-only argument is not specified, this will upgrade the control plane AND all nodepools to version 1.25.6. Continue? (y/N): y\n</code></pre> <p>Message: Upgrades are disallowed while cluster is in a failed state. For resolution steps visit https://aka.ms/aks-cluster-failed to troubleshoot why the cluster state may have failed and steps to fix cluster state.\u2190[0m</p> <p>fix for the error: https://serverfault.com/questions/1067433/aks-version-upgrade-error-operation-failed-with-status-conflict-details-up</p>"},{"location":"kubernetes/aks-cluster-upgrade/#upgrade-kubernetes-version-on-control-plane-alone","title":"Upgrade kubernetes version on control plane alone","text":"<p>If we only want to upgrade the kubernetes version for control plane alone - we run the same command but with the --control-plane-only switch </p> <pre><code>az aks upgrade \\\n   --resource-group ResourceGroupName --name AKSClusterName \\\n   --control-plane-only --no-wait \\\n   --kubernetes-version KubernetesVersion\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#upgrade-kubernetes-version-on-specific-node-pools","title":"Upgrade Kubernetes version on specific node pools","text":"<p>If we need to upgrade the kubernetes version on only specific node pools we can run the following command by specifiying the nodepoolname and the kubernetes version </p> <pre><code>az aks nodepool upgrade \\\n   --resource-group ResourceGroupName --cluster-name AKSClusterName --name NodePoolName \\\n   --no-wait --kubernetes-version KubernetesVersion\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#nodepool-image-upgrade","title":"Nodepool image upgrade","text":"<p>Running the following command lists all the current nodeimage versions of the  nodepool </p> <pre><code>az aks nodepool list \\\n   --resource-group ResourceGroupName --cluster-name AKSClusterName \\\n   --query \"[].{Name:name,NodeImageVersion:nodeImageVersion}\" --output table\n\n# example\naz aks nodepool list `\n   --resource-group 'rg-aks-dev' --cluster-name 'aks-cluster1-dev' `\n   --query \"[].{Name:name,NodeImageVersion:nodeImageVersion}\" --output table\n</code></pre> <p>the following command to list the available latest nodeimage version for us </p> <pre><code>az aks nodepool get-upgrades \\\n   --resource-group ResourceGroupName --cluster-name AKSClusterName \\\n   --nodepool-name NodePoolName --output table\n\n# example\naz aks nodepool get-upgrades `\n   --resource-group 'rg-aks-dev' --cluster-name 'aks-cluster1-dev' `\n   --nodepool-name agentpool --output table\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#upgrade-nodeimage-version-for-specific-nodepool","title":"Upgrade nodeimage version for specific nodepool","text":"<p>In order to upgrade the nodeimage version for a specific nodepool we run the following command : </p> <pre><code>az aks nodepool upgrade \\\n    --resource-group myResourceGroup \\\n    --cluster-name myAKSCluster \\\n    --name mynodepool \\\n    --node-image-only\n</code></pre> <pre><code>az aks nodepool show \\\n    --resource-group myResourceGroup \\\n    --cluster-name myAKSCluster \\\n    --name mynodepool\n\n# example\naz aks nodepool show `\n    --resource-group 'rg-aks-dev' `\n    --cluster-name 'aks-cluster1-dev' `\n    --name agentpool\n</code></pre>"},{"location":"kubernetes/aks-cluster-upgrade/#upgrade-nodeimage-version-for-all-nodepools-at-once","title":"Upgrade nodeimage version for all nodepools at once","text":"<p><pre><code>az aks upgrade \\\n    --resource-group myResourceGroup \\\n    --name myAKSCluster \\\n    --node-image-only\n</code></pre> Remember If you don\u2019t run the nodeimage switch remember the whole cluster including the kubernetes version of the control plane is also upgraded.</p> <pre><code>az aks show \\\n    --resource-group myResourceGroup \\\n    --name myAKSCluster\n\n\naz aks show -g 'rg-aks-dev' -n  'aks-cluster1-dev' \n</code></pre> <p>AKS Patching and Node Pool Upgrade Azure Kubernetes Services (AKS) </p> <p>Type of upgrades:</p> <ul> <li>Kubernetes version upgrades  - https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli</li> <li>node images version Upgrade - https://learn.microsoft.com/en-us/azure/aks/node-image-upgrade</li> <li>Automatically upgrade an AKS cluster - https://learn.microsoft.com/en-us/azure/aks/auto-upgrade-cluster </li> </ul> <p>3 ways to upgrade nodes - All at once - Only Control plan - only selected nodes</p>"},{"location":"kubernetes/aks-cluster-upgrade/#reference","title":"Reference","text":"<ul> <li>Supported Kubernetes versions in Azure Kubernetes Service (AKS)</li> <li>Create node pools for a cluster in AKS</li> <li>Azure Kubernetes Services (AKS) Node Pools</li> <li>Azure Kubernetes Services (AKS) Node Pools</li> <li>Taints, Tolerations, NodeSelector in AKS (Azure Kubernetes Services)</li> <li>AKS Patching and Node Pool Upgrade Azure Kubernetes Services (AKS) </li> <li>Kubernetes version upgrades  - https://learn.microsoft.com/en-us/azure/aks/upgrade-cluster?tabs=azure-cli</li> <li>node images version Upgrade - https://learn.microsoft.com/en-us/azure/aks/node-image-upgrade</li> <li>Automatically upgrade an AKS cluster - https://learn.microsoft.com/en-us/azure/aks/auto-upgrade-cluster </li> </ul>"},{"location":"kubernetes/aks-create-node-pools%20copy/","title":"Aks create node pools copy","text":""},{"location":"kubernetes/aks-create-node-pools%20copy/#update-deploymentyaml-files-for-tolerations","title":"Update deployment.yaml files for tolerations","text":"<pre><code>      tolerations:\n        - key: \"TenantName\"\n          operator: \"Equal\"\n          value: \"tenant1\"\n          effect: \"NoSchedule\"  # This pod tolerates the taint\n</code></pre>"},{"location":"kubernetes/aks-create-node-pools%20copy/#check-the-status-of-your-node-pools","title":"Check the status of your node pools","text":"<pre><code>az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster\n\n# example\naz aks nodepool list --resource-group 'rg-aks-dev' --cluster-name 'aks-cluster1-dev'\n\n# output \n[\n  {\n    \"availabilityZones\": null,\n    \"capacityReservationGroupId\": null,    \n    \"count\": 2,\n    \"creationData\": null,\n    \"currentOrchestratorVersion\": \"1.26.6\",\n    \"enableAutoScaling\": true,\n    \"enableCustomCaTrust\": false,\n    \"enableEncryptionAtHost\": false,\n    \"enableFips\": false,\n    \"enableNodePublicIp\": false,\n    \"enableUltraSsd\": false,\n    \"gpuInstanceProfile\": null,\n    \"hostGroupId\": null,\n    \"id\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourcegroups/rg-aks-dev/providers/Microsoft.ContainerService/managedClusters/aks-cluster1-dev/agentPools/agentpool\",\n    \"kubeletConfig\": null,\n    \"kubeletDiskType\": \"OS\",\n    \"linuxOsConfig\": null,\n    \"maxCount\": 5,\n    \"maxPods\": 110,\n    \"messageOfTheDay\": null,\n    \"minCount\": 2,\n    \"mode\": \"System\",\n    \"name\": \"agentpool\",\n    \"networkProfile\": null,\n    \"nodeImageVersion\": \"AKSUbuntu-2204gen2containerd-202308.22.0\",\n    \"nodeLabels\": null,\n    \"nodePublicIpPrefixId\": null,\n    \"nodeTaints\": null,\n    \"orchestratorVersion\": \"1.26.6\",\n    \"osDiskSizeGb\": 128,\n    \"osDiskType\": \"Managed\",\n    \"osSku\": \"Ubuntu\",\n    \"osType\": \"Linux\",\n    \"podSubnetId\": null,\n    \"powerState\": {\n      \"code\": \"Running\"\n    },\n    \"provisioningState\": \"Succeeded\",\n    \"proximityPlacementGroupId\": null,\n    \"resourceGroup\": \"rg-aks-dev\",\n    \"scaleDownMode\": \"Delete\",\n    \"scaleSetEvictionPolicy\": null,\n    \"scaleSetPriority\": null,\n    \"spotMaxPrice\": null,\n    \"tags\": null,\n    \"type\": \"Microsoft.ContainerService/managedClusters/agentPools\",\n    \"typePropertiesType\": \"VirtualMachineScaleSets\",\n    \"upgradeSettings\": {\n      \"maxSurge\": null\n    },\n    \"vmSize\": \"Standard_B4ms\",\n    \"vnetSubnetId\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-vnet1-dev/providers/Microsoft.Network/virtualNetworks/vnet-spoke-dev/subnets/snet-aks1\",\n    \"windowsProfile\": null,\n    \"workloadRuntime\": null\n  },\n  {\n    \"availabilityZones\": [\n      \"2\",\n      \"3\",\n      \"1\"\n    ],\n    \"capacityReservationGroupId\": null,\n    \"count\": 1,\n    \"creationData\": null,\n    \"currentOrchestratorVersion\": \"1.26.6\",\n    \"enableAutoScaling\": true,\n    \"enableCustomCaTrust\": false,\n    \"enableEncryptionAtHost\": false,\n    \"enableFips\": false,\n    \"enableNodePublicIp\": false,\n    \"enableUltraSsd\": false,\n    \"gpuInstanceProfile\": null,\n    \"hostGroupId\": null,\n    \"id\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourcegroups/rg-aks-dev/providers/Microsoft.ContainerService/managedClusters/aks-cluster1-dev/agentPools/usernodepool\",\n    \"kubeletConfig\": null,\n    \"kubeletDiskType\": \"OS\",\n    \"linuxOsConfig\": null,\n    \"maxCount\": 5,\n    \"maxPods\": 110,\n    \"messageOfTheDay\": null,\n    \"minCount\": 1,\n    \"mode\": \"User\",\n    \"name\": \"usernodepool\",\n    \"networkProfile\": null,\n    \"nodeImageVersion\": \"AKSUbuntu-2204gen2containerd-202308.22.0\",\n    \"nodeLabels\": {\n      \"TenantName\": \"tenant1\"\n    },\n    \"nodePublicIpPrefixId\": null,\n    \"nodeTaints\": [\n      \"TenantName=tenant1:NoSchedule\"\n    ],\n    \"orchestratorVersion\": \"1.26.6\",\n    \"osDiskSizeGb\": 128,\n    \"osDiskType\": \"Managed\",\n    \"osSku\": \"Ubuntu\",\n    \"osType\": \"Linux\",\n    \"podSubnetId\": null,\n    \"powerState\": {\n      \"code\": \"Running\"\n    },\n    \"provisioningState\": \"Succeeded\",\n    \"proximityPlacementGroupId\": null,\n    \"resourceGroup\": \"rg-aks-dev\",\n    \"scaleDownMode\": \"Delete\",\n    \"scaleSetEvictionPolicy\": null,\n    \"scaleSetPriority\": null,\n    \"spotMaxPrice\": null,\n    \"tags\": {\n      \"CreatedBy\": \"Anji.Keesari\",\n      \"Environment\": \"dev\",\n      \"Owner\": \"Anji.Keesari\",\n      \"Project\": \"Project-1\"\n    },\n    \"type\": \"Microsoft.ContainerService/managedClusters/agentPools\",\n    \"typePropertiesType\": \"VirtualMachineScaleSets\",\n    \"upgradeSettings\": {\n      \"maxSurge\": null\n    },\n    \"vmSize\": \"Standard_B8ms\",\n    \"vnetSubnetId\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-vnet1-dev/providers/Microsoft.Network/virtualNetworks/vnet-spoke-dev/subnets/snet-aks1\",\n    \"windowsProfile\": null,\n    \"workloadRuntime\": null\n  }\n]\n</code></pre> <pre><code>az aks get-versions --location eastus --query \"orchestrators\" -o table\n\n# output\nOrchestratorType    OrchestratorVersion    Default\n------------------  ---------------------  ---------\nKubernetes          1.27.3\nKubernetes          1.27.1\nKubernetes          1.26.6                 True\nKubernetes          1.26.3\nKubernetes          1.25.11\nKubernetes          1.25.6\n</code></pre>"},{"location":"kubernetes/aks-create-node-pools%20copy/#create-a-new-node-pool-with-the-desired-sku","title":"Create a new node pool with the desired SKU","text":"<p>Standard_DS2_v2 Standard_B8ms</p> <pre><code>az aks nodepool add \\ \n    --resource-group myResourceGroup \\ \n    --cluster-name myAKSCluster \\ \n    --name mynodepool \\ \n    --node-count 3 \\ \n    --node-vm-size Standard_DS3_v2 \\ \n    --mode System \\ \n    --no-wait\n\n# example\naz aks nodepool add \\\n    --resource-group 'rg-aks-dev' \\\n    --cluster-name 'aks-cluster1-dev' \\\n    --name agentpool1 \\\n    --node-count 2 \\\n    --node-vm-size Standard_DS3_v2 \\\n    --mode System \\\n    --no-wait\n\naz aks nodepool add `\n    --resource-group 'rg-aks-dev' `\n    --cluster-name 'aks-cluster1-dev' `\n    --name nodepool2 `\n    --node-count 3 `\n    --node-vm-size Standard_B8ms `\n    --mode System `\n    --no-wait\n\n\n# output\n</code></pre> <pre><code>kubectl get nodes\n\n# output\nNAME                                 STATUS   ROLES   AGE   VERSION\naks-mynodepool-20823458-vmss000000   Ready    agent   23m   v1.21.9\naks-mynodepool-20823458-vmss000001   Ready    agent   23m   v1.21.9\naks-mynodepool-20823458-vmss000002   Ready    agent   23m   v1.21.9\naks-nodepool1-31721111-vmss000000    Ready    agent   10d   v1.21.9\naks-nodepool1-31721111-vmss000001    Ready    agent   10d   v1.21.9\naks-nodepool1-31721111-vmss000002    Ready    agent   10d   v1.21.9\n</code></pre>"},{"location":"kubernetes/aks-create-node-pools%20copy/#cordon-the-existing-nodes","title":"Cordon the existing nodes","text":"<pre><code>az aks nodepool update --disable-cluster-autoscaler -g 'rg-aks-dev' -n nodepool1 --cluster-name 'aks-cluster1-dev'\n\n# output\n\n\n\n# Cordon the existing nodes\n\nkubectl cordon `\naks-nodepool2-23546727-vmss00001r `\naks-nodepool2-23546727-vmss00001s `\naks-nodepool2-23546727-vmss00001u \n</code></pre>"},{"location":"kubernetes/aks-create-node-pools%20copy/#drain-the-existing-nodes","title":"Drain the existing nodes","text":"<pre><code># Drain the existing nodes\n\nkubectl drain `\naks-nodepool2-23546727-vmss00001r `\naks-nodepool2-23546727-vmss00001s `\naks-nodepool2-23546727-vmss00001u `\n--ignore-daemonsets --delete-emptydir-data\n</code></pre> <pre><code># Remove the existing node pool\n\naz aks nodepool delete `\n    --resource-group 'rg-aks-dev' `\n    --cluster-name 'aks-cluster1-dev' `\n    --name agentpool\n</code></pre>"},{"location":"kubernetes/aks-create-node-pools%20copy/#reference","title":"Reference","text":"<ul> <li>Resize node pools in AKS</li> <li>Azure Container Node Pool</li> <li>Azure Container Node Pool</li> <li></li> <li></li> </ul>"},{"location":"kubernetes/aks-create-nodepool/","title":"Create a new user node pool in AKS","text":""},{"location":"kubernetes/aks-create-nodepool/#create-a-new-user-node-pool-in-aks-using-terraform","title":"Create a new user node pool in AKS using terraform","text":""},{"location":"kubernetes/aks-create-nodepool/#introduction","title":"Introduction","text":"<p>In Azure Kubernetes Service (AKS), there are two types of node pools: system node pools and user node pools. These node pools serve different purposes and are used for distinct workloads. </p> <p>This guide will walk you through creating a new user node pool in Azure Kubernetes Service (AKS) using Terraform and implementing taints and tolerations.</p>"},{"location":"kubernetes/aks-create-nodepool/#technical-scenario","title":"Technical Scenario","text":"<p>In this scenario, we will create a new user node pool in an existing AKS cluster using Terraform. We'll apply a taint to this node pool. Then, we'll deploy an application with tolerations to ensure it runs on nodes with the corresponding taint.</p>"},{"location":"kubernetes/aks-create-nodepool/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Declare &amp; Define Variables</li> <li>Step 2: Create a new user node pool in AKS using Terraform</li> <li>Step 3: Verify the new user node pool Taint</li> <li>Step 4: Deploy an application with tolerations to run on nodes with the taint</li> <li>Step 5: Verify your application status</li> </ul>"},{"location":"kubernetes/aks-create-nodepool/#prerequisites","title":"Prerequisites:","text":"<ol> <li> <p>Terraform Installed: Ensure you have Terraform installed on your local machine.</p> </li> <li> <p>Azure CLI: Install the Azure Command-Line Interface (CLI) for authentication.</p> </li> <li> <p>Terraform Configuration: Make sure you have an existing Terraform configuration for your AKS cluster.</p> </li> </ol>"},{"location":"kubernetes/aks-create-nodepool/#implementation-details","title":"Implementation Details","text":"<p>This guide will walk you through creating a new user node pool in Azure Kubernetes Service (AKS) using Terraform and implementing taints and tolerations.</p> <p>login to Azure</p> <p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Set Azure subscription\naz account set -s \"anji.keesari\"\n</code></pre> <p>Connect to Cluster</p> <p>Use the following command to connect to your AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/aks-create-nodepool/#step-1-declare-define-variables","title":"Step 1: Declare &amp; Define Variables","text":"<p>In your Terraform configuration file, declare variables for AKS cluster details, and node pool configuration.</p> <p>This table presents the variables along with their descriptions, data types, and default values:</p> Variable Name Description Type Default Value user_node_pool_enabled Should the User Node Pool enabled? Defaults to false. bool false user_node_pool_vm_size Specifies the VM size of the user node pool string Standard_B4ms user_node_pool_availability_zones Specifies the availability zones of the user node pool list(string) [\"1\", \"2\", \"3\"] user_node_pool_name Specifies the name of the user node pool string agentpool user_node_pool_subnet_name Specifies the name of the subnet that hosts the user node pool string SystemSubnet user_node_pool_subnet_address_prefix Specifies the address prefix of the subnet that hosts the user node pool list(string) [\"10.0.0.0/20\"] user_node_pool_enable_auto_scaling Whether to enable auto-scaler. Defaults to false. bool false user_node_pool_enable_host_encryption Should the nodes in this Node Pool have host encryption enabled? Defaults to false. bool false user_node_pool_enable_node_public_ip Should each node have a Public IP Address? Defaults to false. Changing this forces a new resource to be created. bool false user_node_pool_max_pods The maximum number of pods that can run on each agent. Changing this forces a new resource to be created. number 30 user_node_pool_node_labels A list of Kubernetes taints which should be applied to nodes in the agent pool (e.g key=value:NoSchedule). Changing this forces a new resource to be created. map(any) {\"kubernetes.azure.com/scalesetpriority\" = \"spot\"} user_node_pool_node_taints A map of Kubernetes labels which should be applied to nodes in this Node Pool. Changing this forces a new resource to be created. list(string) [\"kubernetes.azure.com/scalesetpriority=spot:NoSchedule\"] user_node_pool_os_disk_type The type of disk which should be used for the Operating System. Possible values are Ephemeral and Managed. Defaults to Managed. Changing this forces a new resource to be created. string Managed user_node_pool_os_type The type of Operating System. The operating system used on each Node in this Node Pool. string Linux user_node_pool_priority The priority of the Virtual Machines in the Virtual Machine Scale Set backing this Node Pool. string Regular user_node_pool_max_count The maximum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be greater than or equal to min_count. number 5 user_node_pool_min_count The minimum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be less than or equal to max_count. number 1 user_node_pool_node_count The initial number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be a value in the range min_count - max_count. number 2 user_node_pool_os_disk_size_gb The size of the OS Disk on each Node in this Node Pool. number 128 <p>Declare Variables variables.tf<pre><code>// ========================== Azure Kubernetes services (AKS)- User Node Pool ==========================\n\nvariable \"user_node_pool_enabled\" {\n  description = \"(Optional) Should the User Node Pool enabled? Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\n\nvariable \"user_node_pool_vm_size\" {\n  description = \"Specifies the vm size of the user node pool\"\n  default     = \"Standard_B4ms\"\n  type        = string\n}\n\nvariable \"user_node_pool_availability_zones\" {\n  description = \"Specifies the availability zones of the user node pool\"\n  default     = [\"1\", \"2\", \"3\"]\n  type        = list(string)\n}\n\nvariable \"user_node_pool_name\" {\n  description = \"Specifies the name of the user node pool\"\n  default     = \"agentpool\"\n  type        = string\n}\n\nvariable \"user_node_pool_subnet_name\" {\n  description = \"Specifies the name of the subnet that hosts the user node pool\"\n  default     = \"SystemSubnet\"\n  type        = string\n}\n\nvariable \"user_node_pool_subnet_address_prefix\" {\n  description = \"Specifies the address prefix of the subnet that hosts the user node pool\"\n  default     = [\"10.0.0.0/20\"]\n  type        = list(string)\n}\n\nvariable \"user_node_pool_enable_auto_scaling\" {\n  description = \"(Optional) Whether to enable auto-scaler. Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"user_node_pool_enable_host_encryption\" {\n  description = \"(Optional) Should the nodes in this Node Pool have host encryption enabled? Defaults to false.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"user_node_pool_enable_node_public_ip\" {\n  description = \"(Optional) Should each node have a Public IP Address? Defaults to false. Changing this forces a new resource to be created.\"\n  type        = bool\n  default     = false\n}\n\nvariable \"user_node_pool_max_pods\" {\n  description = \"(Optional) The maximum number of pods that can run on each agent. Changing this forces a new resource to be created.\"\n  type        = number\n  default     = 30\n}\n\nvariable \"user_node_pool_node_labels\" {\n  description = \"(Optional) A list of Kubernetes taints which should be applied to nodes in the agent pool (e.g key=value:NoSchedule). Changing this forces a new resource to be created.\"\n  type        = map(any)\n  default     = { \"kubernetes.azure.com/scalesetpriority\" = \"spot\" }\n}\n\nvariable \"user_node_pool_node_taints\" {\n  description = \"(Optional) A map of Kubernetes labels which should be applied to nodes in this Node Pool. Changing this forces a new resource to be created.\"\n  type        = list(string)\n  default     = [\"kubernetes.azure.com/scalesetpriority=spot:NoSchedule\"]\n}\n\nvariable \"user_node_pool_os_disk_type\" {\n  description = \"(Optional) The type of disk which should be used for the Operating System. Possible values are Ephemeral and Managed. Defaults to Managed. Changing this forces a new resource to be created.\"\n  type        = string\n  default     = \"Managed\"\n}\n\nvariable \"user_node_pool_os_type\" {\n  description = \"(Optional) The type of Operating System. The operating system used on each Node in this Node Pool.\"\n  type        = string\n  default     = \"Linux\"\n}\nvariable \"user_node_pool_priority\" {\n  description = \"(Optional) The priority of the Virtual Machines in the Virtual Machine Scale Set backing this Node Pool.\"\n  type        = string\n  default     = \"Regular\"\n}\n\n\nvariable \"user_node_pool_max_count\" {\n  description = \"(Required) The maximum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be greater than or equal to min_count.\"\n  type        = number\n  default     = 5\n}\n\nvariable \"user_node_pool_min_count\" {\n  description = \"(Required) The minimum number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be less than or equal to max_count.\"\n  type        = number\n  default     = 1\n}\n\nvariable \"user_node_pool_node_count\" {\n  description = \"(Optional) The initial number of nodes which should exist within this Node Pool. Valid values are between 0 and 1000 and must be a value in the range min_count - max_count.\"\n  type        = number\n  default     = 2\n}\nvariable \"user_node_pool_os_disk_size_gb\" {\n  description = \"(Optional) The size of the OS Disk on each Node in this Node Pool.\"\n  type        = number\n  default     = 128\n}\n</code></pre></p> <p>Define variables</p> <p><code>dev-variables.tfvar</code> - update this existing file for AKS values for development environment.</p> dev-variables.tfvar<pre><code># user_node_pool\nuser_node_pool_enabled              = true\nuser_node_pool_enable_auto_scaling  = true\nuser_node_pool_max_count            = 5\nuser_node_pool_min_count            = 1\nuser_node_pool_max_pods             = 110\nuser_node_pool_node_count           = 1\nuser_node_pool_node_labels          = {\"TenantName\" = \"tenant1\"}\nuser_node_pool_node_taints          = [\"TenantName=tenant1:NoSchedule\"]\nuser_node_pool_name                 = \"usernodepool\" #\"sysnodepool\"\nuser_node_pool_os_disk_size_gb      = 128\nuser_node_pool_vm_size              = \"Standard_B8ms\"\nuser_node_pool_availability_zones   = [\"1\", \"2\", \"3\"]\n</code></pre>"},{"location":"kubernetes/aks-create-nodepool/#step-2-create-a-new-user-node-pool-in-aks-using-terraform","title":"Step 2: Create a new user node pool in AKS using Terraform","text":"<p>Define the new user node pool in your Terraform configuration. Ensure that you specify the desired taint on the node pool.</p> aks.tf<pre><code>resource \"azurerm_kubernetes_cluster_node_pool\" \"user\" {\n  count                 = var.user_node_pool_enabled ? 1 : 0\n  zones                 = var.user_node_pool_availability_zones\n  enable_auto_scaling   = var.user_node_pool_enable_auto_scaling\n  os_disk_type          = var.user_node_pool_os_disk_type\n  os_type               = var.user_node_pool_os_type\n  priority              = var.user_node_pool_priority\n  os_disk_size_gb       = var.user_node_pool_os_disk_size_gb\n  vm_size               = var.user_node_pool_vm_size\n  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks.id\n  max_count             = var.user_node_pool_max_count\n  min_count             = var.user_node_pool_min_count\n  max_pods              = var.user_node_pool_max_pods\n  node_count            = var.user_node_pool_node_count\n  node_labels           = var.user_node_pool_node_labels\n  node_taints           = var.user_node_pool_node_taints\n  mode                  = \"User\"\n  name                  = var.user_node_pool_name\n  # orchestrator_version  = data.azurerm_kubernetes_service_versions.current.latest_version\n  vnet_subnet_id = azurerm_subnet.aks.id\n\n\n  tags = merge(local.default_tags, var.aks_tags)\n  lifecycle {\n    ignore_changes = [\n      tags,\n    ]\n  }\n}\n</code></pre> <p>Terraform validate</p> <pre><code>terraform validate\n# output\nSuccess! The configuration is valid.\n</code></pre> <p>run terraform plan &amp; apply again here.</p> <p>Terraform plan</p> <pre><code>terraform plan -out=dev-plan -var-file=\"./environments/dev-variables.tfvars\"\n</code></pre> <p>terraform apply</p> <pre><code>terraform apply dev-plan\n\n\n# output\nazurerm_kubernetes_cluster_node_pool.user[\"1\"]: Creating...\n.\n.\n.\n\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.\n\nOutputs:\n</code></pre>"},{"location":"kubernetes/aks-create-nodepool/#step-3-verify-the-new-user-node-pool-taint","title":"Step 3: Verify the new user node pool Taint","text":"<p>After creating the new user node pool, it's essential to confirm that taint has been successfully applied. You can use the azure portal to verification the process.</p> <p></p> <p>Node List Status Before and After:</p> <p>Before creating new user node pool, the list of nodes may look like this:</p> <p>Before</p> <pre><code>kubectl get nodes -o wide\n\n# output\nNAME                                   STATUS   ROLES   AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\naks-agentpool-25316841-vmss000000      Ready    agent   27h   v1.26.6   10.64.4.4     &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.1+azure-1\naks-agentpool-25316841-vmss000001      Ready    agent   26h   v1.26.6   10.64.4.113   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.1+azure-1\n</code></pre> <p>After</p> <p>After creating the new user node pool, the node list may include the new user node pool and look like this:</p> <pre><code>kubectl get nodes -o wide\n\n# output\nNAME                                   STATUS   ROLES   AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\naks-agentpool-25316841-vmss000000      Ready    agent   27h   v1.26.6   10.64.4.4     &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.1+azure-1\naks-agentpool-25316841-vmss000001      Ready    agent   26h   v1.26.6   10.64.4.113   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.1+azure-1\naks-usernodepool-19872531-vmss000000   Ready    agent   21m   v1.26.6   10.64.4.222   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.1+azure-1\n</code></pre>"},{"location":"kubernetes/aks-create-nodepool/#step-4-deploy-an-application-with-tolerations","title":"Step 4: Deploy an Application with Tolerations","text":"<p>Now, let's deploy your application onto the AKS cluster, considering the taint we've applied to the user node pool. To achieve this, you need to create a Kubernetes manifest for your application with tolerations that match the taint on the node pool. Here's an example of a tolerations YAML manifest:</p> <pre><code>      tolerations:\n        - key: \"TenantName\"\n          operator: \"Equal\"\n          value: \"tenant1\"\n          effect: \"NoSchedule\"  # This pod tolerates the taint\n</code></pre> <p>Now, let's integrate this into your complete deployment.yaml file:</p> deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: aspnet-api\n  namespace: sample\nspec:\n  replicas: 1\n  selector: \n    matchLabels:\n      app: aspnet-api\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  minReadySeconds: 5 \n  template:\n    metadata:\n      labels:\n        app: aspnet-api\n    spec:\n      nodeSelector:\n        \"kubernetes.io/os\": linux\n      serviceAccountName: default\n      containers:\n        - name: aspnet-api\n          image: acr1dev.azurecr.io/sample/aspnet-api:20230323.15\n          imagePullPolicy: Always\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n      tolerations:\n        - key: \"TenantName\"\n          operator: \"Equal\"\n          value: \"tenant1\"\n          effect: \"NoSchedule\"  # This pod tolerates the taint\n# kubectl apply -f deployment.yaml -n sample\n</code></pre> <p>Now that you have your deployment.yaml ready, proceed to the next steps.</p> <p>Deploy the Application</p> <p>To deploy your application to the AKS cluster, apply the Kubernetes manifest using the following command:</p> <pre><code>kubectl apply -f deployment.yaml\n</code></pre>"},{"location":"kubernetes/aks-create-nodepool/#step-5-verify-your-application-status","title":"Step 5: Verify Your Application Status","text":"<p>To ensure your application has been successfully deployed, you can check the status of your application pods using <code>kubectl</code>. Here's how:</p> <p>Before Applying the Tolerations: Before</p> <pre><code># output\nsample              aspnet-api-7d96f84c56-88dnz                         1/1     Running   0          3m31s   10.64.4.225   aks-agentpool-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nsample              aspnet-app-c5c847d44-tfhw6                          1/1     Running   0          7s      10.64.4.248   aks-agentpool-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>After Applying the Tolerations:</p> <pre><code>kubectl get pods -o wide -A\n\n# output\nsample              aspnet-api-7d96f84c56-88dnz                         1/1     Running   0          3m31s   10.64.4.225   aks-usernodepool-19872531-vmss000000   &lt;none&gt;           &lt;none&gt;\nsample              aspnet-app-c5c847d44-tfhw6                          1/1     Running   0          7s      10.64.4.248   aks-usernodepool-19872531-vmss000000   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>You should now observe your application pods running on nodes with the corresponding taint, as indicated by the change in the node pool. This ensures that your pods are scheduled on the appropriate nodes, considering the taints and tolerations you've configured.</p>"},{"location":"kubernetes/aks-create-nodepool/#conclusion","title":"Conclusion","text":"<p>You've successfully created a new user node pool in AKS using Terraform, applied a taint to it, and deployed an application with tolerations to ensure it runs on nodes with the taint.</p>"},{"location":"kubernetes/aks-upgrade-nodepool/","title":"Upgrade or Resize node pools in AKS","text":""},{"location":"kubernetes/aks-upgrade-nodepool/#introduction","title":"Introduction","text":"<p>In the Microservices containerized applications, it's often necessary to adjust the capacity of your Azure Kubernetes Service (AKS) clusters. This guide will walk you through the process of upgrading or resizing a node pool in AKS. </p>"},{"location":"kubernetes/aks-upgrade-nodepool/#prerequisites","title":"Prerequisites:","text":"<p>Before proceeding, ensure you have the following:</p> <ul> <li><code>AKS</code> Azure Kubernetes Service cluster set up.</li> <li><code>kubectl</code> command-line tool installed.</li> </ul>"},{"location":"kubernetes/aks-upgrade-nodepool/#objective","title":"Objective","text":"<p>In this exercise we will accomplish &amp; learn how to implement following:</p> <ul> <li>Step 1: Check status of existing nodes and pods</li> <li>Step 2: Create a new node pool with the desired SKU</li> <li>Step 3: Cordon the existing nodes</li> <li>Step 4: Drain the existing nodes</li> <li>Step 5: Validate new node status</li> <li>Step 6: Remove the existing node pool</li> </ul>"},{"location":"kubernetes/aks-upgrade-nodepool/#login-to-azure","title":"login to Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Set Azure subscription\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#connect-to-cluster","title":"Connect to Cluster","text":"<p>Use the following command to connect to your AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#technical-scenario","title":"Technical Scenario","text":"<p>We'll assume a scenario where you need to resize an existing node pool named <code>nodepool1</code> from the SKU size <code>Standard_B8ms</code> to <code>Standard_D8s_v5</code>. This process involves creating a new node pool <code>nodepool2</code>, moving workloads, and then removing the old node pool.</p> <ul> <li>Existing node pool named <code>nodepool1</code></li> <li>Existing VM size <code>Standard_B8ms</code></li> <li>New node pool named <code>nodepool2</code></li> <li>New node pool named <code>Standard_D8s_v5</code></li> </ul>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-1-check-status-of-existing-nodes-and-pods","title":"Step-1: Check status of existing nodes and pods","text":"<p>Use the following command to check the current status of nodes and pods in your AKS cluster:</p> <p><pre><code>kubectl get nodes -o wide\n\n# output\nNAME                                STATUS   ROLES   AGE    VERSION    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\naks-nodepool1-25316841-vmss000000   Ready    agent   198d   v1.23.12   10.64.4.4     &lt;none&gt;        Ubuntu 18.04.6 LTS   5.4.0-1101-azure   containerd://1.6.15+azure-1\naks-nodepool1-25316841-vmss000001   Ready    agent   198d   v1.23.12   10.64.4.113   &lt;none&gt;        Ubuntu 18.04.6 LTS   5.4.0-1101-azure   containerd://1.6.15+azure-1\n</code></pre> This provides an overview of your existing nodes and running pods.</p> <pre><code>kubectl get pods -o wide -A\n\n# output\nkube-system         azure-policy-557988f6df-q5rgf                       1/1     Running   0             23d    10.64.4.171   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         azure-policy-webhook-5dfcfc5998-cn7p6               1/1     Running   0             23d    10.64.4.161   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         cloud-node-manager-j8zg4                            1/1     Running   1 (23d ago)   72d    10.64.4.4     aks-nodepool1-25316841-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system         cloud-node-manager-p8nk5                            1/1     Running   0             72d    10.64.4.113   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         coredns-785fcf7bdd-6d84r                            1/1     Running   0             82d    10.64.4.214   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         coredns-785fcf7bdd-n8bvt                            1/1     Running   0             23d    10.64.4.199   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         coredns-autoscaler-65bb858f95-6lwkf                 1/1     Running   0             23d    10.64.4.117   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         csi-azuredisk-node-2n6md                            3/3     Running   0             10d    10.64.4.4     aks-nodepool1-25316841-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system         csi-azuredisk-node-r7snn                            3/3     Running   0             10d    10.64.4.113   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         csi-azurefile-node-7tjxl                            3/3     Running   0             3d1h   10.64.4.113   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         csi-azurefile-node-nfntk                            3/3     Running   0             3d1h   10.64.4.4     aks-nodepool1-25316841-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system         konnectivity-agent-6d4987776d-2wbc2                 1/1     Running   0             23d    10.64.4.142   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         konnectivity-agent-6d4987776d-hdgx9                 1/1     Running   0             82d    10.64.4.204   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         kube-proxy-dvl5k                                    1/1     Running   0             44d    10.64.4.113   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         kube-proxy-jx2qw                                    1/1     Running   1 (23d ago)   44d    10.64.4.4     aks-nodepool1-25316841-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system         metrics-server-7757d565cf-rrktr                     2/2     Running   0             41d    10.64.4.181   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system         metrics-server-7757d565cf-zz97t                     2/2     Running   0             23d    10.64.4.190   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nsample              aks-helloworld-one-6965865b8b-dk6w9                 1/1     Running   0             192d   10.64.4.116   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nsample              aks-helloworld-two-66c5cf894b-vmztv                 1/1     Running   0             192d   10.64.4.115   aks-nodepool1-25316841-vmss000001   &lt;none&gt;           &lt;none&gt;\nsample              aspnet-api-79b4cbf4bb-54cng                         1/1     Running   0             10d    10.64.4.100   aks-nodepool1-25316841-vmss000000   &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>Check the status of your node pools </p> <p><pre><code>az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster\n\n# example\naz aks nodepool list --resource-group 'rg-aks-dev' --cluster-name 'aks-cluster1-dev'\n\n# output \n[\n  {\n    \"availabilityZones\": null,\n    \"capacityReservationGroupId\": null,\n    \"count\": 2,\n    \"creationData\": null,\n    \"currentOrchestratorVersion\": \"1.24.9\",\n    \"enableAutoScaling\": true,\n    \"enableCustomCaTrust\": false,\n    \"enableEncryptionAtHost\": false,\n    \"enableFips\": false,\n    \"enableNodePublicIp\": false,\n    \"enableUltraSsd\": false,\n    \"gpuInstanceProfile\": null,\n    \"hostGroupId\": null,\n    \"id\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourcegroups/rg-aks-dev/providers/Microsoft.ContainerService/managedClusters/aks-cluster1-dev/agentPools/agentpool\",\n    \"kubeletConfig\": null,\n    \"kubeletDiskType\": \"OS\",\n    \"linuxOsConfig\": null,\n    \"maxCount\": 5,\n    \"maxPods\": 110,\n    \"messageOfTheDay\": null,\n    \"minCount\": 2,\n    \"mode\": \"System\",\n    \"name\": \"agentpool\",\n    \"networkProfile\": null,\n    \"nodeImageVersion\": \"AKSUbuntu-1804gen2containerd-2023.02.09\",\n    \"nodeLabels\": null,\n    \"nodePublicIpPrefixId\": null,\n    \"nodeTaints\": null,\n    \"orchestratorVersion\": \"1.24.9\",\n    \"osDiskSizeGb\": 128,\n    \"osDiskType\": \"Managed\",\n    \"osSku\": \"Ubuntu\",\n    \"osType\": \"Linux\",\n    \"podSubnetId\": null,\n    \"powerState\": {\n      \"code\": \"Running\"\n    },\n    \"provisioningState\": \"Failed\",\n    \"proximityPlacementGroupId\": null,\n    \"resourceGroup\": \"rg-aks-dev\",\n    \"scaleDownMode\": \"Delete\",\n    \"scaleSetEvictionPolicy\": null,\n    \"scaleSetPriority\": null,\n    \"spotMaxPrice\": null,\n    \"tags\": null,\n    \"type\": \"Microsoft.ContainerService/managedClusters/agentPools\",\n    \"typePropertiesType\": \"VirtualMachineScaleSets\",\n    \"upgradeSettings\": {\n      \"maxSurge\": null\n    },\n    \"vmSize\": \"Standard_B4ms\",\n    \"vnetSubnetId\": \"/subscriptions/b635d52c-5170-4366-b262-cc12cba2d9be/resourceGroups/rg-vnet1-dev/providers/Microsoft.Network/virtualNetworks/vnet-spoke-dev/subnets/snet-aks1\",\n    \"windowsProfile\": null,\n    \"workloadRuntime\": null\n  }\n]\n</code></pre> Use the following command to find out what versions are currently available for your subscription and region. The following example lists the available Kubernetes versions for the EastUS region:</p> <p><pre><code>az aks get-versions --location eastus --output table\n# or\naz aks get-versions --location eastus --query \"orchestrators\" -o table\n\n# output\nOrchestratorType    OrchestratorVersion    Default\n------------------  ---------------------  ---------\nKubernetes          1.27.3\nKubernetes          1.27.1\nKubernetes          1.26.6                 True\nKubernetes          1.26.3\nKubernetes          1.25.11\nKubernetes          1.25.6\n</code></pre> The kubectl describe nodes command provides status information of nodes</p> <pre><code>kubectl describe nodes aks-nodepool1-23144520-vmss00001s\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-2-create-a-new-node-pool-with-the-desired-sku","title":"Step-2: Create a new node pool with the desired SKU","text":"<p>Use the az aks nodepool add command to create a new node pool called <code>nodepool2</code> with required number of nodes using the <code>Standard_DS3_v2</code> VM SKU:</p> <ul> <li>Existing node pool named <code>nodepool1</code></li> <li>Existing VM size <code>Standard_B8ms</code></li> <li>New node pool named <code>nodepool2</code></li> <li>New node pool named <code>Standard_D8s_v5</code></li> </ul> <pre><code>az aks nodepool add \\ \n    --resource-group myResourceGroup \\ \n    --cluster-name myAKSCluster \\ \n    --name nodepool2 \\ \n    --node-count 3 \\ \n    --node-vm-size Standard_D8s_v5 \\ \n    --mode System \\ \n    --no-wait\n\n# bash example \naz aks nodepool add \\\n    --resource-group 'rg-aks-dev' \\\n    --cluster-name 'aks-cluster1-dev' \\\n    --name nodepool2 \\\n    --node-count 3 \\\n    --node-vm-size Standard_D8s_v5 \\\n    --mode System \\\n    --no-wait\n\n# powershell System example \n\naz aks nodepool add `\n    --resource-group 'rg-aks-dev' `\n    --cluster-name 'aks-cluster1-dev' `\n    --enable-cluster-autoscaler `\n    --name nodepool2 `\n    --node-count 2 `\n    --min-count 1 `\n    --max-count 3 `\n    --max-pods 50 `\n    --node-vm-size Standard_D8s_v5 `\n    --mode System `\n    --no-wait\n\n# output\nThe behavior of this command has been altered by the following extension: aks-preview\n</code></pre> <p>After a few minutes, the new node pool has been created:</p> <pre><code>kubectl get nodes\n\n# output\nNAME                                 STATUS   ROLES   AGE   VERSION\naks-nodepool1-20823458-vmss000000   Ready    agent   23m   v1.21.9\naks-nodepool1-20823458-vmss000001   Ready    agent   23m   v1.21.9\naks-nodepool2-31721111-vmss000000    Ready    agent   10d   v1.21.9\naks-nodepool2-31721111-vmss000001    Ready    agent   10d   v1.21.9\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-3-cordon-the-existing-nodes","title":"Step-3: Cordon the existing nodes","text":"<p>Cordoning marks specified nodes as unschedulable and prevents any more pods from being added to the nodes.</p> <p>Next, using <code>kubectl cordon &lt;node-names&gt;</code>, specify the desired nodes in a space-separated list:</p> <pre><code>kubectl cordon `\naks-nodepool1-23546727-vmss00001r `\naks-nodepool1-23546727-vmss00001s\n\n# output\nnode/aks-nodepool1-23546727-vmss00001r cordoned\naks-nodepool1-23546727-vmss00001s cordoned\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-4-drain-the-existing-nodes","title":"Step-4: Drain the existing nodes","text":"<p>Draining nodes will cause pods running on them to be evicted and recreated on the other, schedulable nodes.</p> <p>To drain nodes, use <code>kubectl drain &lt;node-names&gt; --ignore-daemonsets --delete-emptydir-data</code>, again using a space-separated list of node names:</p> <p><pre><code>kubectl drain `\naks-nodepool1-23546727-vmss00001r `\naks-nodepool1-23546727-vmss00001s `\n--ignore-daemonsets --delete-emptydir-data\n</code></pre> After the drain operation finishes, all pods other than those controlled by daemon sets are running on the new node pool:</p> <p>This ensures that the pods are safely moved to other nodes.</p>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-5-validate-new-node-status","title":"Step 5: Validate new node status","text":"<p>Check the status of the new nodes in the <code>nodepool2</code>:</p> <p><pre><code>kubectl get nodes\n\n#output \nNAME                                STATUS                     ROLES   AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME   \naks-nodepool1-40415315-vmss000000   Ready,SchedulingDisabled   agent   53d     v1.28.0   10.65.4.4     &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.5-1\naks-nodepool1-40415315-vmss000001   Ready,SchedulingDisabled   agent   53d     v1.28.0   10.65.4.113   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1041-azure   containerd://1.7.5-1\naks-nodepool2-44502347-vmss000000    Ready                      agent   5m49s   v1.28.3   10.65.4.222   &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1051-azure   containerd://1.7.5-1\naks-nodepool2-44502347-vmss000001    Ready                      agent   5m51s   v1.28.3   10.65.5.22    &lt;none&gt;        Ubuntu 22.04.3 LTS   5.15.0-1051-azure   containerd://1.7.5-1\n</code></pre> Ensure that the new nodes are in the <code>Ready</code> state.</p> <pre><code>kubectl get pods -o wide -A\n\n# output\nNAMESPACE     NAME                                  READY   STATUS    RESTARTS   AGE     IP           NODE                                 NOMINATED NODE   READINESS GATES\ndefault       sampleapp2-74b4b974ff-676sz           1/1     Running   0          15m     10.244.4.5   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\ndefault       sampleapp2-76b6c4c59b-rhmzq           1/1     Running   0          16m     10.244.4.3   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-4n66k             1/1     Running   0          10d     10.240.0.6   aks-nodepool2-31721111-vmss000002    &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-9p4c8             1/1     Running   0          10d     10.240.0.4   aks-nodepool2-31721111-vmss000000    &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-nb7mx             1/1     Running   0          10d     10.240.0.5   aks-nodepool2-31721111-vmss000001    &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-sxn96             1/1     Running   0          49m     10.240.0.9   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-tsq98             1/1     Running   0          49m     10.240.0.8   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   azure-ip-masq-agent-xzrdl             1/1     Running   0          49m     10.240.0.7   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-845757d86-d2pkc               1/1     Running   0          17m     10.244.3.2   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-845757d86-f8g9s               1/1     Running   0          17m     10.244.5.2   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-autoscaler-5f85dc856b-f8xh2   1/1     Running   0          17m     10.244.4.2   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-7md2w              3/3     Running   0          49m     10.240.0.7   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-9nfzt              3/3     Running   0          10d     10.240.0.4   aks-nodepool2-31721111-vmss000000    &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-bblsb              3/3     Running   0          10d     10.240.0.5   aks-nodepool2-31721111-vmss000001    &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-lcmtz              3/3     Running   0          49m     10.240.0.9   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-mmncr              3/3     Running   0          49m     10.240.0.8   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azuredisk-node-tjhj4              3/3     Running   0          10d     10.240.0.6   aks-nodepool2-31721111-vmss000002    &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-29w6z              3/3     Running   0          49m     10.240.0.9   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-4nrx7              3/3     Running   0          49m     10.240.0.7   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-9pcr8              3/3     Running   0          3d11h   10.240.0.6   aks-nodepool2-31721111-vmss000002    &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-bh2pc              3/3     Running   0          3d11h   10.240.0.5   aks-nodepool2-31721111-vmss000001    &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-gqqnv              3/3     Running   0          49m     10.240.0.8   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   csi-azurefile-node-h75gq              3/3     Running   0          3d11h   10.240.0.4   aks-nodepool2-31721111-vmss000000    &lt;none&gt;           &lt;none&gt;\nkube-system   konnectivity-agent-6cd55c69cf-2bbp5   1/1     Running   0          17m     10.240.0.7   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   konnectivity-agent-6cd55c69cf-7xzxj   1/1     Running   0          16m     10.240.0.8   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-4wzx7                      1/1     Running   0          10d     10.240.0.4   aks-nodepool2-31721111-vmss000000    &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-7h8r5                      1/1     Running   0          49m     10.240.0.7   aks-nodepool2-20823458-vmss000000   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-g5tvr                      1/1     Running   0          10d     10.240.0.6   aks-nodepool2-31721111-vmss000002    &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-mrv54                      1/1     Running   0          10d     10.240.0.5   aks-nodepool2-31721111-vmss000001    &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-nqmnj                      1/1     Running   0          49m     10.240.0.9   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-zn77s                      1/1     Running   0          49m     10.240.0.8   aks-nodepool2-20823458-vmss000001   &lt;none&gt;           &lt;none&gt;\nkube-system   metrics-server-774f99dbf4-2x6x8       1/1     Running   0          16m     10.244.4.4   aks-nodepool2-20823458-vmss000002   &lt;none&gt;           &lt;none&gt;\n</code></pre>"},{"location":"kubernetes/aks-upgrade-nodepool/#step-6-remove-the-existing-node-pool","title":"Step-6: Remove the existing node pool","text":"<p>To delete the existing node pool, use the Azure portal or the az aks nodepool delete command:</p> <p><pre><code>az aks nodepool delete `\n    --resource-group 'rg-aks-dev' `\n    --cluster-name 'aks-cluster1-dev' `\n    --name agentpool\n</code></pre> After completion, the final result is the AKS cluster having a single, new node pool with the new, desired SKU size and all the applications and pods properly running:</p>"},{"location":"kubernetes/aks-upgrade-nodepool/#conclusion","title":"Conclusion","text":"<p>You have successfully upgraded or resized a node pool in Azure Kubernetes Service. This process ensures a smooth transition of workloads to the new nodes, minimizing disruptions to your applications. Adjust the commands and configurations based on your specific requirements and cluster setup.</p>"},{"location":"kubernetes/aks-upgrade-nodepool/#reference","title":"Reference","text":"<ul> <li>Resize node pools in AKS</li> <li>Manage system node pools in Azure Kubernetes Service (AKS)</li> </ul>"},{"location":"kubernetes/pod-troubleshooting/","title":"Kubernetes Pod troubleshooting","text":""},{"location":"kubernetes/pod-troubleshooting/#introduction","title":"Introduction","text":"<p>There are not hard and fast rules for troubleshooting AKS pods, these are some basic general guidelines that may help for your troubleshooting.</p> <p>Troubleshooting Kubernetes Pods can be a complex process, but there are several steps you can take to identify and resolve issues. Here are some general steps to follow when troubleshooting Kubernetes Pods:</p>"},{"location":"kubernetes/pod-troubleshooting/#login-to-azure","title":"login to Azure","text":"<p>Verify that you are logged into the right Azure subscription before start anything in visual studio code</p> <pre><code># Login to Azure\naz login \n\n# Set Azure subscription\naz account set -s \"anji.keesari\"\n</code></pre>"},{"location":"kubernetes/pod-troubleshooting/#connect-to-cluster","title":"Connect to Cluster","text":"<p>Use the following command to connect to your AKS cluster.</p> <pre><code># Azure Kubernetes Service Cluster User Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\"\n\n# Azure Kubernetes Service Cluster Admin Role\naz aks get-credentials -g \"rg-aks-dev\" -n \"aks-cluster1-dev\" --admin\n\n# get nodes\nkubectl get no\nkubectl get namespace -A\n</code></pre>"},{"location":"kubernetes/pod-troubleshooting/#verify-the-pod-status","title":"Verify the pod status","text":"<p>This is the first thing we want to check, verify the pod status by running following commands.</p> <pre><code>kubectl get pods -n sample\nkubectl get svc -n sample\n</code></pre> <p>If the Pod is in a \"Pending\" state, it may indicate a problem with the node or resource allocation. If the Pod is in a \"Running\" state, but the application is not working correctly, it may indicate an issue with the container or application configuration.</p>"},{"location":"kubernetes/pod-troubleshooting/#describe-the-pod","title":"Describe the pod","text":"<p>To describe a pod in Kubernetes, you can use the kubectl describe command. </p> <pre><code>kubectl describe pod/aspnet-api-6b7d8fbf9f-mqtnd -n sample\n</code></pre> <p>This can help you identify any issues with the container, such as a crash or resource allocation problems.</p> <p>this command will also allow us to view the configuration for the Pod, including any environment variables or volume mounts. This can help you identify any issues with the Pod configuration that may be causing problems.</p>"},{"location":"kubernetes/pod-troubleshooting/#check-the-logs","title":"Check the logs","text":"<p>Use this command to view the logs generated by the pod and its containers. This can give you valuable information about what is causing the issue with the pod.</p> <pre><code>kubectl logs pod/aspnet-api-6b7d8fbf9f-mqtnd -n sample\n# since 5 mins\nkubectl logs --since=5m pod/aspnet-api-6b7d8fbf9f-mqtnd -n sample\n</code></pre> <p>This can help you identify any error messages or issues with the application.</p>"},{"location":"kubernetes/pod-troubleshooting/#debug-the-container","title":"Debug the container","text":"<p>For further debugging, let's dig into the container itself by running following commands, this will allows you to execute a command in a container in a pod.</p> <pre><code>kubectl exec -n sample -it aspnet-api-6b7d8fbf9f-mqtnd -- bash\n# or\nkubectl exec -n sample -it aspnet-api-6b7d8fbf9f-mqtnd -- /bin/sh\n# or\nkubectl exec -n sample -it aspnet-api-6b7d8fbf9f-mqtnd -- sh\n</code></pre> <p>if you want to check the deployed files run <code>ls</code> command inside the container. <pre><code>root@aspnet-api-6b7d8fbf9f-mqtnd:/app# ls\nAspNetApi            AspNetApi.runtimeconfig.json                         Newtonsoft.Json.dll                    appsettings.Development.json\nAspNetApi.deps.json  Microsoft.AspNetCore.Mvc.Versioning.ApiExplorer.dll  Swashbuckle.AspNetCore.Swagger.dll     appsettings.json\nAspNetApi.dll        Microsoft.AspNetCore.Mvc.Versioning.dll              Swashbuckle.AspNetCore.SwaggerGen.dll  manifest\nAspNetApi.pdb        Microsoft.OpenApi.dll                                Swashbuckle.AspNetCore.SwaggerUI.dll   web.config\n</code></pre></p> <p><code>kubectl exec</code> command will also allow us to check network connectivity, use the <code>kubectl exec</code> command to connect to the container and test network connectivity to other services or resources.</p>"},{"location":"kubernetes/pod-troubleshooting/#exit-from-the-container","title":"Exit from the container","text":"<p>run the <code>exit</code> command to exit from running container.</p> <pre><code># exit\n</code></pre>"},{"location":"kubernetes/pod-troubleshooting/#install-curl-inside-container","title":"install curl inside container","text":"<p>Use following apt commands to install tools as per the need while debugging.</p> <pre><code>apt-get -y update; apt-get -y install curl\n</code></pre> <p>output <pre><code>Get:1 http://deb.debian.org/debian bullseye InRelease [116 kB]\nGet:2 http://deb.debian.org/debian-security bullseye-security InRelease [48.4 kB]\nGet:3 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\nGet:4 http://deb.debian.org/debian bullseye/main amd64 Packages [8183 kB]\nGet:5 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [229 kB]\nGet:6 http://deb.debian.org/debian bullseye-updates/main amd64 Packages [14.6 kB]\nFetched 8635 kB in 1s (5772 kB/s)\nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n.\n.\n.\n</code></pre></p>"},{"location":"kubernetes/pod-troubleshooting/#curl-internal-pod-url","title":"curl internal pod URL","text":"<p>You can use the curl command inside the container to check the network connectivity of the pods installed on the same AKS cluster.</p> <pre><code>curl http://aspnet-api.namespace.svc.cluster.local:80/aspnetapi\ncurl http://aspnet-api.sample.svc.cluster.local:80/aspnetapi\n</code></pre>"},{"location":"kubernetes/pod-troubleshooting/#restart-the-pod","title":"Restart the pod:","text":"<p>To restart a pod in Kubernetes, you can use the <code>kubectl delete pod</code> command followed by the name of the pod you want to restart. When you delete a pod, Kubernetes will automatically create a new one to replace it.</p> <p><pre><code>kubectl delete pod/aspnet-api-6b7d8fbf9f-mqtnd -n sample\n</code></pre> output <pre><code>pod \"aspnet-api-6b7d8fbf9f-mqtnd\" deleted\n</code></pre></p> <p>Alternatively, you can use a deployment to manage the desired number of replicas of a pod, and perform a rolling update of the deployment to restart pods one by one. This approach avoids downtime and ensures high availability of your application.</p> <p>It's also important to keep in mind that the troubleshooting steps may vary depending on the specific problem with the pod.</p>"},{"location":"kubernetes/pod-troubleshooting/#print-environment-variables-in-pod","title":"Print environment variables in pod","text":"<p>To print environment variables in a pod within a Kubernetes cluster, you can use following command.</p> <pre><code>kubectl exec aspnet-api-6b7d8fbf9f-mqtnd -n sample -- printenv\nkubectl exec aspnet-api-6b7d8fbf9f-mqtnd -n sample -- env\n</code></pre>"},{"location":"kubernetes/pod-troubleshooting/#reference","title":"Reference","text":"<ul> <li>https://learn.microsoft.com/en-us/azure/aks/ingress-basic?tabs=azure-powershell</li> </ul>"},{"location":"kubernetes/monitoring/grafana-workspace/","title":"Create an Azure Managed Grafana Using Terraform","text":""},{"location":"kubernetes/monitoring/grafana-workspace/#introduction","title":"Introduction","text":"<p>Azure Managed Grafana is a fully managed service that provides powerful, and flexible tool for visualizing and analyzing data. It allows seamless integration with various Azure services, including Azure Monitor and Log Analytics, making it a great choice for monitoring applications running on Azure Kubernetes Service (AKS).</p> <p>In this article, we will explore how to create and configure an Azure Managed Grafana workspace using Terraform.</p>"},{"location":"kubernetes/monitoring/grafana-workspace/#technical-scenario","title":"Technical Scenario","text":"<p>As a cloud infrastructure engineer, I want to set up Azure Managed Grafana for monitoring workloads running on Azure Kubernetes Service (AKS), So that we can visualize real-time metrics, logs, and health of our AKS clusters and applications.</p> <p>Acceptance Criteria:</p> <ol> <li>A dedicated Azure Managed Grafana workspace should be provisioned.</li> <li>The workspace should be securely connected to an Azure Monitor and Log Analytics workspace to fetch telemetry data.</li> <li>Diagnostic settings for the Grafana workspace should be enabled to capture logs and metrics for troubleshooting.</li> <li>Private endpoint access should be configured to ensure secure connectivity.</li> <li>Role-based access control (RBAC) should be implemented for Grafana access, with specific permissions granted to users and groups.</li> <li>The setup should be automated using terraform to ensure consistency across environments and to simplify maintenance.</li> </ol> <p>Why Azure Managed Grafana?</p> <ul> <li>Seamless Integration: Azure Managed Grafana integrates effortlessly with Azure services such as Azure Monitor, Log Analytics, and Azure Metrics, offering a smooth experience for visualizing metrics from AKS and other cloud-native applications.</li> <li>Scalability: The service automatically scales with your workload, so as your AKS clusters grow, your monitoring infrastructure can scale with it.</li> <li>Security: The service provides built-in security features such as RBAC, private endpoints, and audit logging, making it a suitable choice for regulated environments.</li> </ul>"},{"location":"kubernetes/monitoring/grafana-workspace/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following prerequisites are met:</p> <ul> <li>An active Azure subscription with sufficient permissions (Contributor or Owner role).</li> <li>Terraform installed and configured locally.</li> <li>Familiarity with terraform syntax and Azure resources.</li> <li>Azure CLI installed for authentication purposes.</li> <li>Predefined Azure AD groups for managing role assignments.</li> <li>Basic knowledge of Azure Monitor and its resources.</li> <li>Azure Log Analytics Workspace: You need an existing Azure Log Analytics workspace for setting up diagnostics</li> </ul>"},{"location":"kubernetes/monitoring/grafana-workspace/#implementation-details","title":"Implementation Details","text":"<p>The following steps detail the implementation process:</p>"},{"location":"kubernetes/monitoring/grafana-workspace/#task-1-configure-terraform-variables-for-azure-managed-grafana","title":"Task-1: Configure terraform variables for Azure Managed Grafana","text":"<p>Start by defining the necessary variables in the <code>variables.tf</code> file to make your terraform configuration modular and reusable:</p> <pre><code># Configure terraform variables for Azure Managed Grafana\nvariable \"grafana_name_prefix\" {\n  type        = string\n  default     = \"amg\"\n  description = \"Prefix of the  Azure Managed Grafana name that's combined with name of the  Azure Managed Grafana.\"\n}\nvariable \"grafana_name\" {\n  description = \"(Required) Specifies the name which should be used for this Dashboard Grafana\"\n  type        = string\n  default     = \"grafana\"\n}\nvariable \"api_key_enabled\" {\n  description = \" (Optional) Whether to enable the api key setting of the Grafana instance. Defaults to false.\"\n  type        = bool\n  default     = false\n}\nvariable \"grafana_major_version\" {\n  description = \"(Optional) Which major version of Grafana to deploy. Defaults to 9. Possible values are 9, 10\"\n  type        = number\n  default     = 10\n}\nvariable \"deterministic_outbound_ip_enabled\" {\n  description = \"(Optional) Whether to enable the Grafana instance to use deterministic outbound IPs. Defaults to false.\"\n  type        = bool\n  default     = false\n}\nvariable \"grafana_public_access\" {\n  description = \"(Optional) Whether to enable traffic over the public interface. Defaults to true.\"\n  type        = bool\n  default     = true\n}\nvariable \"monitoring_tags\" {\n  description = \"(Optional) A mapping of tags which should be assigned to the Azure Monitor Workspace.\"\n  type        = map(any)\n  default     = {\n    \"Application\" = \"Monitoring\"\n  }\n}\nvariable \"diagnostic_setting_name\" {\n  description = \"The name of this Diagnostic Setting.\"\n  type        = string\n  default     = \"diag-grafana\"\n}\nvariable \"diagnostic_setting_enabled_log_categories\" {\n  description = \"A list of log categories to be enabled for this diagnostic setting.\"\n  type        = list(string)\n  default     = [\"GrafanaLoginEvents\"]\n}\nvariable \"grafana_rbac\" {}\n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#task-2-create-an-azure-managed-grafana-using-terraform","title":"Task-2: Create an Azure Managed Grafana using terraform","text":"<p>Provision an Azure Managed Grafana, which is the backbone of Azure's monitoring services:</p> <pre><code># Step-1: Create the Azure Managed Grafana\nresource \"azurerm_dashboard_grafana\" \"grafana\" {\n  name                              = \"${var.grafana_name_prefix}-${var.grafana_name}-${local.environment}\"\n  resource_group_name               = azurerm_resource_group.monitoring.name\n  location                          = azurerm_resource_group.monitoring.location\n  sku                               = \"Standard\"\n  public_network_access_enabled     = var.grafana_public_access\n  api_key_enabled                   = var.api_key_enabled\n  deterministic_outbound_ip_enabled = var.deterministic_outbound_ip_enabled\n  grafana_major_version             = var.grafana_major_version\n  tags                              = merge(local.default_tags, var.monitoring_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags,\n    ]\n  }\n  identity {\n    type = \"SystemAssigned\"\n  }\n\n  azure_monitor_workspace_integrations {\n    resource_id = azurerm_monitor_workspace.amw.id\n  }\n  depends_on = [\n    azurerm_resource_group.monitoring,\n    azurerm_monitor_workspace.amw\n  ]\n}\n</code></pre> <p>This code will create a Grafana workspace linked to a Log Analytics workspace for monitoring.</p>"},{"location":"kubernetes/monitoring/grafana-workspace/#task-4-configure-diagnostic-settings-for-azure-managed-grafana","title":"Task-4: Configure diagnostic settings for Azure Managed Grafana","text":"<p>Diagnostic settings allow you to monitor and troubleshoot the performance of your Azure Managed Grafana workspace.</p> <pre><code># Step-2: Configure diagnostic settings for Azure Managed Grafana\nresource \"azurerm_monitor_diagnostic_setting\" \"grafana\" {\n  name                       = var.diagnostic_setting_name\n  target_resource_id         = azurerm_dashboard_grafana.grafana.id\n  log_analytics_workspace_id = data.azurerm_log_analytics_workspace.workspace.id\n\n  dynamic \"enabled_log\" {\n    for_each = toset(var.diagnostic_setting_enabled_log_categories)\n\n    content {\n      category = enabled_log.value\n    }\n  }\n  metric {\n    category = \"AllMetrics\"\n    enabled  = false\n  }\n  depends_on = [\n    azurerm_dashboard_grafana.grafana,\n    data.azurerm_log_analytics_workspace.workspace\n  ]\n}\n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-4-create-a-private-endpoint-for-azure-managed-grafana","title":"Step 4: Create a private endpoint for Azure Managed Grafana","text":"<p>To ensure secure access to the Azure Managed Grafana workspace, create a private endpoint.</p> <pre><code># Step-3 Create a private endpoint for Azure Managed Grafana\nresource \"azurerm_private_endpoint\" \"pe_grafana\" {\n  name                = lower(\"${var.private_endpoint_prefix}-${azurerm_dashboard_grafana.grafana.name}\")\n  location            = var.location # azurerm_dashboard_grafana.grafana.location\n  resource_group_name = azurerm_dashboard_grafana.grafana.resource_group_name\n  subnet_id           = data.azurerm_subnet.jumpserver.id\n  tags                = merge(local.default_tags)\n\n  private_service_connection {\n    name                           = \"pe-${azurerm_dashboard_grafana.grafana.name}\"\n    private_connection_resource_id = azurerm_dashboard_grafana.grafana.id\n    is_manual_connection           = false\n    subresource_names              = var.pe_grafana_subresource_names\n    request_message                = try(var.request_message, null)\n  }\n\n  private_dns_zone_group {\n    name                 = \"default\"\n    private_dns_zone_ids = [data.azurerm_private_dns_zone.pdz_grafana_core_sub.id]\n  }\n\n  lifecycle {\n    ignore_changes = [\n      custom_network_interface_name,\n      # location\n    ]\n  }\n\n  depends_on = [\n    data.azurerm_subnet.jumpserver,\n    azurerm_dashboard_grafana.grafana,\n    data.azurerm_private_dns_zone.pdz_grafana_core_sub\n  ]\n}   \n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-5-create-role-assignment-over-resource-group-containing-the-azure-log-analytics-workspace","title":"Step 5: Create role assignment over resource group containing the azure log analytics workspace","text":"<p>To enable access for the Grafana workspace to the Log Analytics workspace, create a role assignment.</p> <pre><code># Step-4: Add required role assignment over resource group containing the Azure Monitor Workspace\nresource \"azurerm_role_assignment\" \"grafana\" {\n  scope                = azurerm_resource_group.monitoring.id\n  role_definition_name = \"Monitoring Reader\"\n  principal_id         = azurerm_dashboard_grafana.grafana.identity[0].principal_id\n  depends_on = [\n    azurerm_resource_group.monitoring,\n    azurerm_monitor_workspace.amw,\n    azurerm_dashboard_grafana.grafana\n  ]\n}    \n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-6-create-role-assignment-over-resource-group-containing-the-azure-monitor-workspace","title":"Step 6: Create role assignment over resource group containing the Azure Monitor Workspace","text":"<p>Similarly, create role assignments for Azure Monitor.</p> <pre><code># Step-5: Add required role assignment over resource group containing the Azure Log Analytics Workspace for AKS logs access\nresource \"azurerm_role_assignment\" \"grafana_law\" {\n  scope                = data.azurerm_resource_group.workspace.id\n  role_definition_name = \"Monitoring Reader\"\n  principal_id         = azurerm_dashboard_grafana.grafana.identity[0].principal_id\n  depends_on = [\n    azurerm_resource_group.monitoring,\n    data.azurerm_resource_group.workspace\n  ]\n}\n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-7-create-role-assignment-over-resource-group-containing-the-aks","title":"Step 7: Create role assignment over resource group containing the AKS","text":"<p>Grant the necessary permissions to AKS.</p> <pre><code># Step-6: Add required role assignment over ewm30 resource group access for AKS\nresource \"azurerm_role_assignment\" \"monitoring_reader_ewm30_rg\" {\n  scope                = data.azurerm_resource_group.ewm30_rg.id\n  role_definition_name = \"Monitoring Reader\"\n  principal_id         = azurerm_dashboard_grafana.grafana.identity[0].principal_id\n  depends_on = [\n    azurerm_resource_group.monitoring,\n    data.azurerm_resource_group.workspace\n  ]\n}\n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-8-create-azure-ad-groups-for-grafana-access","title":"Step 8: Create azure AD groups for Grafana Access","text":"<p>Define Azure AD groups to assign users to Grafana.</p> <pre><code># Step-7: Create azure ad groups for Grafana access\nresource \"azuread_group\" \"grafana_admin\" {\n  for_each         = toset([\"admin\", \"editor\", \"viewer\"])\n  display_name     = lower(\"${local.ad_group_prefix}-grafana-${each.key}-ewm30-${local.environment}\")\n  owners           = [data.azurerm_client_config.current.object_id]\n  security_enabled = true\n\n  lifecycle {\n    ignore_changes = [owners]\n  }\n}    \n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#step-9-setup-rbac-for-grafana-access","title":"Step 9: Setup RBAC for Grafana Access","text":"<p>Configure RBAC to grant users specific access levels to Grafana dashboards.</p> <pre><code># Step-8: RBAC setup for Grafana instance access\nresource \"azurerm_role_assignment\" \"grafana_rbac\" {\n  for_each             = var.grafana_rbac\n  principal_id         = each.value\n  role_definition_name = each.key\n  scope                = azurerm_dashboard_grafana.grafana.id\n  depends_on = [\n    azurerm_dashboard_grafana.grafana\n  ]\n}   \n</code></pre>"},{"location":"kubernetes/monitoring/grafana-workspace/#reference","title":"Reference","text":"<ul> <li>terraform Azure Provider Documentation</li> <li>Microsoft Azure Managed Grafana workspace</li> </ul>"},{"location":"kubernetes/monitoring/introduction/","title":"Monitor AKS with Azure Managed Services for Prometheus &amp; Grafana","text":""},{"location":"kubernetes/monitoring/introduction/#introduction","title":"Introduction","text":"<p>Monitoring is essential for effectively managing Kubernetes clusters and ensuring the smooth operation of applications within them. By implementing robust monitoring, organizations can:</p> <ul> <li>Ensure Performance: Track key metrics such as CPU, memory, and storage to prevent bottlenecks, optimize workloads, and allocate resources efficiently.  </li> <li>Enhance Reliability: Detect issues like node failures, pod crashes, or application errors early to minimize downtime and ensure a seamless user experience.  </li> <li>Enable Troubleshooting: Leverage real-time logs and metrics to diagnose and resolve issues swiftly.  </li> </ul> <p>Azure Monitor simplifies Kubernetes monitoring with a comprehensive suite of tools for Kubernetes environments:</p> <ul> <li>Managed Prometheus: A fully managed service for collecting metrics, enabling developers to focus on insights rather than infrastructure.  </li> <li>Container Insights: Provides detailed telemetry data, including performance metrics and logs, offering visibility into workloads on Azure Kubernetes Service (AKS).  </li> <li>Managed Grafana: Offers customizable and visually appealing dashboards, empowering teams to make data-driven decisions.  </li> </ul>"},{"location":"kubernetes/monitoring/introduction/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level architecture of monitor AKS with azure managed services for Prometheus &amp; Grafana</p> <p></p> <p>Before enabling monitoring in AKS, understanding the key open-source tools, Prometheus and Grafana, is crucial as they form the backbone of many Kubernetes monitoring solutions.</p>"},{"location":"kubernetes/monitoring/introduction/#prometheus","title":"Prometheus","text":"<p>Prometheus is a widely adopted open-source monitoring and alerting system, well-suited for cloud-native environments like Kubernetes. Initially developed by SoundCloud, it is now part of the Cloud Native Computing Foundation (CNCF).</p> <p>Key Features:</p> <ul> <li>Time-Series Database: Stores metrics data as time-series for easy trend analysis.  </li> <li>Pull-Based Data Collection: Scrapes metrics from endpoints, reducing reliance on external pushes.  </li> <li>Kubernetes Integration: Natively integrates with Kubernetes, automatically discovering services and pods.  </li> <li>PromQL: A powerful query language for analyzing collected metrics.  </li> <li>Alerting: Built-in rules trigger notifications based on specific conditions.  </li> </ul>"},{"location":"kubernetes/monitoring/introduction/#grafana","title":"Grafana","text":"<p>Grafana is a leading open-source visualization and analytics platform that integrates seamlessly with a wide range of data sources, including Prometheus.</p> <p>Key Features:</p> <ul> <li>Customizable Dashboards: Create personalized dashboards with various visualization options.  </li> <li>Multiple Data Sources: Supports integration with Prometheus, Elasticsearch, Azure Monitor, and more.  </li> <li>Alerting: Provides robust alerting mechanisms that integrate with tools like Slack, PagerDuty, or email.  </li> </ul>"},{"location":"kubernetes/monitoring/introduction/#azure-managed-services-for-monitoring","title":"Azure Managed Services for Monitoring","text":"<p>Azure enhances Kubernetes monitoring by providing managed services that extend popular open-source tools like Prometheus and Grafana.</p>"},{"location":"kubernetes/monitoring/introduction/#azure-managed-prometheus","title":"Azure Managed Prometheus","text":"<p>Azure offers Prometheus as a fully managed service through Azure Monitor, removing the need for manual infrastructure management.</p> <p>Key Features:</p> <ul> <li>Fully Managed: No setup, scaling, or maintenance required.  </li> <li>Native AKS Integration: Simplifies metrics collection with automatic Kubernetes object discovery.  </li> <li>Preconfigured Features: Includes built-in alerts, rules, and dashboards.  </li> <li>Centralized Metrics Storage: Stores Prometheus metrics in an Azure Monitor workspace.  </li> <li>Seamless Integration: Works with Azure services, including AKS and Azure Managed Grafana.  </li> <li>Comprehensive Analysis: Query metrics using PromQL or visualize them in Metrics Explorer and Grafana.  </li> </ul> <p>For more detailed information about Prometheus and its capabilities, refer to the official Azure documentation.</p>"},{"location":"kubernetes/monitoring/introduction/#azure-monitor-workspace","title":"Azure Monitor Workspace","text":"<p>Azure Monitor workspaces store metric data collected by Azure Monitor, focusing primarily on Prometheus metrics.</p> <p>Difference Between Azure Monitor Workspace and Log Analytics Workspace:</p> Feature Azure Monitor Workspace Log Analytics Workspace Data Type Prometheus metrics Logs, traces, and some metrics data Use Case Metrics-based monitoring Centralized log and trace analysis Query Language PromQL KQL (Kusto Query Language) Visualization Tools Metrics Explorer, Managed Grafana Log Analytics, Workbooks, and third-party tools <p>Note: Azure Monitor workspaces currently focus on Prometheus metrics but aim to unify storage for all Azure Monitor-collected metrics.</p> <p>To explore more about Azure Monitor workspace capabilities, visit the official documentation.</p>"},{"location":"kubernetes/monitoring/introduction/#azure-managed-grafana-workspace","title":"Azure Managed Grafana Workspace","text":"<p>Azure Managed Grafana Workspace offers a fully managed Grafana service, allowing teams to visualize and analyze monitoring data without managing infrastructure.</p> <p>Key Features:</p> <ul> <li>Fully Managed Service: Handles setup, updates, scaling, and maintenance.  </li> <li>Integrated with AKS: Connects seamlessly to AKS and Azure Managed Prometheus.  </li> <li>Pre-Built Dashboards: Provides curated dashboards for common use cases.  </li> <li>Azure AD Integration: Secure access with role-based permissions via Azure Active Directory (Azure AD).  </li> <li>Custom Visualization: Supports a wide range of visualization plugins and custom dashboards.  </li> </ul> <p>To explore more about Azure managed workspace capabilities, visit the official documentation.</p> <p>Workspaces and Features</p> <p>Different Azure services require specific workspaces for monitoring data storage and management.</p> Feature Workspace Description Managed Prometheus Azure Monitor Workspace Stores Prometheus metrics from AKS and other Azure services. Container Insights Log Analytics Workspace Stores logs and performance data from AKS containers. Managed Grafana Azure Managed Grafana Workspace Hosts the managed Grafana service for data visualization. <p>Explanation of Workspaces</p> <ul> <li>Azure Monitor Workspace: Focuses on real-time metrics storage, particularly Prometheus metrics.  </li> <li>Log Analytics Workspace: A centralized repository for log data, used for deep insights and dashboards.  </li> <li>Azure Managed Grafana Workspace: Dedicated to visualizing data with Grafana\u2019s customizable dashboards.  </li> </ul>"},{"location":"kubernetes/monitoring/monitor-workspace/","title":"Create an Azure Monitoring Workspace Using Terraform","text":""},{"location":"kubernetes/monitoring/monitor-workspace/#introduction","title":"Introduction","text":"<p>Azure Monitor workspaces store metric data collected by Azure Monitor, focusing primarily on Prometheus metrics.</p> <p>This article explores the detailed steps required to set up an Azure Monitoring Workspace using terraform.</p>"},{"location":"kubernetes/monitoring/monitor-workspace/#technical-scenario","title":"Technical Scenario","text":"<p>Consider an enterprise managing several AKS clusters with the need for a unified monitoring solution. One of the prerequisites for enabling monitoring for Kubernetes clusters is having an Azure Monitoring workspace ready. This allows us to set up Prometheus and Grafana and integrate them with AKS.</p>"},{"location":"kubernetes/monitoring/monitor-workspace/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, ensure the following prerequisites are met:</p> <ul> <li>An active Azure subscription with sufficient permissions (Contributor or Owner role).</li> <li>Terraform installed and configured locally.</li> <li>Familiarity with terraform syntax and Azure resources.</li> <li>Azure CLI installed for authentication purposes.</li> <li>Predefined Azure AD groups for managing role assignments.</li> <li>Basic knowledge of Azure Monitor and its resources.</li> </ul>"},{"location":"kubernetes/monitoring/monitor-workspace/#implementation-details","title":"Implementation Details","text":"<p>The following steps detail the implementation process:</p>"},{"location":"kubernetes/monitoring/monitor-workspace/#task-1-configure-terraform-variables-for-azure-monitoring-workspace","title":"Task-1: Configure terraform variables for azure monitoring workspace","text":"<p>Start by defining the necessary variables in the <code>variables.tf</code> file to make your terraform configuration modular and reusable:</p> <pre><code>variable \"rg_prefix\" {\n  type        = string\n  default     = \"rg\"\n  description = \"Prefix of the resource group name that's combined with name of the resource group.\"\n}\nvariable \"monitoring_rg_name\" {\n  description = \"(Required) Specifies the name of the Resource Group where the Azure Monitor Workspace should exist. \"\n  type        = string\n  default     = \"monitoring\"\n}\nvariable \"monitoring_rg_name\" {\n  description = \"(Required) Specifies the name of the Resource Group where the Azure Monitor Workspace should exist. \"\n  type        = string\n  default     = \"monitoring\"\n}\nvariable \"monitoring_rg_location\" {\n  description = \"(Required) Specifies the Azure Region where the Azure Monitor Workspace should exist.\"\n  type        = string\n  default     = \"eastus\"\n}\nvariable \"monitoring_tags\" {\n  description = \"(Optional) A mapping of tags which should be assigned to the Azure Monitor Workspace.\"\n  type        = map(any)\n  default     = {\n    \"Application\" = \"Monitoring\"\n  }\n}\nvariable \"azure_monitor_workspace_prefix\" {\n  type        = string\n  default     = \"amw\"\n  description = \"Prefix of the Azure Monitor Workspace name that's combined with name of the Azure Monitor Workspace.\"\n}\nvariable \"azure_monitor_workspace_name\" {\n  description = \"(Required) Specifies the name which should be used for this Azure Monitor Workspace.\"\n  type        = string\n  default     = \"workspace\"\n}\n</code></pre>"},{"location":"kubernetes/monitoring/monitor-workspace/#task-2-create-new-resource-group-for-azure-monitoring-workspace","title":"Task-2: Create new resource group for azure monitoring workspace","text":"<p>Create a new resource group to host the Azure Monitoring Workspace. Resource groups provide logical groupings for Azure resources, making management easier:</p> <pre><code># Create the resource group for monitoring\nresource \"azurerm_resource_group\" \"monitoring\" {\n  name     = lower(\"${var.rg_prefix}-${var.monitoring_rg_name}-${local.environment}\")\n  location = var.monitoring_rg_location\n  tags     = merge(local.default_tags, var.monitoring_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags,\n    ]\n  }\n}\n</code></pre>"},{"location":"kubernetes/monitoring/monitor-workspace/#task-3-create-an-azure-monitoring-workspace-using-terraform","title":"Task-3: Create an azure monitoring workspace using terraform","text":"<p>Provision an Azure Monitor workspace, which is the backbone of Azure's monitoring services:</p> <pre><code># Create the Azure Monitor Workspace.\nresource \"azurerm_monitor_workspace\" \"amw\" {\n  name                          = lower(\"${var.azure_monitor_workspace_prefix}-${var.azure_monitor_workspace_name}-${local.environment}\")\n  resource_group_name           = azurerm_resource_group.monitoring.name\n  location                      = azurerm_resource_group.monitoring.location\n  public_network_access_enabled = var.monitor_workspace_public_access\n  tags                          = merge(local.default_tags, var.monitoring_tags)\n  lifecycle {\n    ignore_changes = [\n      # tags,\n    ]\n  }\n  depends_on = [\n    azurerm_resource_group.monitoring\n  ]\n}\n</code></pre>"},{"location":"kubernetes/monitoring/monitor-workspace/#task-4-lock-the-resource-group-to-prevent-accidental-deletions","title":"Task-4: Lock the resource group to prevent accidental deletions","text":"<p>Add a resource lock to the resource group to prevent accidental deletions. This lock can be configured using Terraform:</p> <pre><code># Lock the resource group -rg-monitoring-&lt;env&gt;\nresource \"azurerm_management_lock\" \"monitoring\" {\n  name       = \"CanNotDelete\"\n  scope      = azurerm_resource_group.monitoring.id\n  lock_level = \"CanNotDelete\"\n  notes      = \"This resource group can not be deleted - lock set by Terraform\"\n  depends_on = [\n    azurerm_resource_group.monitoring,\n    azurerm_monitor_workspace.amw,    \n  ]\n}\n</code></pre>"},{"location":"kubernetes/monitoring/monitor-workspace/#reference","title":"Reference","text":"<ul> <li>terraform Azure Provider Documentation</li> <li>Microsoft Azure Monitoring Workspace</li> <li>Azure Monitor Workspace</li> </ul>"},{"location":"microservices/0-docker-getting-started/","title":"Getting Started with Docker","text":""},{"location":"microservices/0-docker-getting-started/#overview","title":"Overview","text":"<p>Docker is a platform for developing, shipping, and running applications in containers. Containers allow you to package an application and its dependencies into a single unit, making it easy to deploy consistently across different environments.  </p> <p>In this lab, I will guide you through the process of creating Docker images, containers, and finally accessing the sample application in the web browser.</p> <p>If you are new to Docker and want to learn its fundamental concepts, please visit our website. - Exploring Docker Fundamentals </p>"},{"location":"microservices/0-docker-getting-started/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ol> <li>Step 1: Get the Sample Application</li> <li>Step 2: Create Docker Image</li> <li>Step 3: Create Docker Container</li> <li>Step 4: Port Binding</li> <li>Step 5: Browse the Frontend Application</li> <li>Step 6: View Docker Logs</li> <li>Step 7: Docker Commands</li> </ol>"},{"location":"microservices/0-docker-getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Visual Studio Code :  - Visual Studio Code Downloads.</li> <li>Docker desktop :  - Docker Downloads.</li> <li>Git Client tool:  - Git Downloads.</li> </ul> <p>Verify the docker installation by running following commands:</p> <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre>"},{"location":"microservices/0-docker-getting-started/#step-1-get-the-sample-application","title":"Step 1: Get the Sample Application","text":"<p>To begin, you'll need a sample application to work with. You can either use an existing application or create a simple one. </p> <p>In this task, we'll start by searching for an image to run locally. For example, we'll use the <code>Nginx</code> image from Docker Hub using the following URL: Docker Hub Search</p>"},{"location":"microservices/0-docker-getting-started/#step-2-create-docker-image","title":"Step 2: Create Docker Image","text":"<p>Now that we've identified the image we want to use, let's pull it from Docker Hub into our local Docker Desktop and run it locally.</p> <p><pre><code>docker pull nginx\n\n# output\nUsing default tag: latest\nlatest: Pulling from library/nginx\na5573528b1f0: Pull complete \n8897d65c8417: Pull complete \nfbc138d1d206: Pull complete \n06f386eb9182: Pull complete \naeb2f3db77c3: Pull complete \n64fb762834ec: Pull complete \ne5a7e61f6ff4: Pull complete \nDigest: sha256:4c0fdaa8b6341bfdeca5f18f7837462c80cff90527ee35ef185571e1c327beac\nStatus: Downloaded newer image for nginx:latest\ndocker.io/library/nginx:latest\n</code></pre> </p> <p>List Docker images from Docker Desktop:</p> <pre><code>docker images\n\n# output\nREPOSITORY   TAG       IMAGE ID       CREATED        SIZE\nnginx        latest    6c7be49d2a11   2 months ago   192MB\n</code></pre>"},{"location":"microservices/0-docker-getting-started/#step-3-create-docker-container","title":"Step 3: Create Docker Container","text":"<p>In this step, we'll create a Docker container by running the <code>docker run</code> command for the image.</p> <pre><code>docker run nginx\n\n# output\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n2024/01/15 04:27:56 [notice] 1#1: using the \"epoll\" event method\n2024/01/15 04:27:56 [notice] 1#1: nginx/1.25.3\n2024/01/15 04:27:56 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) \n2024/01/15 04:27:56 [notice] 1#1: OS: Linux 6.3.13-linuxkit\n2024/01/15 04:27:56 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\n2024/01/15 04:27:56 [notice] 1#1: start worker processes\n2024/01/15 04:27:56 [notice] 1#1: start worker process 29\n2024/01/15 04:27:56 [notice] 1#1: start worker process 30\n2024/01/15 04:27:56 [notice] 1#1: start worker process 31\n2024/01/15 04:27:56 [notice] 1#1: start worker process 32\n2024/01/15 04:27:56 [notice] 1#1: start worker process 33\n</code></pre> <p></p> <p>Open a new terminal and run the following command to list containers:</p> <pre><code>dockder ps\n\n# output\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS     NAMES\n8d23e3ceb3da   nginx     \"/docker-entrypoint.\u2026\"   3 minutes ago   Up 2 minutes   80/tcp    lucid_edison\n</code></pre> <p>You can watch the <code>container logs</code> in the first terminal.</p> <p>To exit the container, press <code>Ctrl + C</code>.</p> <pre><code>2024/01/15 04:28:00 [notice] 1#1: signal 28 (SIGWINCH) received\n2024/01/15 04:28:00 [notice] 1#1: signal 28 (SIGWINCH) received\n2024/01/15 04:30:50 [notice] 1#1: signal 28 (SIGWINCH) received\n2024/01/15 04:30:50 [notice] 1#1: signal 28 (SIGWINCH) received\n</code></pre> <pre><code>docker run -d nginx \n\n# output \n6f5dbcae83bd3ac6a0ea8bdb45f753bf72a723179503d4b4ebce4ddeae2378e2\n\n# Now, you can run the following command to see the list of running containers:\ndocker ps\n</code></pre> <p>Alternatively, you can also run the image directly from Docker Hub. Here are the example commands:</p> <p><pre><code>docker run nginx:1.25.3-alpine\n\n# output\nUnable to find image 'nginx:1.25.3-alpine' locally\n1.25.3-alpine: Pulling from library/nginx\n2c03dbb20264: Pull complete \n0ed066aadd11: Pull complete \n4eeb1ddd7404: Pull complete \n9ba8827f116b: Pull complete \n2bc60ecca38f: Pull complete \n11d942ec6258: Pull complete \nfed1b403bb45: Pull complete \n392e92e0a8e8: Pull complete \nDigest: sha256:a59278fd22a9d411121e190b8cec8aa57b306aa3332459197777583beb728f59\nStatus: Downloaded newer image for nginx:1.25.3-alpine\n/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration\n/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh\n10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf\n10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf\n/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh\n/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh\n/docker-entrypoint.sh: Configuration complete; ready for start up\n2024/01/15 04:39:23 [notice] 1#1: using the \"epoll\" event method\n2024/01/15 04:39:23 [notice] 1#1: nginx/1.25.3\n2024/01/15 04:39:23 [notice] 1#1: built by gcc 12.2.1 20220924 (Alpine 12.2.1_git20220924-r10) \n2024/01/15 04:39:23 [notice] 1#1: OS: Linux 6.3.13-linuxkit\n2024/01/15 04:39:23 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576\n2024/01/15 04:39:23 [notice] 1#1: start worker processes\n2024/01/15 04:39:23 [notice] 1#1: start worker process 29\n2024/01/15 04:39:23 [notice] 1#1: start worker process 30\n2024/01/15 04:39:23 [notice] 1#1: start worker process 31\n2024/01/15 04:39:23 [notice] 1#1: start worker process 32\n2024/01/15 04:39:23 [notice] 1#1: start worker process 33\n</code></pre> Now, you'll see two containers running, one from local Docker Desktop and the second one from the remote Docker Hub registry.</p> <pre><code>docker ps\n\n# output\nCONTAINER ID   IMAGE                 COMMAND                  CREATED              STATUS              PORTS     NAMES\n4df5f0ae77d9   nginx:1.25.3-alpine   \"/docker-entrypoint.\u2026\"   About a minute ago   Up About a minute   80/tcp    nostalgic_lamarr\n6f5dbcae83bd   nginx                 \"/docker-entrypoint.\u2026\"   6 minutes ago        Up 6 minutes        80/tcp    modest_hermann\n</code></pre> <p>docker images from docker desktop</p> <p></p> <p>docker containers from docker desktop</p> <p></p> <p>you can also run following commnds to see images and containers running locally.</p> <pre><code>docker image ls\n\n# output\nREPOSITORY   TAG             IMAGE ID       CREATED        SIZE\nnginx        latest          6c7be49d2a11   2 months ago   192MB\nnginx        1.25.3-alpine   74077e780ec7   2 months ago   43.5MB\n</code></pre> <pre><code>docker container ls\n\n# output\nCONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS          PORTS     NAMES\n4df5f0ae77d9   nginx:1.25.3-alpine   \"/docker-entrypoint.\u2026\"   5 minutes ago    Up 5 minutes    80/tcp    nostalgic_lamarr\n6f5dbcae83bd   nginx                 \"/docker-entrypoint.\u2026\"   10 minutes ago   Up 10 minutes   80/tcp    modest_hermann\n</code></pre>"},{"location":"microservices/0-docker-getting-started/#step-4-port-binding","title":"Step 4: Port Binding","text":"<p>Your application is now running inside the Docker container, and you've mapped port 8080 from the container to your host. This means you can access your application using <code>http://localhost:8080</code> in your web browser.</p> <pre><code># list the containers\ndocker ps\n\n# then stop the container \ndocker stop 6f5dbcae83bd\n</code></pre> <pre><code>docker run -d -p 8080:80 nginx:1.25.3-alpine\n\n# output\nf21ada11af57b799c9b834d0a6c8e6e1628c6289d64cf65fdc0968cbe94500fd\n</code></pre>"},{"location":"microservices/0-docker-getting-started/#step-5-browse-the-frontend-application","title":"Step 5: Browse the Frontend Application","text":"<p>Open your web browser and navigate to <code>http://localhost:8080</code> to access your Node.js application running in the Docker container.</p> <p></p>"},{"location":"microservices/0-docker-getting-started/#step-6-view-docker-logs","title":"Step 6: View Docker Logs","text":"<p>To view the logs of your running container, use the following command:</p> <p>This will display the logs generated by your application.</p> <p><pre><code>docker logs f21ada11af57\n</code></pre> </p> <pre><code>docker ps -a \n\n# output\nCONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS                      PORTS                  NAMES\nf21ada11af57   nginx:1.25.3-alpine   \"/docker-entrypoint.\u2026\"   7 minutes ago    Up 7 minutes                0.0.0.0:8080-&gt;80/tcp   nifty_goldberg\n4df5f0ae77d9   nginx:1.25.3-alpine   \"/docker-entrypoint.\u2026\"   19 minutes ago   Up 19 minutes               80/tcp                 nostalgic_lamarr\n6f5dbcae83bd   nginx                 \"/docker-entrypoint.\u2026\"   25 minutes ago   Exited (0) 10 minutes ago                          modest_hermann\n8d23e3ceb3da   nginx                 \"/docker-entrypoint.\u2026\"   31 minutes ago   Exited (0) 25 minutes ago                          lucid_edison\n</code></pre> <p>Naming the Container:</p> <p>You can also name the container using the <code>--name</code> flag:</p> <pre><code>docker run --name nginx-app -d -p 8080:80 nginx:1.25.3-alpine\n\n# output\n58e464680a8da16b717171732fb1b67b678b1c8efb115f9adad8d3257c6cc875\n\n# run following command to see the name\ndocker ps\ndocker logs nginx-app\n</code></pre>"},{"location":"microservices/0-docker-getting-started/#step-7-docker-commands","title":"Step 7: Docker Commands","text":"<p>For more comprehensive details on Docker commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"microservices/0-docker-getting-started/#conclusion","title":"Conclusion","text":"<p>In summary, this guide introduced you to Docker, a tool that simplifies how we build, package, and run applications. We've covered essential steps, like getting an example application (nginx), creating Docker images and running containers. We've also learned how to manage ports, access apps in web browsers, and check what's happening behind the scenes with Docker logs. Plus, we touched on some common Docker commands.</p> <p>Docker is a powerful tool for containerization, enabling you to package and deploy applications with ease. By following these steps, you've created your first Docker application. </p>"},{"location":"microservices/0-docker-getting-started/#references","title":"References","text":"<ul> <li>Getting started guide</li> <li>Docker images</li> <li>Docker Documentation</li> <li>Docker Hub</li> <li>Nginx Docker Official Images</li> </ul>"},{"location":"microservices/0.gettingstarted/","title":"Getting Started with Microservices","text":""},{"location":"microservices/0.gettingstarted/#overview","title":"Overview","text":"<p>Welcome to the first chapter of our book. In this chapter, we will begin our journey by understanding microservices architectures and how they are different comparing with traditional monolithic architectures. We'll also learn the advantages of the microservices architectures, including scalability, flexibility, and easier maintenance. we will also learn challenges with microservices and considerations that need to be carefully addressed, key technologies and communication patterns. Finally we will perform four tasks such as identifying list of microservices, git repos needed, create org and project in azure devops to continue our journey in this book.</p>"},{"location":"microservices/0.gettingstarted/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>What are Microservices?</li> <li>Microservices vs Monolithic Architectures</li> <li>Advantages of Microservices</li> <li>Challenges and Considerations</li> <li>Key Technologies and Tools</li> <li>Microservices Communication</li> <li>Domain-Driven Design (DDD)</li> <li>Task-1: Identify Microservices for the book</li> <li>Task-2: Identify the List of Git Repositories Needed</li> <li>Task-3: Create new Azure DevOps Organization</li> <li>Task-4: Create new Azure DevOps Project</li> </ul>"},{"location":"microservices/0.gettingstarted/#what-are-microservices","title":"What are Microservices?","text":"<p><code>Microservices</code> are architectural style that structures an application as a collection of small, independent, and loosely coupled services. These services, known as microservices  , are designed to be self-contained and focused on specific functions or features of the application. Unlike monolithic applications, where all components are tightly integrated into a single codebase, microservices allow for the decomposition of an application into smaller, manageable parts.</p>"},{"location":"microservices/0.gettingstarted/#microservices-vs-monolithic-architectures","title":"Microservices vs Monolithic Architectures","text":"<p>Monolithic Architectures:</p> <ul> <li>In a monolithic architecture, the entire application is built as a single, unified codebase.</li> <li>All components of the application, including user interfaces, business logic, and data access layers, are tightly coupled.</li> <li>Scaling a monolithic application typically involves replicating the entire application, even if only specific parts require additional resources.</li> <li>Maintenance and updates often require making changes to the entire codebase, making it challenging to isolate and fix issues.</li> </ul> <p>Microservices:</p> <ul> <li>Microservices architecture promotes breaking down the application into smaller, independent services.</li> <li>Each microservice is responsible for a specific application's functionality.</li> <li>Microservices can be developed, deployed, and scaled independently.</li> <li>Updates and maintenance are easier to manage, as changes to one microservice do not impact the entire system.</li> </ul>"},{"location":"microservices/0.gettingstarted/#advantages-of-microservices","title":"Advantages of Microservices","text":"<p>Microservices architecture offers several advantages, including:</p> <ul> <li> <p>Scalability: Microservices can be easily scaled horizontally to handle increased traffic, ensuring that the system remains responsive during high-demand periods.</p> </li> <li> <p>Flexibility: Developers can work on individual microservices without affecting the entire application. This makes it easier to introduce new features, fix bugs, or update a specific service without disrupting the entire system.</p> </li> <li> <p>Easy Maintenance: Smaller, self-contained services are typically easier to maintain and manage. Updates and changes can be isolated to specific microservices, reducing the risk of unintended consequences.</p> </li> <li> <p>Improved Fault Isolation: When a microservice fails, it usually doesn't bring down the entire system. Failures are contained within the affected service, minimizing the impact on the overall application.</p> </li> <li> <p>Technology Agnosticism: Microservices allow you to use different technologies and programming languages for different services, which can be chosen based on the specific requirements of each service.</p> </li> <li> <p>Rapid Development: Smaller teams can work independently on microservices, enabling faster development cycles and quicker time-to-market for new features or products.</p> </li> <li> <p>Enhanced Testing: Isolated microservices can be tested more thoroughly, leading to better quality assurance and reduced testing complexity compared to monolithic applications.</p> </li> <li> <p>Easier Deployment: Smaller, independent services are easier to deploy, reducing the risk of deployment failures and making it possible to implement continuous integration and continuous delivery (CI/CD) practices.</p> </li> </ul>"},{"location":"microservices/0.gettingstarted/#challenges-and-considerations","title":"Challenges and Considerations","text":"<p>While microservices offer numerous advantages, they also come with their set of challenges and considerations that need to be carefully addressed. Careful planning and architectural decisions are important for realizing the benefits of microservices while mitigating their challenges.</p> <p>Challenges of microservices</p> <ul> <li> <p>Complexity: Microservices introduce complexity, as an application is divided into multiple services, each with its own codebase, data store, and dependencies. Managing the interactions between microservices and ensuring the overall system's integrity can be challenging.</p> </li> <li> <p>Data Consistency: Maintaining data consistency in a distributed microservices architecture can be complex. With each microservice managing its data, ensuring data synchronization and integrity across services is important.</p> </li> <li> <p>Distributed Systems Issues: Microservices are inherently distributed, which introduces challenges such as network latency, message serialization, and handling communication failures. Implementing robust error handling and resilience mechanisms becomes essential.</p> </li> <li> <p>Operational Complexity: Managing and monitoring a large number of microservices in a production environment can be operationally complex.  Tools and practices for deployment, monitoring, and scaling need to be in place to ensure smooth operations.</p> </li> </ul> <p>Considerations for Microservices Adoption</p> <ul> <li> <p>Application Complexity: Microservices are well-suited for complex, large-scale applications with multiple modules or functionalities. For simpler applications, a monolithic architecture may be more appropriate.</p> </li> <li> <p>Team Structure: Consider your organization's team structure. Microservices often align with small, cross-functional teams that can own and manage individual microservices. Ensure your teams have the necessary skills for microservices development and operations.</p> </li> <li> <p>Scalability and Performance: Microservices can provide scalability benefits, particularly for applications with varying workloads. Evaluate whether your application requires the ability to scale individual components independently.</p> </li> <li> <p>Frequent Updates: If your application requires frequent updates and releases, microservices can support continuous integration and deployment practices. Ensure you have the necessary CI/CD pipelines and infrastructure.</p> </li> </ul> <p>Choosing the Right Architecture</p> <p>The choice between monolithic and microservices architecture depends on various factors, including the complexity of the application, team structure, scalability requirements, and development speed. Monolithic architectures excel in simplicity and are suitable for smaller applications with straightforward requirements. Microservices, on the other hand, offer flexibility and scalability for larger, more complex applications but introduce operational complexities.</p>"},{"location":"microservices/0.gettingstarted/#key-technologies-and-tools","title":"Key Technologies and Tools","text":"<p>Microservices development relies on a set of essential technologies and tools that facilitate the creation, deployment, and management of individual microservices. </p> <ul> <li> <p>Docker: Docker is a containerization platform that allows developers to package applications and their dependencies into lightweight containers.  Docker containers provide consistency in deployment across different environments, ensuring that microservices run reliably on any system.</p> </li> <li> <p>DevContainers: DevContainers streamline the development and testing of microservices locally by providing a controlled, isolated, and consistent environment that enhances collaboration among team members and simplifies the management of complex microservices ecosystems.</p> </li> <li> <p>Kubernetes: Kubernetes is a container orchestration platform that automates the deployment, scaling, and management of containerized applications, including microservices. Kubernetes simplifies the management of microservices at scale, enabling features like load balancing, auto-scaling, and rolling updates.</p> </li> </ul> <ul> <li> <p>API Gateways: API gateways act as a front-end for microservices, providing a unified entry point for clients and handling tasks such as authentication, rate limiting, and request routing. API gateways simplify client interactions with microservices, centralize security controls, and enable API versioning and documentation.</p> </li> <li> <p>Continuous Integration/Continuous Deployment (CI/CD) Tools: CI/CD tools such as Azure DevOps, Argocd, Helmcharts automate the building, testing, and deployment of microservices, supporting rapid development and delivery. CI/CD pipelines streamline the development process, allowing for frequent updates and reducing the risk of errors.</p> </li> <li> <p>Monitoring and Observability Tools (e.g., Prometheus, Grafana, Jaeger): Monitoring and observability tools provide insights into the performance, availability, and behavior of microservices, helping to detect and troubleshoot issues. These tools ensure the reliability of microservices in production by offering real-time monitoring, logging, and tracing capabilities.</p> </li> </ul>"},{"location":"microservices/0.gettingstarted/#microservices-communication","title":"Microservices Communication","text":"<p>Microservices can communicate with each other using different communication patterns, both synchronous and asynchronous.</p> <p>Synchronous:</p> <ul> <li> <p>HTTP/HTTPS: Microservices can communicate over standard HTTP/HTTPS protocols, making it easy to create RESTful APIs or web services.    Synchronous communication is suitable for scenarios where immediate responses are required.</p> </li> <li> <p>gRPC: gRPC is a high-performance, language-agnostic remote procedure call (RPC) framework that allows microservices to communicate efficiently. It is ideal for scenarios where low-latency, binary-encoded communication is needed.</p> </li> </ul> <p>Asynchronous:</p> <ul> <li> <p>Message Queues (e.g., RabbitMQ, Apache Kafka): Microservices can exchange messages through message queues or publish-subscribe systems. Asynchronous communication is useful for decoupling services and handling background tasks or event-driven scenarios.</p> </li> <li> <p>Event Sourcing and Event-driven Architecture: In event-driven architecture, microservices issue and consume events to communicate changes or trigger actions. This pattern is beneficial for building scalable, loosely coupled systems that respond to real-time events.</p> </li> </ul>"},{"location":"microservices/0.gettingstarted/#domain-driven-design-ddd","title":"Domain-Driven Design (DDD)","text":"<p>Domain-Driven Design (DDD) is a set of principles, patterns, and techniques for designing applicatio with a focus on the domain of the problem being solved. In the context of microservices architecture, DDD plays a importantent role in helping you define the boundaries of your microservices and ensure that they align with your business domain. Here's how DDD techniques can be applied in microservices architecture:</p> <ul> <li> <p>Bounded Contexts:In DDD, a bounded context is a specific boundary within which a domain model is defined and applicable. In microservices, each microservice typically corresponds to a bounded context. Bounded contexts ensure that each microservice has a well-defined scope and encapsulates a specific aspect of the business domain.</p> </li> <li> <p>Aggregates: Aggregates in DDD represent a cluster of domain objects treated as a single unit. In microservices, an aggregate can be considered a microservice that manages a set of related entities. Microservices encapsulate aggregates and provide APIs for manipulating them. This helps maintain data consistency and isolation.</p> </li> <li> <p>Entities and Value Objects: DDD distinguishes between entities (objects with a distinct identity) and value objects (objects with no distinct identity). In microservices, entities and value objects are used to model domain concepts within the microservice's scope, helping to define data structures and behavior.</p> </li> <li> <p>Context Mapping: Context mapping in DDD deals with defining relationships and interactions between bounded contexts. It helps manage the integration points between different parts of the system.  In microservices architecture, context mapping is essential for specifying how microservices interact and communicate with each other, either through APIs or messaging.</p> </li> </ul>"},{"location":"microservices/0.gettingstarted/#task-1-identify-microservices-for-the-book","title":"Task-1: Identify Microservices for the book","text":"<p>To fully explore the microservices architecture in this book, we will create several containerized microservices and microfrontend applications and couple of databases. These applications will allow us to demonstrate real-world scenarios and provide a practical understanding of microservices implementation. In this case study, we will create the following microservices, which will be developed in the upcoming labs.  we have purposely selected diverse options to ensure a broader learning experience.</p> Microservice/Website/Database Technology Used Name First Microservice .NET Core Web API (C#) aspnet-api Second Microservice Node.js (Node) nodejs-api First Website ASP.NET Core MVC (C#) aspnet-app Second Website React.js (Node) react-app First Database SQL Server sqlserver-db Second Database PostgreSQL postgresql-db Keycloak Identity and Access Management keycloak-service Drupal Content Management System drupal-service <p>for example, here is how the folder structure of our Microservices and MicroFrontend Applications looks like    .</p> <pre><code>Microservices/\n\u251c\u2500\u2500 aspnet-api/\n\u2502   \u251c\u2500\u2500 Controllers/\n\u2502   \u251c\u2500\u2500 Models/\n\u2502   \u251c\u2500\u2500 appsettings.json\n\u2502   \u251c\u2500\u2500 Program.cs\n\u2502   \u251c\u2500\u2500 Startup.cs\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 aspnet-api.csproj\n\u2514\u2500\u2500 node-api/\n    \u251c\u2500\u2500 routes/\n    \u251c\u2500\u2500 models/\n    \u251c\u2500\u2500 package.json\n    \u251c\u2500\u2500 app.js\n    \u2514\u2500\u2500 Dockerfile\n\nWebsites/\n\u251c\u2500\u2500 aspnet-app/\n\u2502   \u251c\u2500\u2500 Controllers/\n\u2502   \u251c\u2500\u2500 Models/\n\u2502   \u251c\u2500\u2500 Views/\n\u2502   \u251c\u2500\u2500 appsettings.json\n\u2502   \u251c\u2500\u2500 Program.cs\n\u2502   \u251c\u2500\u2500 Startup.cs\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u2514\u2500\u2500 aspnet-app.csproj\n\u251c\u2500\u2500 react-app/\n\u2502   \u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 public/\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 node_modules/\n\u2502   \u2514\u2500\u2500 README.md\n\u2514\u2500\u2500 ...\nDatabases/\n\u251c\u2500\u2500 sqlserver-db/\n\u2502   \u251c\u2500\u2500 tables/\n\u2502   \u251c\u2500\u2500 procedures/\n\u2502   \u251c\u2500\u2500 views/\n\u2502   \u251c\u2500\u2500 functions/\n\u2502   \u2514\u2500\u2500 triggers/\n\u2502   \u2514\u2500\u2500 Dockerfile\n|\n\u2514\u2500\u2500 postgresql-db/\n    \u251c\u2500\u2500 tables/\n    \u251c\u2500\u2500 procedures/\n    \u251c\u2500\u2500 views/\n    \u251c\u2500\u2500 functions/\n    |\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 triggers/\n</code></pre> <p>Important</p> <p>If you noticed, each project has its own <code>Dockerfile</code>, indicating that all these applications will be containerized and ready for deployment to a Kubernetes cluster.</p> <p>The following diagram shows the conceptual view of the microservices environment</p> <p></p> <p>For example:</p> <p></p>"},{"location":"microservices/0.gettingstarted/#task-2-identify-the-list-of-git-repositories-needed","title":"Task-2: Identify the List of Git Repositories Needed","text":"<p>Once you have determined the list of domains or microservices required for your project, it's time to analyze how they will be organized within the source control system, such as Git repositories. One important consideration is determining the number of Git repositories you need.</p> <p>There are multiple ways to organize source code and pipelines in Azure DevOps Git, and the approach you choose depends on how you want to manage your source code and pipelines for your microservices architecture while ensuring ease of maintenance in the future.</p> <p>In my preference, I recommend creating a separate Git repository for each domain or microservice. Within each domain, you may have multiple microservices, MicroFrontends, and databases.</p> <p>For example, let's visualize how the Git structure may look:</p> <ul> <li>Organization1 (Name of your organization)<ul> <li>Project1 (Name of the project)<ul> <li>Repo-1 (for Domain1)<ul> <li>APIs - Create one or more APIs with separate folders</li> <li>Websites - Create one or more websites with separate folders</li> <li>Databases - Create one or more databases with separate folders</li> </ul> </li> <li>Repo-2 (for Domain2)<ul> <li>APIs - Create one or more APIs with separate folders</li> <li>Websites - Create one or more websites with separate folders</li> <li>Databases - Create one or more databases with separate folders</li> </ul> </li> <li>Repo-3 (for Domain3)<ul> <li>APIs - Create one or more APIs with separate folders</li> <li>Websites - Create one or more websites with separate folders</li> <li>Databases - Create one or more databases with separate folders</li> </ul> </li> </ul> </li> <li>Project2 (Project2)<ul> <li>Repo-1 (Name of the repository under Project2)<ul> <li>APIs - Create one or more APIs with separate folders</li> <li>Websites - Create one or more websites with separate folders</li> <li>Databases - Create one or more databases with separate folders</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Repeat this structure as the organization grows and new projects or domains are introduced.</p> <p>By following this approach, each domain or microservice will have its dedicated Git repository, providing a clear separation and organization of the source code and related artifacts. This structure facilitates easier maintenance, collaboration, and version control.</p> <p>Remember, this is just a sample structure, and you can adapt it based on your organization's specific needs and preferences.</p> <p>Visual representation of a sample DevOps Git structure: <pre><code>Organization1\n\u2514\u2500\u2500 Project1\n    \u251c\u2500\u2500 Repo-1 (Microservice-1)\n    \u2502   \u251c\u2500\u2500 APIs\n    \u2502   \u251c\u2500\u2500 Websites\n    \u2502   \u2514\u2500\u2500 Databases\n    \u251c\u2500\u2500 Repo-2 (Microservice-2)\n    \u2502   \u251c\u2500\u2500 APIs\n    \u2502   \u251c\u2500\u2500 Websites\n    \u2502   \u2514\u2500\u2500 Databases\n    \u251c\u2500\u2500 Repo-3 (Microservice-3)\n    \u2502   \u251c\u2500\u2500 APIs\n    \u2502   \u251c\u2500\u2500 Websites\n    \u2502   \u2514\u2500\u2500 Databases\n    Project2\n    \u2514\u2500\u2500 Repo-1\n        \u251c\u2500\u2500 APIs\n        \u251c\u2500\u2500 Websites\n        \u2514\u2500\u2500 Databases\n</code></pre></p> <p>By adopting this Git structure, you can effectively manage and scale your microservices projects while ensuring a clear and organized source control system.</p>"},{"location":"microservices/0.gettingstarted/#task-3-create-new-azure-devops-organization","title":"Task-3: Create new Azure DevOps Organization","text":"<p>With the planning and preparation of your Microservices application complete, the next step is to create a DevOps organization where you can manage the lifecycle of your projects.</p> <p>To create a new Azure DevOps organization, follow these steps:</p> <ol> <li>Sign in to Azure DevOps. - https://dev.azure.com</li> <li>Click on <code>New organization</code> in the left nav. </li> <li>Enter name of the Organization and create new organization. </li> </ol> <p>Once you have completed these steps, you will have a new Azure DevOps organization that is ready for use. You can then invite members to join your organization and start creating new projects.</p>"},{"location":"microservices/0.gettingstarted/#task-4-create-new-azure-devops-project","title":"Task-4: Create new Azure DevOps Project","text":"<p>You need a new project in Azure DevOps to manage your source code and other project related activities.</p> <p>Follow these steps to create a new project in Azure DevOps:</p> <ol> <li> <p>Sign in to the Azure DevOps website https://dev.azure.com/ with your Azure DevOps account.</p> </li> <li> <p>Click on the <code>Create a project</code> button.</p> </li> <li> <p>Enter a name for your project and select a process template. The process template determines the default work item types, source control repository, and other settings for your project.</p> </li> <li> <p>Click the <code>Create project</code> button to create your new project.</p> </li> <li> <p>Follow the screen to configure your project settings, including source control, work item types, and team members.</p> </li> <li> <p>When you are finished, click the <code>Create</code> button to complete the project creation process.</p> </li> </ol> <p>For example: </p> <p>Project Name - <code>Microservices</code></p> <p>Description - <code>Microservices project will be used to roll out sample microservices applications for demonstrating microservices architecture.</code></p> <p></p> <p>We have created new organization in azure DevOps and created new project so that we can start working on containerized microservices applications in the next labs.</p>"},{"location":"microservices/0.gettingstarted/#references","title":"References","text":"<ul> <li>Microsoft MSDN - Microservice architecture style</li> <li>Microsoft MSDN - Create an organization</li> <li>Microsoft MSDN - Create a project in Azure DevOps</li> <li>Microservice Architecture</li> </ul>"},{"location":"microservices/1.aspnet-api/","title":"Create Your First Microservice with .NET Core Web API","text":""},{"location":"microservices/1.aspnet-api/#introduction","title":"Introduction","text":"<p>Welcome to the first lab in our Microservices chapter. In this lab, we will look into creating a simple RESTful service using the ASP.NET Core Web API project template.</p> <p>This lab will demonstrate the process of building a RESTful service and generating a docker container using Dockerfile. By following this example, you will learn the fundamentals of creating RESTful APIs using the ASP.NET Core.</p>"},{"location":"microservices/1.aspnet-api/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Backend (BE)</code> developer, you have been tasked with creating a RESTful service using .NET Core Web API, which is one of the services on our microservices list. This lab will serve as your introduction to the Microservices Architecture, starting with the basics of setting up a repository, creating a small project, and ultimately containerizing the microservice you build. The containerized microservice will then be pushed to the Azure Container Registry (ACR).</p> <p>The primary objective of this lab is to prepare an application for deployment on Kubernetes. The microservices you create in this lab will be utilized in subsequent labs, such as the creation of DevOps pipelines or the deployment to Azure Kubernetes Services (AKS). By completing this lab, you will gain a foundational understanding of how microservices can be developed, containerized, and integrated into a Kubernetes environment.</p>"},{"location":"microservices/1.aspnet-api/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Create a new repo in azure DevOps</li> <li>Step-2: Clone the repository</li> <li>Step-3: Create a new Web API project</li> <li>Step-4: Test Web API project</li> <li>Step-5: Add Dockerfiles to the project</li> <li>Step-6: Build &amp; Test docker container locally</li> <li>Step-7: Publish docker container to ACR</li> </ul>"},{"location":"microservices/1.aspnet-api/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>An Organization in Azure DevOps</li> <li>A Project in Azure DevOps</li> <li>Create Repository permission</li> <li>Git client tool</li> <li>Download and install software for .NET development </li> <li>Docker and the VS Code Docker extension</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"microservices/1.aspnet-api/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create the Restful service using .NET Core.</p> <p></p>"},{"location":"microservices/1.aspnet-api/#step-1-create-a-new-repo-in-azure-devops","title":"Step-1: Create a new repo in azure DevOps","text":"<p>We will create a new repository in Azure DevOps to store our project code and related files.</p> <p>To create a new repository in Azure DevOps, follow these steps:</p> <ol> <li>Login into azure DevOps -  Azure DevOps</li> <li>Select the project where we want to create the repo</li> <li>Click on <code>Repos</code> left nav link</li> <li>From the repo drop-down, select <code>New repository</code></li> <li>In the <code>Create a new repository</code> dialog, verify that Git is the repository type and enter a name for the new repository. </li> <li>You can also add a README and create a <code>.gitignore</code> for the type of code you plan to manage in the repo.</li> <li>I'd prefer to use lower case for all repos (one of the best practice)<ul> <li>Repo name - <code>aspnetapi</code> </li> </ul> </li> </ol> <p>Best-practice</p> <p>When creating repositories in Azure DevOps, it is recommended to use lower case for all repository names. Using lower case consistently throughout your repositories helps maintain consistency, readability, and ease of navigation within your projects.</p> <p>By adhering to this best practice, you ensure that your repository names are uniform and standardized, regardless of the specific domain or microservice they represent. This practice promotes clarity and reduces the chances of confusion or inconsistencies when working with multiple repositories within your organization.</p> <p>For example: </p> <p></p>"},{"location":"microservices/1.aspnet-api/#step-2-clone-the-repo-from-azure-devops","title":"Step-2: Clone the repo from azure DevOps","text":"<p>After creating the repository, we will clone it locally to establish a local working copy of the project.</p> <p>To clone a repository from Azure DevOps, you will need to have the Git client installed on your local machine. follow these steps to clone the source code locally:</p> <ol> <li> <p>Sign in to the Azure DevOps website Azure DevOps Login with your Azure DevOps account.</p> </li> <li> <p>Navigate to the project that contains the repository you want to clone.</p> </li> <li> <p>Click on the <code>Repos</code> tab in the navigation menu.</p> </li> <li> <p>Find the repository you want to clone and click on the <code>Clone</code> button.</p> </li> <li> <p>Copy the URL of the repository.</p> </li> <li> <p>Open a terminal window or command prompt on your local machine, and navigate to the directory where you want to clone the repository.</p> </li> <li> <p>Run the following command to clone the repository:</p> </li> </ol> <pre><code>git clone &lt;repository URL&gt;\n</code></pre> <p>When prompted, enter your Azure DevOps credentials.</p> <p>The repository will be cloned to your local machine, and you can start working with the code.</p> <p>Examples:</p> <pre><code>C:\\Users\\anji.keesari&gt;cd C:\\Source\\Repos\nC:\\Source\\Repos&gt;git clone https://keesari.visualstudio.com/Microservices/_git/aspnetapi\n\nor\n\n# cloning from main branch for the first time\ngit clone git clone https://keesari.visualstudio.com/Microservices/_git/aspnetapi  -b main C:\\Source\\Repos\\Microservices\\aspnetapi\n\n# cloning from feature branches\ngit clone https://keesari.visualstudio.com/Microservices/_git/aspnetapi  -b develop C:\\Source\\Repos\\Microservices\\aspnetapi\n</code></pre> <p>Please refer to our Git Cheat-Sheet, which provides a comprehensive list of Git commands and their usage. </p> <p>Anji Keesari - Git Commands</p>"},{"location":"microservices/1.aspnet-api/#step-3-create-a-new-net-core-web-api-project","title":"Step-3: Create a new .NET Core Web API project","text":"<p>Using the .NET Core Web API template, we will create a new project that serves as the foundation for our RESTful service.</p> <p>We will be using Visual Studio Code instead of Visual Studio to make things faster and easy and save time and money.</p> <p>Best-practice</p> <p>I recommend using Visual Studio Code (VS Code) as your preferred development environment instead of Visual Studio.</p> <p>Visual Studio Code is a lightweight, cross-platform code editor that offers powerful features and extensions tailored for modern development workflows. It provides a streamlined and customizable interface, allowing you to focus on coding without unnecessary overhead.</p> <p>To create a new .NET Core Web API project, you will need to have the .NET Core SDK installed on your machine. You can download the .NET Core SDK from the .NET website Download .NET.</p> <p>Once you have the .NET Core SDK installed, follow these steps to create a new .NET Core Web API project:</p> <ol> <li>Open a terminal window and navigate to the directory where you want to create your project.</li> <li>Run the <code>dotnet new</code> command to create a new .NET Core Web API project: Let's take a look some useful <code>dotnet</code> command before creating the project. Use this command to get the <code>dotnet</code> commands help so that your get idea on how use these commands better.  <pre><code>dotnet --help\n</code></pre> Use this command to get list of available <code>dotnet</code> project templates <pre><code>dotnet new --list\n\n# output\n\nThese templates matched your input: \n\nTemplate Name                                 Short Name           Language    Tags\n--------------------------------------------  -------------------  ----------  -------------------------------------\nASP.NET Core Empty                            web                  [C#],F#     Web/Empty\nASP.NET Core gRPC Service                     grpc                 [C#]        Web/gRPC\nASP.NET Core Web API                          webapi               [C#],F#     Web/WebAPI\nASP.NET Core Web App                          razor,webapp         [C#]        Web/MVC/Razor Pages\nASP.NET Core Web App (Model-View-Controller)  mvc                  [C#],F#     Web/MVC\nASP.NET Core with Angular                     angular              [C#]        Web/MVC/SPA\nASP.NET Core with React.js                    react                [C#]        Web/MVC/SPA\nASP.NET Core with React.js and Redux          reactredux           [C#]        Web/MVC/SPA\nBlazor Server App                             blazorserver         [C#]        Web/Blazor\nBlazor WebAssembly App                        blazorwasm           [C#]        Web/Blazor/WebAssembly/PWA\nClass Library                                 classlib             [C#],F#,VB  Common/Library\nConsole App                                   console              [C#],F#,VB  Common/Console\n.\n.\nand more....\n</code></pre> Use this command to actually create new project <pre><code>dotnet new webapi -o aspnetapi\n\nor \ndotnet new webapi -o aspnetapi --no-https -f net7.0\n\ncd aspnetapi\n\ncode . \n\nor \ncode -r ../aspnetapi\n</code></pre> Additional Notes: <pre><code>  `-o` parameter creates a directory\n  `--no-https` flag creates an app that will run without an HTTPS certificate\n  `-f` parameter indicates creation\n\n# Output\n\nC:\\WINDOWS\\system32&gt;cd C:\\Source\\Repos\n\nC:\\Source\\Repos&gt;dotnet new webapi -o aspnetapi\nThe template \"ASP.NET Core Web API\" was created successfully.\n\nProcessing post-creation actions...\nRunning 'dotnet restore' on C:\\Source\\Repos\\aspnetapi\\aspnetapi.csproj...\n  Determining projects to restore...\n  Restored C:\\Source\\Repos\\aspnetapi\\aspnetapi.csproj (in 247 ms).\nRestore succeeded.\n\nC:\\Source\\Repos&gt;cd aspnetapi\n\nC:\\Source\\Repos\\aspnetapi&gt;code .\n</code></pre></li> <li>Here is the example of adding packages to .net projects. <pre><code>dotnet add package Microsoft.EntityFrameworkCore.InMemory\n</code></pre></li> <li>Run the following command to restore the project's dependencies: <pre><code>dotnet restore\n</code></pre></li> </ol> <p>Mac</p> <p>If you're on a Mac with an Apple M1 chip, you need to install the Arm64 version of the SDK before following above commands.</p> <p>Download .NET 7.0.</p> <p>Check the install typing by running following in terminal</p> <pre><code>dotnet\n</code></pre> <p>You should see an output similar to the following if the installation is successful</p> <pre><code>anjikeesari@Anjis-MacBook-Pro-2 MyMicroservice % dotnet\n\nUsage: dotnet [options]\nUsage: dotnet [path-to-application]\n\nOptions:\n  -h|--help         Display help.\n  --info            Display .NET information.\n  --list-sdks       Display the installed SDKs.\n  --list-runtimes   Display the installed runtimes.\n\npath-to-application:\n  The path to an application .dll file to execute.\n</code></pre>"},{"location":"microservices/1.aspnet-api/#step-4-test-the-new-net-core-web-api-project","title":"Step-4: Test the new .NET core Web API project","text":"<p>dotnet build</p> <p>Run the following command to build the project:</p> <p><code>dotnet build</code> command will look for the project or solution file in the current directory and compile the code in it. It will also restore any dependencies required by the project and create the output files in the bin directory.</p> <pre><code>dotnet build\n\n# output\n\nMicrosoft (R) Build Engine version 17.0.1+b177f8fa7 for .NET\nCopyright (C) Microsoft Corporation. All rights reserved.\n\n  Determining projects to restore...\n  All projects are up-to-date for restore.\n  AspNetApi -&gt; C:\\Source\\Repos\\AspNetApi\\aspnet-api\\bin\\Debug\\net6.0\\AspNetApi.dll\n\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n\nTime Elapsed 00:00:01.51\n</code></pre> <p>dotnet run</p> <p>Run the following command to start the development server:</p> <p><code>dotnet run</code> command will look for the project or solution file in the current directory and compile the code in it. After compiling, it will run the application and any output will be displayed in the console. <pre><code>dotnet run\n\n# output\n\nBuilding...\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: https://localhost:7136\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: http://localhost:5136\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Development\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: C:\\Source\\Repos\\AspNetApi\\aspnet-api\\\n</code></pre></p> <p>You will notice the URL in the output, copy the URL and paste it in your favorite browser. you will get a <code>404 error.</code> don\u2019t worry. Just type swagger at the end of the URL and press enter and you will get the following webpage.</p> <p></p> <ul> <li> <p>https://localhost:7136</p> </li> <li> <p>https://localhost:7136/swagger/index.html - Swagger URL</p> </li> <li> <p>https://localhost:7136/api/aspnetapi/v1/weatherforecast - API endpoint URL</p> </li> </ul> <p>If you are able to see this swagger URL in your browser then everything is created and setup as expected.</p> <p></p> <p>Use the following command to stop the application in VS Code</p> <p><pre><code>ctrl + c\n</code></pre> It is time to push your basic project template source into Azure DevOps Git repo.</p> <p>Best-practice</p> <p>To maintain good version control and ensure a reliable development process, it is strongly recommended to commit and push source code changes to your Git repository before proceeding to the next step.</p> <p>Use these git commands to push the source code.</p> <pre><code>git add .\ngit commit -am \"My fist commit - Create Web API project\"\ngit push\n</code></pre>"},{"location":"microservices/1.aspnet-api/#step-5-add-dockerfiles-to-the-api-project","title":"Step-5: Add Dockerfiles to the API project","text":"<p>Dockerfiles will be added to the project, which provide instructions for building a container image of our Web API application.</p> <p>There are multiple way to create <code>Dockerfile</code> depending on your code editor.  Here are the step-by-step instructions for creating a <code>Dockerfile</code> in a .NET Core Web API project:</p> <ol> <li>First, open your .NET Core Web API project in Visual Studio code or your favorite code editor.</li> <li>Next, create a new file in the root directory of your project and name it Dockerfile (with no file extension).</li> <li> <p>Open the Dockerfile and add the following code to the file: <pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"AspNetApi.csproj\", \".\"]\nRUN dotnet restore \"./AspNetApi.csproj\"\nCOPY . .\nWORKDIR \"/src/.\"\nRUN dotnet build \"AspNetApi.csproj\" -c Release -o /app/build\n\nFROM build AS publish\nRUN dotnet publish \"AspNetApi.csproj\" -c Release -o /app/publish\n\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\nENTRYPOINT [\"dotnet\", \"AspNetApi.dll\"]\n</code></pre> This code defines a Docker image that is based on the aspnet:6.0 image from Microsoft's container registry. The image is divided into four stages:</p> </li> <li> <p><code>base:</code> sets up the working directory and exposes port 80.</p> </li> <li><code>build:</code> restores the project dependencies, builds the project in Release mode, and copies the build output to the /app/build directory.</li> <li><code>publish:</code> publishes the project in Release mode and copies the published output to the /app/publish directory.</li> <li><code>final:</code> sets the working directory to <code>/app</code> and copies the published output from the <code>publish</code> stage to the current directory. It also specifies the entry point for the container, which is the <code>dotnet</code> command with the name of your project's DLL file.</li> </ol>"},{"location":"microservices/1.aspnet-api/#step-6-docker-build-run","title":"Step-6: Docker Build &amp; Run","text":"<p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application functions as expected.</p> <p><code>docker build</code> is a command that allows you to build a Docker image from a Dockerfile. The Dockerfile is a text file that contains instructions for Docker to build the image, including the base image to use, the files to include, the commands to run, and the ports to expose.</p> <p>To build and publish a container image for a .NET Core Web API project, you will need to have Docker installed on your machine. You can download Docker from the Docker website Get Started with Docker</p> <p>Once you have Docker installed, follow these steps to build and publish a container for your .NET Core Web API project:</p> <ol> <li>Open a terminal window and navigate to the root of the project.</li> <li>Run the <code>docker build</code> command to build the Docker image: <pre><code>docker build -t sample/aspnet-api:20230226.1 .\n</code></pre> output <pre><code>[+] Building 9.5s (19/19) FINISHED\n =&gt; [internal] load build definition from Dockerfile                                                                                                          0.0s \n =&gt; =&gt; transferring dockerfile: 878B                                                                                                                          0.0s \n =&gt; [internal] load .dockerignore                                                                                                                             0.0s \n =&gt; =&gt; transferring context: 374B  \n..            \n..\n..\n\n =&gt; =&gt; naming to docker.io/sample/aspnet-api:20230226.1                                                                                                             \n</code></pre> Verify the new image</li> </ol> <p>if you open the docker desktop you should be able to see the newly created image there.  3. Run the <code>docker run</code> command to start a container based on the image: <pre><code>docker run --rm -p 8080:80 sample/aspnet-api:20230226.1\n</code></pre> output <pre><code>info: Microsoft.Hosting.Lifetime[14]  \n      Now listening on: http://[::]:80\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Production\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: /app/\n</code></pre> Wait for the container to start. You should see output in the terminal indicating that the container is listening on port 80. Open the docker desktop to see the newly created container in the docker desktop app </p> <p>Open a web browser and navigate to http://localhost:8080/api/values (or whatever URL corresponds to your Web API endpoint) to confirm that the Web API is running inside the Docker container.</p> <p>use these links for testing when you run docker command from vs code</p> <ul> <li>http://localhost:8080/swagger/index.html</li> <li>http://localhost:8080/api/aspnetapi/v1/heartbeat/ping</li> <li>http://localhost:8080/api/aspnetapi/v1/weatherforecast</li> </ul> <p></p> <p>Best-practice</p> <p>When working with Docker containers, it is recommended to follow a consistent naming convention to ensure clarity and organization. The following pattern is suggested for naming Docker containers:</p> <pre><code>docker build -t projectname/domainname/appname:yyyymmdd.sequence .\n\nexample:\ndocker build -t project1/sample/aspnet-api:20230226 .\n</code></pre> <p>You've successfully created a Dockerfile and built a Docker image for your .NET Core Web API project. You can now distribute the Docker image to other machines or deploy it to a cloud service like Azure or AWS.</p> <p>Tip</p> <p>If you need to clean up containers and images locally in Docker Desktop, you can use the following commands:</p> <pre><code># To delete all containers including its volumes use,\n#     docker rm -vf $(docker ps -aq)\n\n# To delete all the images,\n#     docker rmi -f $(docker images -aq)\n</code></pre>"},{"location":"microservices/1.aspnet-api/#step-7-push-docker-container-to-acr","title":"Step-7: Push docker container to ACR","text":"<p>Finally, we will publish the built Docker container to the Azure Container Registry (ACR), making it accessible for deployment and distribution.</p> <p>Now we've Docker Containers ready for push to Container Registry so that we can use them in future labs.</p> <p>To publish a Docker container image to Azure Container Registry (ACR), you will need to have the following:</p> <ol> <li>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI. As part of the Chapter-2 we will create this azure resource, you can come back to this steps after ACR is created.</li> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre></li> <li><code>Tag</code> your Docker container with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command: <pre><code>docker tag sample/aspnet-api:20230226.1 acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> Use this command to see a list of your current local images <pre><code>docker images\n\n# output\n\nREPOSITORY                             TAG          IMAGE ID       CREATED         SIZE\nacr1dev.azurecr.io/sample/aspnet-api   20230226.1   1bab8ba123ca   2 hours ago     213MB\n</code></pre></li> <li>Push your Docker container to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker push acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n\n# output\n\nThe push refers to repository [acr1dev.azurecr.io/sample/aspnet-api]\na592c2e20b23: Pushed\n5f70bf18a086: Layer already exists\nd57ad0aaee3b: Layer already exists\naff5d88d936a: Layer already exists\nb3b2bd456a19: Layer already exists\n2540ef4bc011: Layer already exists\n94100d1041b6: Layer already exists\nbd2fe8b74db6: Layer already exists\n20230226.1: digest: sha256:026ec79d24fca0f30bcd90c7fa17e82a2347cf7bc5ac5d762a630277086ed0d1 size: 1995\n</code></pre></li> <li>Wait for the push to complete. Depending on the size of your Docker container and the speed of your internet connection, this may take a few minutes.</li> <li>Verify the newly pushed image to ACR. <pre><code># List images in registry\naz acr repository list --name acr1dev --output table\n\n# output\n\nResult\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\n</code></pre></li> <li>Show the new tags of a image in the acr <pre><code>az acr repository show-tags --name acr1dev --repository sample/aspnet-api --output table\n\n# output\n\nResult\n----------\n20230220.1\n20230226.1\n</code></pre></li> </ol> <p>You've successfully pushed your Docker container to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/1.aspnet-api/#step-8-pull-docker-container-from-acr","title":"Step-8: Pull docker container from ACR","text":"<p>Pull docker container from ACR is something may be helpful during container troubleshooting.</p> <p>To pull a Docker container from Azure Container Registry (ACR), you need to perform the following steps:</p> <ol> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># Log in to the container registry\naz acr login --name acr1dev\n</code></pre></li> <li>Pull your Docker container from your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker pull acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n\n# output\n\n20230226.1: Pulling from sample/aspnet-api\n01b5b2efb836: Already exists\nc4c81489d24d: Already exists\n95b82a084bc9: Already exists\nbb369c4b0f26: Already exists \nc888ac593815: Already exists\n14ce87409b2e: Already exists\n4f4fb700ef54: Already exists\nd15d1be868b7: Already exists\nDigest: sha256:026ec79d24fca0f30bcd90c7fa17e82a2347cf7bc5ac5d762a630277086ed0d1\nStatus: Downloaded newer image for acr1dev.azurecr.io/sample/aspnet-api:20230226.1\nacr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre></li> <li>Wait for the pull to complete. Depending on the size of your Docker container and the speed of your internet connection, this may take a few minutes. </li> <li>Verify the recently pulled container from ACR to make sure it running as expected <pre><code>docker run --rm -p 8080:80 acr1dev.azurecr.io/sample/aspnet-api:20230226.1\n</code></pre> Test the container running following URL</li> </ol> <p></p> <p>http://localhost:8080/swagger/index.html</p> <p>You've successfully pulled your Docker container from Azure Container Registry. You can now use the Docker command-line interface to manage your container and run them locally or deploy them to other environments.</p>"},{"location":"microservices/1.aspnet-api/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Tutorial: Create a web API with ASP.NET Core</li> <li>Microsoft MSDN - Create and deploy a cloud-native ASP.NET Core microservice</li> <li>Microsoft MSDN - .NET Tutorial - Your First Microservice</li> <li>Visual Studio Core - ASP.NET Core in a container</li> <li>Visual Studio Core - Docker in Visual Studio Code</li> <li>Visual Studio Core - Node.js in a container</li> <li>github - ASP.NET Core Docker Sample</li> <li>Containerize a .NET application</li> </ul>"},{"location":"microservices/2.node-api/","title":"Create Your Second Microservice with Node.js","text":""},{"location":"microservices/2.node-api/#introduction","title":"Introduction","text":"<p>Welcome to the second lab in our Microservices chapter. In this session, I will guide you through the creation of a simple RESTful service using the Node.js <code>npx express-generator</code> project template.</p> <p>This lab aims to illustrate the process of building a RESTful service and generating a Docker container using a Dockerfile. By following this example, you will gain a solid understanding of the fundamentals involved in creating RESTful APIs with Node.js.</p>"},{"location":"microservices/2.node-api/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Backend (BE)</code> developer, you have been tasked with creating a RESTful service using Node JS, which is one of the services on our microservices list. This lab will serve as your introduction to the Microservices Architecture, starting with the basics of setting up a repository, creating a small API project, and ultimately containerizing the microservice you build. The containerized microservice will then be pushed to the Azure Container Registry (ACR).</p> <p>The primary objective of this lab is to prepare an application for deployment on Kubernetes. The microservices you create in this lab will be utilized in subsequent labs, such as the creation of DevOps pipelines or the deployment to Azure Kubernetes Services (AKS). By completing this lab, you will gain a foundational understanding of how microservices can be developed, containerized, and integrated into a Kubernetes environment.</p>"},{"location":"microservices/2.node-api/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup repository in Azure DevOps.</li> <li>Step-2: Create a new Node.js API project.</li> <li>Step-3: Test Node.js API project.</li> <li>Step-4: Add Dockerfiles to the project.</li> <li>Step-5: Docker build locally.</li> <li>Step-6: Docker run locally.</li> <li>Step-7: Publish the Docker container to ACR.</li> </ul>"},{"location":"microservices/2.node-api/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Node.js and npm:  - Node.js Downloads.</li> <li>Docker and the VS Code Docker extension :  - Docker Downloads.</li> <li>Git Client tool:  - Git Downloads.</li> <li>A project in Azure DevOps and Git Repository: Initialize a Git repository for your Node.js application.</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"microservices/2.node-api/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create the Rest API using Node JS.</p> <p></p>"},{"location":"microservices/2.node-api/#step-1-setup-repository-in-azure-devops","title":"Step-1: Setup repository in Azure DevOps.","text":"<p>For this project, you can either leverage an existing Git repository created in our first chapter or initiate a new one.</p> <p>To clone an existing repository, execute the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/2.node-api/#step-2-create-a-new-node-js-api-project","title":"Step-2: Create a new <code>Node JS</code> API project","text":"<p>In this step, we will set up a new Node.js API project using a basic Express application as our example. To expedite the process, we'll utilize Express's scaffolding tool to generate the necessary directory structure and essential files.</p> <p>Open your terminal and execute the following commands:</p> <pre><code>$ npx express-generator --no-view src\n$ cd src\n$ npm install\n</code></pre> <p>npx express-generator:</p> <p>The npx express-generator command initializes the project, creating a structure that includes directories like 'bin' and 'routes'.</p> <p></p> <p>npm install:</p> <p>Ensure you run npm install to set up and configure all required Node.js modules.</p> <p>This step ensures that your project is equipped with the necessary dependencies, allowing seamless integration with Docker and efficient containerization of your Node.js application.</p> <p></p> <p>folder structure</p> <p>you've established the foundation for your Node.js API project, complete with a standardized directory structure and essential files.</p> <p></p> <p>This should have created a number of files in your directory, including bin and routes directories. Make sure to run npm install so that npm can get all of your Node.js modules set up and ready to use.</p>"},{"location":"microservices/2.node-api/#step-3-test-the-node-js-api-project","title":"Step-3: Test the Node JS API project","text":"<p>Now, let's verify that our Node.js API project is functioning correctly. We'll initiate the application for the first time, utilizing the default routes defined in <code>app.js</code>.</p> <p>Ensure you are in the project directory, and in your terminal, execute the following command to start the application: <pre><code>npm start\n</code></pre></p> <p>This command launches the Node.js application, making it accessible locally.</p> <p>Open your web browser and navigate to http://localhost:3000</p> <p></p> <p>You confirm that your Node.js API project is up and running on your local environment. This preliminary test ensures the initial functionality of your application before proceeding with additional configurations or containerization.</p>"},{"location":"microservices/2.node-api/#step-4-add-dockerfiles-to-the-mvc-project","title":"Step-4: Add Dockerfiles to the MVC project","text":"<p>To seamlessly containerize our Node.js API project, let's create a Dockerfile in the root directory of your project and incorporate the following code. The Dockerfile provides instructions for building a container image of our Node.js API.</p> <pre><code># Use the official Node.js image from Docker Hub with a specific version\nFROM node:18.16.0-alpine3.17\n\n# Create a directory for the application in the container\nRUN mkdir -p /opt/app\n\n# Set the working directory inside the container to /opt/app\nWORKDIR /opt/app\n\n# Copy package.json and package-lock.json to the container's working directory\nCOPY src/package.json src/package-lock.json .\n\n# Install Node.js dependencies based on the package.json and package-lock.json\nRUN npm install\n\n# Copy the entire contents of the 'src' directory to the container's working directory\nCOPY src/ .\n\n# Expose port 3000 to allow external access to the application\nEXPOSE 3000\n\n# Specify the command to run when the container starts (start the application)\nCMD [\"npm\", \"start\"]\n</code></pre> <p>Note</p> <p>Read inline comments of the Dockerfile for understanding the Dockerfile instructions</p> <p></p>"},{"location":"microservices/2.node-api/#step-5-docker-build-locally","title":"Step-5: Docker build locally","text":"<p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application functions as expected.</p> <p>The <code>docker build</code> command is used to build Docker images from a Dockerfile.  </p> <pre><code>docker build -t sample/node-api:20240101.1 .\n</code></pre> <p>output</p> <p></p> <p>When you run the <code>docker build</code> command, Docker looks for a Dockerfile in the specified directory (PATH) and reads the instructions in the file to build a new image. </p> <p>The Dockerfile contains a series of instructions that define how to build the image, such as copying files, running commands, and setting environment variables. </p>"},{"location":"microservices/2.node-api/#step-6-docker-run-locally","title":"Step-6: Docker run locally","text":"<p>Run the Docker container locally to verify that the application functions correctly within a containerized environment. This step ensures that the containerized application operates as expected on your local machine.</p> <p>Run the <code>docker run</code> command to start a container based on the image:</p> <p><pre><code>docker run --rm -p 3000:3000 sample/node-api:20240101.1 .\n</code></pre> output</p> <p><pre><code>Compiled successfully!\n\nYou can now view node-api in the browser.\n\n  Local:            http://localhost:3000\n  On Your Network:  http://172.17.0.2:3000\n\nNote that the development build is not optimized.\nTo create a production build, use npm run build.\n\nwebpack compiled successfully\nCompiling...\nCompiled successfully!\nwebpack compiled successfully\n</code></pre> if you open the docker desktop you will notice the new image &amp; container started running.</p> <p>Image</p> <p></p> <p>Container</p> <p></p> <p>This will start the Node.js application in the Docker container and map the container's port 3000 to your local machine's port 3000. </p> <p>Your Node.js application is now running inside a Docker container.</p> <p>Open your favorite browser and enter the following URL to see the running application in port 3000</p> <p>http://localhost:3000/</p> <p></p> <p>You now have a basic Node.js application up and running. From here, you can continue building out your application by adding more and more code as per your requirements.</p>"},{"location":"microservices/2.node-api/#step-7-push-docker-container-to-acr","title":"Step-7: Push docker container to ACR","text":"<p>Now that we have Docker containers ready locally, it's time to push them to the Container Registry for future deployment on Azure Kubernetes Services (AKS). This step is crucial for preparing the container for deployment in a cloud environment.</p> <p>To publish a Docker container to Azure Container Registry (ACR), you will need to have the following:</p> <p>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI.</p> <p>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n# Login Succeeded\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre> <p>list all the Docker images that are available on the local system</p> <pre><code>docker images\n\n# output\n\nREPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\nsample/aspnet-app                                         20230312.1                                                                   587f347206bc   8 minutes ago   216MB\n.\n.\n.\n</code></pre> <p><code>Tag</code> your Docker container image with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command:</p> <pre><code>docker tag sample/node-api:20240101.1 acr1dev.azurecr.io/sample/node-api:20240101.1\n</code></pre> <p>Push your Docker container image to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code>docker push acr1dev.azurecr.io/sample/node-api:20240101.1\n\n# Output\nThe push refers to repository [acr1dev.azurecr.io/sample/node-api]\n649a035a1734: Pushed\n4061bd2dd536: Pushed\nc0257b3030b0: Pushed\n912a3b0fc587: Pushed\na36186d93e25: Pushed\na3d997b065bc: Pushed\n65d358b7de11: Pushed\nf97384e8ccbc: Pushed\nd56e5e720148: Pushed\nbeee9f30bc1f: Pushed\n20240101.1: digest: sha256:73f0669d18c6cae79beb81edc8c523191710f9ec4781d590884b46326f9ad6f9 size: 2419\n</code></pre> <p>Wait for the push to complete. Depending on the size of your Docker container image and the speed of your internet connection, this may take a few minutes.</p> <p>Verify the newly pushed image to ACR.</p> <pre><code>az acr repository list --name acr1dev --output table\n\n# Output\nResult\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\nsample/node-api\n</code></pre> <p>Show the new tags of a image in the acr</p> <pre><code>az acr repository show-tags --name acr1dev --repository sample/node-api --output table\n\n# output\n\nResult\n----------\n20240101.1\n</code></pre> <p>You've successfully pushed your Docker container image to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container images and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/2.node-api/#conclusion","title":"Conclusion","text":"<p>So, we've covered Docker and learned how to run a basic Node.js application inside a container. Now, you should feel confident and ready to create your own Dockerfile, tapping into the cool features that Docker brings to your development experience.</p>"},{"location":"microservices/2.node-api/#reference","title":"Reference","text":"<ul> <li>Containerize a Node.js application</li> <li>Dockerizing a Node.js Web Application</li> </ul>"},{"location":"microservices/3.aspnet-app/","title":"Create Your First Website using .NET Core MVC Application","text":""},{"location":"microservices/3.aspnet-app/#introduction","title":"Introduction","text":"<p>In our previous labs, we have explored the creation of Microservices to demonstrate the Microservices architecture pattern. In this lab and the next, we will shift our focus to the MicroFrontend architecture pattern by creating a couple of MicroFrontend UI applications.</p>"},{"location":"microservices/3.aspnet-app/#technical-scenario","title":"Technical Scenario","text":"<p>As a Frontend (FE) developer, your task is to develop a website or UI application using ASP.NET Core MVC technology. This application represents one of the small Website (UI) components in our MicroFrontend applications list.</p> <p>This lab will guide you through the process of building an ASP.NET Core MVC application. We will begin by creating a new Git repository or utilizing an existing one. Next, we will generate an MVC project template and proceed to containerize the UI application. Finally, we will push the containerized UI application to the Azure Container Registry (ACR) in preparation for deployment to Azure Kubernetes Services (AKS).</p> <p>The objective is to prepare a UI application for deployment on Kubernetes. The UI applications developed in this lab will be utilized in subsequent labs, such as the creation of DevOps pipelines or the deployment to Azure Kubernetes Services.</p>"},{"location":"microservices/3.aspnet-app/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Create a new ASP.NET Core Web App (MVC project)</li> <li>Step-2: Test ASP.NET MVC project</li> <li>Step-3: Update home page contents [optional]</li> <li>Step-4: Add Dockerfiles to MVC project</li> <li>Step-5: Docker Build locally</li> <li>Step-6: Docker Run locally</li> <li>Step-7: Publish docker container to ACR</li> </ul>"},{"location":"microservices/3.aspnet-app/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, make sure you have the following prerequisites in place:</p> <ul> <li>Clone existing Microservices repo</li> <li>Download and install software for .NET development </li> <li>Docker desktop</li> <li>VS Code Docker extension</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"microservices/3.aspnet-app/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create the website using ASP.NET Core MVC.</p> <p></p>"},{"location":"microservices/3.aspnet-app/#step-1-create-a-new-aspnet-core-web-app-mvc-project","title":"Step-1: Create a new ASP.NET Core Web App (MVC project)","text":"<p>Create a new ASP.NET Core Web App using the MVC project template. This will serve as the foundation for our UI application.</p> <p>To create new ASP.NET Core Web App (Model-View-Controller) project you can use either Visual Studio Code or Visual Studio 2022 (latest version).</p> <p>Using Visual Studio Code</p> <p>Assuming you already have the .NET Core SDK installed in your system, follow these steps to create a new .NET Core MVC project:</p> <p>Open Visual Studio Code and open the terminal and use following command to see list of templates.</p> <pre><code>dotnet new --list\n</code></pre> <p>output</p> <pre><code>These templates matched your input: \n\nTemplate Name                                 Short Name           Language    Tags\n--------------------------------------------  -------------------  ----------  -------------------------------------\nASP.NET Core Empty                            web                  [C#],F#     Web/Empty\nASP.NET Core gRPC Service                     grpc                 [C#]        Web/gRPC\nASP.NET Core Web API                          webapi               [C#],F#     Web/WebAPI\nASP.NET Core Web App                          razor,webapp         [C#]        Web/MVC/Razor Pages\nASP.NET Core Web App (Model-View-Controller)  mvc                  [C#],F#     Web/MVC\nASP.NET Core with Angular                     angular              [C#]        Web/MVC/SPA\nASP.NET Core with React.js                    react                [C#]        Web/MVC/SPA\nASP.NET Core with React.js and Redux          reactredux           [C#]        Web/MVC/SPA\nBlazor Server App                             blazorserver         [C#]        Web/Blazor\nBlazor WebAssembly App                        blazorwasm           [C#]        Web/Blazor/WebAssembly/PWA\nClass Library                                 classlib             [C#],F#,VB  Common/Library\nConsole App                                   console              [C#],F#,VB  Common/Console\n.\n.\nand more....\n</code></pre> <p>Pick the following template for our MVC project from the list.</p> <p><pre><code>ASP.NET Core Web App (Model-View-Controller)  mvc                  [C#],F#     Web/MVC\n</code></pre> Use <code>dotnet new</code> command to create new MVC project</p> <pre><code>dotnet new mvc -o aspnet-app\n</code></pre> <p>output</p> <pre><code>The template \"ASP.NET Core Web App (Model-View-Controller)\" was created successfully.\nThis template contains technologies from parties other than Microsoft, see https://aka.ms/aspnetcore/6.0-third-party-notices for details.\n\nProcessing post-creation actions...\nRunning 'dotnet restore' on C:\\Source\\Repos\\microservices\\aspnet-app\\aspnet-app.csproj...\n  Determining projects to restore...\n  Restored C:\\Source\\Repos\\microservices\\aspnet-app\\aspnet-app.csproj (in 95 ms).\nRestore succeeded.\n</code></pre> <p>Using Visual Studio 2022</p> <p>In case if you want to use Visual Studio only then, here are the steps to create a new ASP.NET Core Web App using the Model-View-Controller (MVC) architectural pattern:</p> <ul> <li>Open Visual Studio and select \"Create a new project\".</li> <li>In the \"Create a new project\" window, select \"ASP.NET Core Web Application\" and click \"Next\".</li> <li>Choose a name and location for your project and click \"Create\".</li> <li>In the \"Create a new ASP.NET Core Web Application\" window, select \"Web Application (Model-View-Controller)\" and click \"Create\".</li> </ul> <p>Visual Studio will create a new project for you with the necessary files and folders to get started.</p> <p>Once the MVC project is created successfully you will see the project folder structure like below:</p> <p></p> <p>cd to the new folder here <code>aspnet-app</code></p> <pre><code>cd .\\aspnet-app\\\n</code></pre>"},{"location":"microservices/3.aspnet-app/#step-2-test-the-new-aspnet-core-web-app-project","title":"Step-2: Test the new ASP.NET core Web App project","text":"<p>Perform testing of the ASP.NET MVC project to ensure its functionality and identify any issues or bugs that may need to be addressed.</p> <p>Run the following command to build the project:</p> <p><code>dotnet build</code> command will look for the project or solution file in the current directory and compile the code in it. It will also restore any dependencies required by the project and create the output files in the bin directory. </p> <pre><code>dotnet build\n</code></pre> <p>output</p> <p><pre><code>Microsoft (R) Build Engine version 17.0.1+b177f8fa7 for .NET\nCopyright (C) Microsoft Corporation. All rights reserved.\n\n  Determining projects to restore...\n  All projects are up-to-date for restore.\n  aspnet-app -&gt; C:\\Source\\Repos\\microservices\\aspnet-app\\bin\\Debug\\net6.0\\aspnet-app.dll\n\nBuild succeeded.\n    0 Warning(s)\n    0 Error(s)\n\nTime Elapsed 00:00:05.07\n</code></pre> Run the following command to start the development server:</p> <p><code>dotnet run</code> command will look for the project or solution file in the current directory and compile the code in it. After compiling, it will run the application and any output will be displayed in the console.</p> <pre><code>dotnet run\n</code></pre> <p>output</p> <pre><code>Building...\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: https://localhost:7289\ninfo: Microsoft.Hosting.Lifetime[14]\n      Now listening on: http://localhost:5023\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Development\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: C:\\Source\\Repos\\microservices\\aspnet-app\\\n</code></pre> <p>You will notice the URL in the output, copy the URL and paste it in your favorite browser. https://localhost:7289/</p> <p></p> <p>For the first time if you are able to see this page in your browser that means ASP.NET MVC project is created as expected.</p> <p>Use these git commands to push the source code to remote git.</p> <pre><code>git add .\ngit commit -a -m \"My fist mvc app commit.\"\ngit push --set-upstream origin main\ngit status\n</code></pre> <p>Note</p> <p>For the simplicity I am creating all the applications in the same repo called <code>microservices</code> but in reality you may need to follow your organization standards for creating git repos</p> <p>New folder structure will look like below in the microservices git repo. you will notice the new <code>aspnet-app</code> folder with MVC project source code. </p> <p></p>"},{"location":"microservices/3.aspnet-app/#step-3-update-home-page-contentsoptional","title":"Step-3: Update home page contents[Optional]","text":"<p>Let's update our landing page to show .NET version, Operating System, processor, CPU core etc.. this information will provide us some technical details of the application when we deploy it in our AKS in the upcoming labs.  </p> <p>We are going to update the <code>Index.html</code> file with following code.</p> <p>Index.html<pre><code>@page\n@using System.Runtime.InteropServices\n@using System.IO\n@using System.Diagnostics\n@{\n    //ViewData[\"Title\"] = \"Home page\";\n    var hostName = System.Net.Dns.GetHostName();\n    var ipList = await System.Net.Dns.GetHostAddressesAsync(hostName);\n\n    const long Mebi = 1024 * 1024;\n    const long Gibi = Mebi * 1024;\n    GCMemoryInfo gcInfo = GC.GetGCMemoryInfo();\n    string totalAvailableMemory = GetInBestUnit(gcInfo.TotalAvailableMemoryBytes);\n\n    bool cgroup = RuntimeInformation.OSDescription.StartsWith(\"Linux\") &amp;&amp; Directory.Exists(\"/sys/fs/cgroup/memory\");\n    string memoryUsage = string.Empty;\n    string memoryLimit = string.Empty;\n\n    if (cgroup)\n    {\n        string usage = System.IO.File.ReadAllLines(\"/sys/fs/cgroup/memory/memory.usage_in_bytes\")[0];\n        string limit = System.IO.File.ReadAllLines(\"/sys/fs/cgroup/memory/memory.limit_in_bytes\")[0];\n        memoryUsage = GetInBestUnit(long.Parse(usage));\n        memoryLimit = GetInBestUnit(long.Parse(limit));\n    }\n}\n&lt;div align=\"center\"&gt;\n    &lt;table class=\"table table-striped table-hover\"&gt;\n        &lt;tr&gt;\n            &lt;td&gt;.NET version&lt;/td&gt;\n            &lt;td&gt;@RuntimeInformation.FrameworkDescription&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Operating system&lt;/td&gt;\n            &lt;td&gt;@RuntimeInformation.OSDescription&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Processor architecture&lt;/td&gt;\n            &lt;td&gt;@RuntimeInformation.OSArchitecture&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;CPU cores&lt;/td&gt;\n            &lt;td&gt;@Environment.ProcessorCount&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Containerized&lt;/td&gt;\n            &lt;td&gt;@(Environment.GetEnvironmentVariable(\"DOTNET_RUNNING_IN_CONTAINER\") is null ? \"false\" : \"true\")&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Memory, total available GC memory&lt;/td&gt;\n            &lt;td&gt;@totalAvailableMemory&lt;/td&gt;\n        &lt;/tr&gt;\n        @if (cgroup)\n        {\n            &lt;tr&gt;\n                &lt;td&gt;cgroup memory usage&lt;/td&gt;\n                &lt;td&gt;@memoryUsage&lt;/td&gt;\n            &lt;/tr&gt;\n            &lt;tr&gt;\n                &lt;td&gt;cgroup memory limit&lt;/td&gt;\n                &lt;td&gt;@memoryLimit&lt;/td&gt;\n            &lt;/tr&gt;\n        }\n        &lt;tr&gt;\n            &lt;td&gt;Host name&lt;/td&gt;\n            &lt;td&gt;@hostName&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td style=\"vertical-align: top\"&gt;Server IP address&lt;/td&gt;\n            &lt;td&gt;\n                @{\n                    foreach (var ip in ipList)\n                    {\n                        @ip\n                        &lt;br /&gt;\n                    }\n                }\n            &lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/table&gt;\n&lt;/div&gt;\n\n@{\n    string GetInBestUnit(long size)\n    {\n        if (size &lt; Mebi)\n        {\n            return $\"{size} bytes\";\n        }\n        else if (size &lt; Gibi)\n        {\n            decimal mebibytes = Decimal.Divide(size, Mebi);\n            return $\"{mebibytes:F} MiB\";\n        }\n        else\n        {\n            decimal gibibytes = Decimal.Divide(size, Gibi);\n            return $\"{gibibytes:F} GiB\";\n        }\n    }\n}\n</code></pre> Here is the home page with new details:</p> <p></p> <p>Now it is time to commit our source code </p> <pre><code>git add .\ngit commit -am \"updated landing page\"\ngit push \n</code></pre>"},{"location":"microservices/3.aspnet-app/#step-4-add-dockerfiles-to-the-mvc-project","title":"Step-4: Add Dockerfiles to the MVC project","text":"<p>This file define the necessary instructions to build Docker images for the application.</p> <p>Create a Dockerfile in the root directory of the MVC project and copy following code.</p> <pre><code>#See https://aka.ms/containerfastmode to understand how Visual Studio uses this Dockerfile to build your images for faster debugging.\n# Use the official Microsoft ASP.NET Core runtime image as a parent image\nFROM mcr.microsoft.com/dotnet/aspnet:6.0 AS base\nWORKDIR /app\nEXPOSE 80\nEXPOSE 443\n\n# Copy the project files and restore dependencies\nFROM mcr.microsoft.com/dotnet/sdk:6.0 AS build\nWORKDIR /src\nCOPY [\"aspnet-app.csproj\", \".\"]\nRUN dotnet restore \"./aspnet-app.csproj\"\n\n# Copy the remaining files and build the application\nCOPY . .\nWORKDIR \"/src/.\"\nRUN dotnet build \"aspnet-app.csproj\" -c Release -o /app/build\n\n# Publish the application\nFROM build AS publish\nRUN dotnet publish \"aspnet-app.csproj\" -c Release -o /app/publish\n\n# Final image\nFROM base AS final\nWORKDIR /app\nCOPY --from=publish /app/publish .\n# Start the application\nENTRYPOINT [\"dotnet\", \"aspnet-app.dll\"]\n</code></pre> <p>Note</p> <p>Read inline comments of the Dockerfile for understanding the Dockerfile instructions</p>"},{"location":"microservices/3.aspnet-app/#step-5-docker-build-locally","title":"Step-5: Docker Build locally","text":"<p>Build Docker images locally using the Dockerfiles added to the MVC project. This process will generate container images ready for deployment.</p> <p>The <code>docker build</code> command is used to build Docker images from a Dockerfile. The Dockerfile contains a set of instructions that Docker uses to create a new image. </p> <p><pre><code>docker build -t sample/aspnet-app:20230312.1 .\n</code></pre> <code>-t</code> to specify a name and optionally a tag for the image,</p> <p>output</p> <pre><code>[+] Building 49.9s (18/18) FINISHED\n =&gt; [internal] load build definition from Dockerfile\n =&gt; =&gt; transferring dockerfile: 696B                \n =&gt; [internal] load .dockerignore                   \n =&gt; =&gt; transferring context: 2B                     \n =&gt; [internal] load metadata for mcr.microsoft.com/dotnet/sdk:6.0        \n =&gt; [internal] load metadata for mcr.microsoft.com/dotnet/aspnet:6.0 \n .\n .\n .\n =&gt; exporting to image \n =&gt; =&gt; exporting layers\n =&gt; =&gt; writing image sha256:587f347206bcc67dafe3c0b53047862f11b6e52b1b61bce15b8432cc3a488e24\n =&gt; =&gt; naming to docker.io/sample/aspnet-app:20230312.1  \n</code></pre> <p>When you run the <code>docker build</code> command, Docker looks for a Dockerfile in the specified directory (PATH) and reads the instructions in the file to build a new image. The Dockerfile contains a series of instructions that define how to build the image, such as copying files, running commands, and setting environment variales. </p> <p>Error &amp; troubleshooting</p> <p>In case if you are getting following error while running <code>docker build</code> command, that means the docker desktop is not running locally. make sure that run the docker desktop locally to fix this issue.</p> <pre><code>error during connect: This error may indicate that the docker daemon is not running.: Post \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/build?buildargs=%7B%7D&amp;cachefrom=%5B%5D&amp;cgroupparent=&amp;cpuperiod=0&amp;cpuquota=0&amp;cpusetcpus=&amp;cpusetmems=&amp;cpushares=0&amp;dockerfile=Dockerfile&amp;labels=%7B%7D&amp;memory=0&amp;memswap=0&amp;networkmode=default&amp;rm=1&amp;shmsize=0&amp;t=sample%2Faspnet-app%3A20230312.1&amp;target=&amp;ulimits=null&amp;version=1\": open //./pipe/docker_engine: The system cannot find the file specified.\n</code></pre>"},{"location":"microservices/3.aspnet-app/#step-6-docker-run-locally","title":"Step-6: Docker Run locally","text":"<p>Run the Docker container locally to verify that the application functions correctly within a containerized environment. This step ensures that the containerized application operates as expected on your local machine.</p> <p>Run the <code>docker run</code> command to start a container based on the image:</p> <pre><code>docker run --rm -p 8080:80 sample/aspnet-app:20230312.1\n</code></pre> <p>output <pre><code>warn: Microsoft.AspNetCore.DataProtection.Repositories.FileSystemXmlRepository[60]\n      Storing keys in a directory '/root/.aspnet/DataProtection-Keys' that may not be persisted outside of the container. Protected data will be unavailable when container is destroyed.\nwarn: Microsoft.AspNetCore.DataProtection.KeyManagement.XmlKeyManager[35]\n      No XML encryptor configured. Key {90c41ec3-18a3-434a-8d4b-1d0cc5f140af} may be persisted to storage in unencrypted form.\ninfo: Microsoft.Hosting.Lifetime[14]  \n      Now listening on: http://[::]:80\ninfo: Microsoft.Hosting.Lifetime[0]\n      Application started. Press Ctrl+C to shut down.\ninfo: Microsoft.Hosting.Lifetime[0]\n      Hosting environment: Production\ninfo: Microsoft.Hosting.Lifetime[0]\n      Content root path: /app/\nwarn: Microsoft.AspNetCore.HttpsPolicy.HttpsRedirectionMiddleware[3]\n      Failed to determine the https port for redirect.\n</code></pre> if you open the docker desktop you will notice the new container started running.</p> <p></p> <p>http://localhost:8080/</p> <p></p> <p>You now have a basic ASP.NET Core Web App using the MVC pattern up and running. From here, you can continue building out your application by adding more controllers, views, and models as needed.</p>"},{"location":"microservices/3.aspnet-app/#step-7-push-docker-container-to-acr","title":"Step-7: Push docker container to ACR","text":"<p>Publish the Docker container to the Azure Container Registry (ACR) for future deployment to Azure Kubernetes Services (AKS). This step prepares the container for deployment to the cloud environment.</p> <p>Now we've Docker Containers ready locally for push to Container Registry so that we can use them in future labs.</p> <p>To publish a Docker container to Azure Container Registry (ACR), you will need to have the following:</p> <p>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI.</p> <p>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre> <p>list all the Docker images that are available on the local system</p> <pre><code>docker images\n</code></pre> <p>output</p> <p><pre><code>REPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\nsample/aspnet-app                                         20230312.1                                                                   587f347206bc   8 minutes ago   216MB\n.\n.\n.\n</code></pre> <code>Tag</code> your Docker container with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command:</p> <p><pre><code>docker tag sample/aspnet-app:20230312.1 acr1dev.azurecr.io/sample/aspnet-app:20230312.1\n</code></pre> Push your Docker container to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code>docker push acr1dev.azurecr.io/sample/aspnet-app:20230312.1\n</code></pre> <p>Output</p> <p><pre><code>The push refers to repository [acr1dev.azurecr.io/sample/aspnet-app]\nf9c45e227c3a: Pushed\n5f70bf18a086: Mounted from sample/aspnet-api\n478d6dc381e4: Pushed\n355b7bb8c23e: Pushed\nff13768cb51e: Pushed\nfe674e2b138c: Pushed\nf30d150c0152: Pushed\n4695cdfb426a: Pushed\n20230312.1: digest: sha256:049b736aa29e9574010dfe1fc2ef5bb44ed76d54757a8f190b967fa0f854567e size: 1995\n</code></pre> 1. Wait for the push to complete. Depending on the size of your Docker containers and the speed of your internet connection, this may take a few minutes. 1. Verify the newly pushed image to ACR. <pre><code>az acr repository list --name acr1dev --output table\n</code></pre></p> <p>Output</p> <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\n</code></pre> <p>Show the new tags of a image in the acr</p> <pre><code>az acr repository show-tags --name acr1dev --repository sample/aspnet-api --output table\n</code></pre> <p>output</p> <pre><code>Result\n----------\n20230312.1\n</code></pre> <p>You've successfully pushed your Docker container to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/4.react-app/","title":"Create Your Second Website using React.js","text":""},{"location":"microservices/4.react-app/#introduction","title":"Introduction","text":"<p>In this lab, we will create our second website using React JS, which serves as another MicroFrontend application in our Microservices architecture.</p> <p>Our goal is to prepare a React JS application for deployment on Kubernetes. The UI applications developed in this lab will be utilized in subsequent labs, including the creation of DevOps pipelines and the deployment to Azure Kubernetes Services.</p> <p>Let's look into the lab and begin our journey by creating a React JS application as part of the Microservices Architecture!</p>"},{"location":"microservices/4.react-app/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Frontend (FE)</code> developer, you have been assigned the task of developing a website or UI application using React JS technology. This website will be one of the small Website (UI) components in our MicroFrontend applications list.</p> <p>This lab aims to provide you with hands-on experience in creating a React JS application as part of the Microservices Architecture. We will begin by utilizing an existing Git repository and proceed to create a new React JS project within it. Finally, we will containerize this website and push it to the Azure Container Registry (ACR) in preparation for deployment to Azure Kubernetes Services (AKS).</p>"},{"location":"microservices/4.react-app/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step 1: Install Node.js and NPM</li> <li>Step-2: Create new React JS application</li> <li>Step-3: Add Dockerfiles to the React JS project</li> <li>Step-4: Docker Build locally</li> <li>Step-5: Docker Run locally</li> <li>Step-6: Publish docker container to ACR</li> </ul>"},{"location":"microservices/4.react-app/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git Repository</li> <li>Clone existing Microservices repo</li> <li>Download and install software for React Development </li> <li>Docker desktop</li> <li>VS Code with Docker extension</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"microservices/4.react-app/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create the website using React JS.</p> <p></p>"},{"location":"microservices/4.react-app/#step-1-install-nodejs-and-npm","title":"Step-1: Install Node.js and NPM","text":"<p>Before you can create a React app, you'll need to install Node.js and NPM (Node Package Manager) on your system. You can download the latest version of Node.js and NPM from the official website: https://nodejs.org/en/download/</p> <p>Manual install</p> <p>Click on the Installer as per your Operating system preference to install Node.js &amp; NPM both in your system.</p> <p>install using commands</p> <ul> <li>Windows OS</li> </ul> <p>Install Node.js &amp; NPM using Chocolatey (choco) for windows users, assuming you already installed choco in your system, run these commands as administrator from command prompt</p> <pre><code># This command will download and install the latest version of Node.js. \nchoco install nodejs\n\n# install a specific version\nchoco install nodejs --version=14.17.6\n</code></pre> <ul> <li>Mac OS</li> </ul> <p>Install Node.js &amp; NPM using Homebrew for Mac users, assuming you already installed Homebrew in your system:</p> <pre><code># install latest version\nbrew install node\n# install a specific version\nbrew install node@14\n</code></pre> <p>verify that Node.js is installed correctly</p> <p><pre><code>node --version\nnpm version\n</code></pre> output</p> <p><pre><code>v19.8.1\n</code></pre> verify that npm version is installed correctly</p> <p><pre><code>npm version\n</code></pre> output <pre><code>{\n  npm: '9.5.1',\n  node: '19.8.1',\n  acorn: '8.8.2',\n  ada: '1.0.4',\n  ares: '1.19.0',\n  brotli: '1.0.9',\n  cldr: '42.0',\n  icu: '72.1',\n  llhttp: '8.1.0',\n  modules: '111',\n  napi: '8',\n  nghttp2: '1.52.0',\n  nghttp3: '0.7.0',\n  ngtcp2: '0.8.1',\n  openssl: '3.0.8+quic',\n  simdutf: '3.2.2',\n  tz: '2022g',\n  undici: '5.21.0',\n  unicode: '15.0',\n  uv: '1.44.2',\n  uvwasi: '0.0.16',\n  v8: '10.8.168.25-node.12',\n  zlib: '1.2.13'\n}\n</code></pre></p>"},{"location":"microservices/4.react-app/#step-2-create-a-new-react-js-application","title":"Step-2: Create a new React JS application","text":"<p>Once you have Node.js installed, you can create a new React JS application using the <code>create-react-app</code> command. Open a terminal window and run the following command:</p> <p><pre><code>npx create-react-app react-app\n</code></pre> Wait for few mins for completing the installation.</p> <p></p> <p>This command will create a new React JS application with all the necessary files and directories in a folder named <code>react-app</code> in your current directory.</p> <p>output</p> <pre><code>Need to install the following packages:\n  create-react-app@5.0.1\nOk to proceed? (y) Y\nnpm WARN deprecated tar@2.2.2: This version of tar is no longer supported, and will not receive security updates. Please upgrade asap.\n\nCreating a new React app in C:\\Source\\Repos\\Microservices\\react-app.\nInstalling packages. This might take a couple of minutes.\nInstalling react, react-dom, and react-scripts with cra-template...\n\nadded 1419 packages in 2m\n.\n.\n.\n</code></pre> <p>If you closely look at the output produced by installer, it has some details to get start by React JS application.</p> <p>Run the Application</p> <pre><code>cd react-app\nnpm start\n</code></pre> <p>output</p> <pre><code>Compiled successfully!\n\nYou can now view react-app in the browser.\n\n  Local:            http://localhost:3000\n  On Your Network:  http://172.21.128.1:3000\n\nNote that the development build is not optimized.\nTo create a production build, use npm run build.\n\nwebpack compiled successfully\n</code></pre> <p></p>"},{"location":"microservices/4.react-app/#step-3-add-dockerfiles-to-the-mvc-project","title":"Step-3: Add Dockerfiles to the MVC project","text":"<p>Create a Dockerfile in the root directory of your project and copy following code. Dockerfile will provide instructions for building a container image of our React JS Website.</p> <pre><code># pull official base image\nFROM node:13.12.0-alpine\n\n# set working directory\nWORKDIR /app\n\n# add `/app/node_modules/.bin` to $PATH\nENV PATH /app/node_modules/.bin:$PATH\n\n# install app dependencies\nCOPY package.json ./\nCOPY package-lock.json ./\nRUN npm install --silent\nRUN npm install react-scripts@3.4.1 -g --silent\n\n# add app\nCOPY . ./\n\n# start app\nCMD [\"npm\", \"start\"]\n</code></pre> <p>Note</p> <p>Read inline comments of the Dockerfile for understanding the Dockerfile instructions</p>"},{"location":"microservices/4.react-app/#step-4-docker-build-locally","title":"Step-4: Docker Build locally","text":"<p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application functions as expected.</p> <p>The <code>docker build</code> command is used to build Docker images from a Dockerfile.  </p> <pre><code>docker build -t sample/react-app:20230322.1 .\n</code></pre> <p>output</p> <pre><code>=&gt; [6/7] RUN npm install react-scripts@3.4.1 -g\n=&gt; [7/7] COPY .\n=&gt; exporting to image\n=&gt; =&gt; exporting layers\n=&gt; =&gt; writing image sha256:552d7e73a9fbecf6f51397becc9af1b69df05429b3731513f53f6e89dd8a7cab \n=&gt; =&gt; naming to docker.io/sample/react-app:20230322.\n\nUse 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n</code></pre> <p>When you run the <code>docker build</code> command, Docker looks for a Dockerfile in the specified directory (PATH) and reads the instructions in the file to build a new image. </p> <p>The Dockerfile contains a series of instructions that define how to build the image, such as copying files, running commands, and setting environment variables. </p>"},{"location":"microservices/4.react-app/#step-5-docker-run-locally","title":"Step-5: Docker Run locally","text":"<p>Run the Docker container locally to verify that the application functions correctly within a containerized environment. This step ensures that the containerized application operates as expected on your local machine.</p> <p>Run the <code>docker run</code> command to start a container based on the image:</p> <p><pre><code>docker run --rm -p 3000:3000 sample/react-app:20230322.1\n</code></pre> output</p> <p><pre><code>Compiled successfully!\n\nYou can now view react-app in the browser.\n\n  Local:            http://localhost:3000\n  On Your Network:  http://172.17.0.2:3000\n\nNote that the development build is not optimized.\nTo create a production build, use npm run build.\n\nwebpack compiled successfully\nCompiling...\nCompiled successfully!\nwebpack compiled successfully\n</code></pre> if you open the docker desktop you will notice the new image &amp; container started running.</p> <p>Image</p> <p></p> <p>Container</p> <p></p> <p>This will start the ReactJS application in the Docker container and map the container's port 3000 to your local machine's port 3000. </p> <p>Your ReactJS application is now running inside a Docker container.</p> <p>Open your favorite browser and enter the following URL to see the running application in port 3000</p> <p>http://localhost:3000/</p> <p></p> <p>You now have a basic React JS application up and running. From here, you can continue building out your application by adding more and more code as per your requirements.</p>"},{"location":"microservices/4.react-app/#step-6-push-docker-container-to-acr","title":"Step-6: Push docker container to ACR","text":"<p>Now we've Docker Containers ready locally for push to Container Registry so that we can use them in future deployment to Azure Kubernetes Services (AKS). This step prepares the container for deployment to the cloud environment.</p> <p>To publish a Docker container to Azure Container Registry (ACR), you will need to have the following:</p> <ol> <li>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI.</li> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n# Login Succeeded\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre> list all the Docker images that are available on the local system <pre><code>docker images\n</code></pre> output <pre><code>REPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\nsample/aspnet-app                                         20230312.1                                                                   587f347206bc   8 minutes ago   216MB\n.\n.\n.\n</code></pre></li> <li><code>Tag</code> your Docker container image with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command: <pre><code>docker tag sample/react-app:20230322.1 acr1dev.azurecr.io/sample/react-app:20230322.1\n</code></pre></li> <li>Push your Docker container image to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker push acr1dev.azurecr.io/sample/react-app:20230322.1\n</code></pre> Output <pre><code>The push refers to repository [acr1dev.azurecr.io/sample/react-app]\n649a035a1734: Pushed\n4061bd2dd536: Pushed\nc0257b3030b0: Pushed\n912a3b0fc587: Pushed\na36186d93e25: Pushed\na3d997b065bc: Pushed\n65d358b7de11: Pushed\nf97384e8ccbc: Pushed\nd56e5e720148: Pushed\nbeee9f30bc1f: Pushed\n20230322.1: digest: sha256:73f0669d18c6cae79beb81edc8c523191710f9ec4781d590884b46326f9ad6f9 size: 2419\n</code></pre></li> <li>Wait for the push to complete. Depending on the size of your Docker container image and the speed of your internet connection, this may take a few minutes.</li> <li>Verify the newly pushed image to ACR. <pre><code>az acr repository list --name acr1dev --output table\n</code></pre> Output <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\nsample/react-app\n</code></pre></li> <li>Show the new tags of a image in the acr <pre><code>az acr repository show-tags --name acr1dev --repository sample/react-app --output table\n</code></pre> output <pre><code>Result\n----------\n20230322.1\n</code></pre></li> </ol> <p>You've successfully pushed your Docker container image to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container images and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/5.blazor-app/","title":"Create your third containerized website using Blazor","text":""},{"location":"microservices/5.blazor-app/#introduction","title":"Introduction","text":""},{"location":"microservices/5.blazor-app/#technical-scenario","title":"Technical Scenario","text":""},{"location":"microservices/5.blazor-app/#objective","title":"Objective","text":""},{"location":"microservices/5.blazor-app/#prerequisites","title":"Prerequisites","text":""},{"location":"microservices/5.blazor-app/#task-1-under-construction","title":"Task-1: under construction","text":""},{"location":"microservices/5.blazor-app/#task-2-under-construction","title":"Task-2: under construction","text":""},{"location":"microservices/5.blazor-app/#task-3-under-construction","title":"Task-3: under construction","text":""},{"location":"microservices/6.sqlserver-db-no-docker/","title":"Create Your First Database with SQL Server","text":""},{"location":"microservices/6.sqlserver-db-no-docker/#introduction","title":"Introduction","text":"<p>SQL Server is a popular relational database management system (RDBMS) developed by Microsoft and widely used in the industry. It serves as a database server that stores and retrieves data as requested by other software applications, whether on the same computer or a remote one, using the client-server model.</p> <p>In this lab, I will explain fundamental database concepts and provide practical examples covering the creation of a SQL Server, designing databases, creating tables, inserting and retrieving data, deleting records, and finally, dropping an existing database.</p>"},{"location":"microservices/6.sqlserver-db-no-docker/#databases-concepts","title":"Databases concepts","text":"<p>Database</p> <p>A database in SQL Server is made up of a collection of tables that stores a specific set of structured data. A table contains a collection of rows, also referred to as records or tuples, and columns, also referred to as attributes. Each column in the table is designed to store a certain type of information, for example, dates, names, dollar amounts, and numbers.</p> <p>SQL (Structured Query Language)</p> <p>Structured query language (SQL) is a programming language for storing and processing information in a relational database. A relational database stores information in tabular form, with rows and columns representing different data attributes and the various relationships between the data values.</p>"},{"location":"microservices/6.sqlserver-db-no-docker/#sql-server-management-studio-ssms","title":"SQL Server Management Studio (SSMS)","text":"<p>SQL Server Management Studio (SSMS) is a powerful and comprehensive integrated environment for managing SQL Server databases, Developed by Microsoft. SSMS provides a graphical user interface (GUI) that allows database administrators, application developers, and any IT users to interact with SQL Server databases efficiently.</p> <p>Features SSMS:</p> <p>Database Object Explorer: SSMS provides a hierarchical view of all database objects, including tables, views, stored procedures, and more. This makes it easy to navigate and manage database structures.</p> <p>Query Editor: The Query Editor is a robust tool for writing and executing Transact-SQL (T-SQL) queries. It includes features like syntax highlighting, IntelliSense for code completion, and debugging tools.</p> <p>Connect: Click the \"Connect\" button to establish a connection to the SQL Server instance.</p> <p>Once connected, you can start exploring and managing the databases, execute queries, and utilize the various features of SSMS to administer and optimize your SQL Server environment</p>"},{"location":"microservices/6.sqlserver-db-no-docker/#create-azure-sql-server","title":"Create Azure SQL Server","text":"<p>In Chapter 2, we'll explore the creation of an Azure SQL Server as part of the Infrastructure as Code (IaC). However, for immediate guidance, here are step-by-step instructions for manually creating an Azure SQL Server using the Azure Portal.</p> <ul> <li>Open your web browser and go to the Azure portal.</li> <li>Sign in with your Azure account.</li> <li>In the left-hand navigation pane, click on \"Create a resource.\"</li> <li>In the \"Search\" box, type \"SQL server.\"</li> <li>Select \"SQL server\" from the results.</li> <li>In the \"SQL server\" pane, click the \"Create\" button.</li> <li>Fill in the required information:<ul> <li>Subscription: Choose your Azure subscription.</li> <li>Resource group: Create a new one or select an existing resource group.</li> <li>Server name: Choose a unique server name.</li> <li>Data source: Choose \"None\" for a new server.</li> <li>Server admin login: Enter a username for the server administrator.</li> <li>Password: Enter a strong password for the server administrator.</li> <li>Location: Choose the region where you want to deploy the server.</li> </ul> </li> <li>Configure the networking settings, including:<ul> <li>Networking rule: Allow access to Azure services and resources.</li> <li>Firewall rules: Configure the firewall rules to allow access to the server from specific IP addresses if needed.</li> </ul> </li> <li>Configure advanced settings (optional):</li> <li>You can configure additional settings like the version of SQL Server, storage, and backup options on this page. Adjust these settings according to your requirements.</li> <li>Click the \"Review + create\" button.</li> <li>Click the \"Create\" button to start the deployment.</li> </ul>"},{"location":"microservices/6.sqlserver-db-no-docker/#creating-your-first-database","title":"Creating your first database","text":"<p>Follow these step-by-step instruction to create your first sql server database.</p> <p>create database using SSMS:</p> <ol> <li>Open SQL Server Management Studio (SSMS).</li> <li>Connect to your SQL Server instance.</li> <li>In Object Explorer, right-click \"Databases\" and select \"New Database.\"</li> <li>Name it \"database1\" and configure optional settings.</li> <li>Click \"OK\" to create the database.</li> <li>Verify its creation in Object Explorer under \"Databases.\"</li> </ol> <p>create database using SQL query:</p> <pre><code>CREATE DATABASE database1;\n</code></pre>"},{"location":"microservices/6.sqlserver-db-no-docker/#creating-tables","title":"Creating Tables","text":"<p>Here I am going to show you a sample table called <code>Article</code>.</p> <p>The purpose of the <code>Article</code> table is to store information related to articles. This table is designed to capture details about individual articles and their associated metadata such as content display, publication tracking, modification history etc...</p> <p>Now let's create an <code>Article</code> table within the database1 <pre><code>CREATE TABLE [dbo].[Article]\n(\n    [Id]                INT                 IDENTITY (1, 1) NOT NULL PRIMARY KEY,   \n    [Title]             NVARCHAR (100)      NOT NULL,   -- Title shown on card\n    [Description]       NVARCHAR (256)      NOT NULL,   -- Description shown on card and article page   \n    [CoverImage]        NVARCHAR (100)      NOT NULL,   -- Image unique key after uploaded on blob\n    [Content]           NVARCHAR (MAX)      NOT NULL,   -- Article actual content\n    [PublishedDate]     DATETIME2           NOT NULL DEFAULT GETDATE(), -- Date on which article will be published on app\n    [IsDeleted]         BIT                 NOT NULL DEFAULT 0, -- Flag for hiding the article on the page.\n    [ModifiedBy]        NVARCHAR(100)       NOT NULL DEFAULT 'System',  -- Who modified the contents\n    [ModifiedDate]      DATETIME2           NOT NULL DEFAULT GETDATE(), -- Audit purpose\n    CONSTRAINT FK_ModifiedBy FOREIGN KEY (ModifiedBy) REFERENCES [dbo].[Users] (UserName) -- Example of a foreign key\n);\n</code></pre> Primary Key:</p> <p>The <code>Id</code> column is defined as the primary key with the PRIMARY KEY constraint. This ensures each row has a unique identifier.</p> <p>Foreign Key:</p> <p>A foreign key (FK_ModifiedBy) is an example linking the ModifiedBy column to a reference table (Users). This is commonly used to maintain referential integrity between tables.</p>"},{"location":"microservices/6.sqlserver-db-no-docker/#inserting-data","title":"Inserting Data","text":"<p>To insert data into the <code>Article</code> table you've created, you can use the <code>INSERT INTO</code> statement. Here are sample SQL queries to demonstrate the insertion of records:</p> <pre><code>-- Inserting a basic article without modification information\nINSERT INTO [dbo].[Article] ([Title], [Description], [CoverImage], [Content], [PublishedDate])\nVALUES ('Introduction to SQL', 'A brief overview of SQL', 'sql_intro.jpg', 'This is the content of the SQL introduction article.', GETDATE());\n\n-- Inserting an article with modification information\nINSERT INTO [dbo].[Article] ([Title], [Description], [CoverImage], [Content], [PublishedDate], [IsDeleted], [ModifiedBy], [ModifiedDate])\nVALUES ('Advanced SQL Techniques', 'Exploring advanced SQL concepts', 'advanced_sql.jpg', 'In-depth exploration of advanced SQL techniques.', GETDATE(), 0, 'AdminUser', GETDATE());\n\n-- Inserting an article with deletion flag\nINSERT INTO [dbo].[Article] ([Title], [Description], [CoverImage], [Content], [PublishedDate], [IsDeleted], [ModifiedBy], [ModifiedDate])\nVALUES ('Hidden Article', 'This article is hidden', 'hidden_article.jpg', 'Content of the hidden article.', GETDATE(), 1, 'EditorUser', GETDATE());\n</code></pre>"},{"location":"microservices/6.sqlserver-db-no-docker/#retrieving-data","title":"Retrieving Data","text":"<p>The <code>SELECT</code> statement is used to retrieve data from one or more tables in a database. It allows you to specify the columns you want to retrieve, filter the rows based on specific conditions, sort the results, and perform various other operations. </p> <p>Basic <code>SELECT</code> Statement:</p> <pre><code>SELECT column1, column2, ...\nFROM table_name;\n</code></pre> <p>To retrieve all columns from the <code>Article</code> table:</p> <pre><code>SELECT *\nFROM [dbo].[Article];\n</code></pre> <p>To retrieve specific columns, such as <code>Title</code> and <code>PublishedDate</code>:</p> <pre><code>SELECT [Title], [PublishedDate]\nFROM [dbo].[Article];\n</code></pre> <p>To filter rows based on a condition, you use the <code>WHERE</code> clause. For example, retrieving articles where the <code>IsDeleted</code> flag is set to 0:</p> <pre><code>SELECT [Title], [Description], [PublishedDate]\nFROM [dbo].[Article]\nWHERE [IsDeleted] = 0;\n</code></pre> <p>To sort the results in ascending or descending order, you use the <code>ORDER BY</code> clause. For example, retrieving articles sorted by <code>PublishedDate</code> in descending order:</p> <pre><code>SELECT [Title], [Description], [PublishedDate]\nFROM [dbo].[Article]\nORDER BY [PublishedDate] DESC;\n</code></pre> <p>You can combine filtering and sorting in a single query. For example, retrieving titles and descriptions of non-deleted articles sorted by <code>PublishedDate</code>:</p> <pre><code>SELECT [Title], [Description], [PublishedDate]\nFROM [dbo].[Article]\nWHERE [IsDeleted] = 0\nORDER BY [PublishedDate] DESC;\n</code></pre>"},{"location":"microservices/6.sqlserver-db-no-docker/#deleting-data","title":"Deleting Data","text":"<p>To delete data from a table, you use the <code>DELETE</code> statement. Here's the basic syntax:</p> <pre><code>DELETE FROM table_name\nWHERE condition;\n</code></pre> <p>For example, to delete an article with a specific title:</p> <pre><code>DELETE FROM [dbo].[Article]\nWHERE [Title] = 'Introduction to SQL';\n</code></pre> <p>Deleting All Data</p> <p>To <code>delete all</code> data from a table without deleting the table structure itself, you can use the following query:</p> <pre><code>DELETE FROM [dbo].[Article];\n</code></pre>"},{"location":"microservices/6.sqlserver-db-no-docker/#dropping-a-database","title":"Dropping a Database","text":"<p>Dropping a database removes the entire database and all its objects (tables, views, stored procedures, etc.).</p> <p>The syntax for dropping a database is as follows:</p> <pre><code>DROP DATABASE database_name;\n</code></pre> <p>For example, to drop the \"database1\" database:</p> <pre><code>DROP DATABASE database1;\n</code></pre> <p>Before dropping a database, it is necessary to ensure that there are no active connections to the database. </p>"},{"location":"microservices/6.sqlserver-db-no-docker/#reference","title":"Reference","text":"<ul> <li>Microsoft MSDN - Create a database</li> </ul>"},{"location":"microservices/6.sqlserver-db/","title":"Setting up SQL Server database in a Docker Container","text":""},{"location":"microservices/6.sqlserver-db/#introduction","title":"Introduction","text":"<p>Running <code>SQL Server</code> in a Docker container is best suited for development, testing, learning, or quick experimentation scenarios where you need a lightweight, isolated environment that can be spun up or torn down easily. It works well offline, and your can run it without any cost, and is perfect for local development workflows. However, it's not ideal for production use. for production environments you can use Azure SQL Database which is a fully managed platform-as-a-service (PaaS) offering that's optimized for production workloads. It provides built-in high availability, automated backups, scalability, and security, making it ideal for applications that need global reach, minimal maintenance, and enterprise-grade reliability. </p> <p>In this lab, I will guide you through the process of creating Docker container for SQL Server database and run SQL Server database in the docker, and finally accessing the SQL Server database using SQL Server Management Studio (<code>SSMS</code>) and <code>Azure Data Studio</code> tools.</p>"},{"location":"microservices/6.sqlserver-db/#objective","title":"Objective","text":"<p>The objective is to establish a local development environment for the SQL Server database. To accomplish this, you will create a Dockerfile file, run them locally. All of these tasks we are doing here will be useful in later chapters when deploying to the Azure Kubernetes Service (AKS).</p> <p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup Git Repository for SQL Server database.</li> <li>Step-2: Create Folder Structure for SQL Server database.</li> <li>Step-3: Add Dockerfiles to the Database Project</li> <li>Step-3.1: Docker Build Locally</li> <li>Step-3.2: Docker Run Locally</li> </ul> <ul> <li>Step-4: Test the SQL Server database connection using SSMS</li> <li>Step-5: Test the SQL Server database connection using Azure Data Studio</li> <li>Step-6: Push Docker Container to ACR</li> </ul> <p>By the end of this lab, you will have a SQL Server database running in a Docker container, managed through Azure DevOps, and ready for use in your development and production environments.</p>"},{"location":"microservices/6.sqlserver-db/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Docker Desktop: - Docker Downloads.</li> <li>Docker compose installed</li> <li>SQL Server Management Studio installed - this will allow you to manage the SQL Server databases</li> <li>Azure Data Studio installed - this will allow you to connect to SQL server databases</li> <li>Basic understanding of Docker and SQL Server.</li> <li>Access to an Azure Container Registry (ACR).</li> </ul> <p>Verify the docker installation by running following commands:</p> <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre> <p>Verify the docker compose by running following commands:</p> <pre><code>docker-compose version\n</code></pre>"},{"location":"microservices/6.sqlserver-db/#step-1-setup-git-repository-for-sql-server-database","title":"Step-1: Setup Git Repository for SQL Server database","text":"<p>Setting up a Git repository for your SQL Server database project allows you to manage your code effectively, work in teams, and track the changes of your database codebase.</p> <ul> <li>Create a new project in Azure DevOps for your database-related work.</li> <li>Create a repository within the project to store your database scripts and Dockerfiles.</li> </ul> <p>For example to clone an existing repository, run the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/6.sqlserver-db/#step-2-create-folder-structure-for-sql-server-database","title":"Step-2: Create Folder Structure for SQL Server database","text":"<p>In this step, we'll create a dedicated project or folder for our SQL Server database</p> <p>Create a new database project:</p> <p>Inside our Git repository, create a new directory or folder specifically for your SQL Server database. This folder will contain all the necessary files for SQL Server database, including databaseschema scripts, sample data scripts, docker compose &amp; Dockerfile and other sql files.</p> <p>Here's a suggestion for a folder structure for a SQL Server database project:</p> <pre><code>your-project-name/\n\u2502\n\u251c\u2500\u2500 sql/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 schema/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tables/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 table1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 table2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 views/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 view1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 view2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 functions/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 function1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 function2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 procedures/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 procedure1.sql\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 procedure2.sql\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 data/\n\u2502   \u2502       \u251c\u2500\u2500 seed_data.sql\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 migrations/\n\u2502       \u251c\u2500\u2500 version1/\n\u2502       \u2502   \u251c\u2500\u2500 up.sql\n\u2502       \u2502   \u2514\u2500\u2500 down.sql\n\u2502       \u251c\u2500\u2500 version2/\n\u2502       \u2502   \u251c\u2500\u2500 up.sql\n\u2502       \u2502   \u2514\u2500\u2500 down.sql\n\u2502       \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Explanation:</p> <ul> <li><code>sql/</code>: This folder contains all SQL-related files for your project.</li> <li><code>scripts/</code>: Contains scripts for creating database objects like tables, views, functions, and stored procedures.<ul> <li><code>schema/</code>: Contains subfolders for different types of database objects.</li> <li><code>tables/</code>, <code>views/</code>, <code>functions/</code>, <code>procedures/</code>: Each of these folders contains SQL scripts for the respective database objects.</li> </ul> </li> <li><code>data/</code>: Contains data scripts such as seed data.</li> <li><code>migrations/</code>: Contains SQL migration scripts for managing database schema changes over time. Each migration version should have an <code>up.sql</code> script for applying the migration and a <code>down.sql</code> script for reverting it.</li> <li><code>Dockerfile</code>: The Dockerfile for building a Docker image for your SQL Server database.</li> <li><code>README.md</code>: Documentation for your project.</li> </ul> <p>You can adjust this structure based on the specific needs of your project. For instance, if you have additional folders or files, you can add them accordingly.</p> <p></p>"},{"location":"microservices/6.sqlserver-db/#step-3-add-dockerfiles-to-the-database-project","title":"Step-3: Add Dockerfiles to the Database Project","text":"<p>To build a Docker image for SQL Server, create a Dockerfile in your project's root directory:</p> <pre><code># Use the official SQL Server 2019 image from Microsoft\nFROM mcr.microsoft.com/mssql/server:2019-latest\n\n# Set the environment variables for SQL Server\nENV ACCEPT_EULA=Y\nENV SA_PASSWORD=Strong@Passw0rd\n# ENV MSSQL_PID=Developer\n# ENV MSSQL_TCP_PORT=1433\n\n# Create a directory inside the container to copy your SQL scripts\nWORKDIR /src\n\n# Copy your SQL scripts into the container [optional]\nCOPY scripts.sql ./scripts.sql\n\n# Set permissions for the SQL scripts\n# RUN chmod +x ./scripts.sql\n\n# RUN SQL SERVER and Access SQL CLI on localhost with given credentials\n# Then run SQL Script - scripts.sql\nRUN (/opt/mssql/bin/sqlservr --accept-eula &amp; ) | grep -q \"Service Broker manager has started\" &amp;&amp;  /opt/mssql-tools/bin/sqlcmd -S127.0.0.1 -Usa -PStrong@Passw0rd -i scripts.sql\n</code></pre> <p>In this Dockerfile:</p> <ul> <li>We start with the official SQL Server 2019 image provided by Microsoft.</li> <li>Set environment variables <code>ACCEPT_EULA</code> to 'Y' and <code>SA_PASSWORD</code> to the desired strong password for the 'sa' account.</li> <li>Create a directory inside the container to copy your SQL scripts (<code>/src</code> in this case).</li> <li>Copy your SQL scripts into the container (assuming you have them in the same directory as your Dockerfile).</li> <li>Set permissions for the SQL scripts (if needed).</li> <li>Finally, specify the command to start SQL Server when the container starts.</li> </ul> <p>You would replace <code>\"./scripts.sql\"</code> with the path to your actual SQL script file.</p> <pre><code>USE master;\nGO\n\n-- Create SampleDB\nCREATE DATABASE SampleDB;\nGO\n\nUSE SampleDB;\nGO\n\n-- Create Users table\nCREATE TABLE Users (\n    UserID INT PRIMARY KEY,\n    Username NVARCHAR(50),\n    Email NVARCHAR(100)\n);\nGO\n\n-- Insert some sample data into Users table\nINSERT INTO Users (UserID, Username, Email) VALUES (1, 'user1', 'user1@example.com');\nINSERT INTO Users (UserID, Username, Email) VALUES (2, 'user2', 'user2@example.com');\nINSERT INTO Users (UserID, Username, Email) VALUES (3, 'user3', 'user3@example.com');\nGO\n</code></pre> <p>Step-3.1: Docker Build Locally</p> <p>To build the Docker image, navigate to the directory containing the Dockerfile and your SQL script, then run:</p> <pre><code>docker build -t my-sqlserver-image .\n</code></pre> <p>Docker desktop &gt; Image</p> <p></p> <p>Step-3.2: Docker Run Locally</p> <p>To run your SQL Server container locally for testing and development, use the following command:</p> <pre><code>docker run -d --name my-sqlserver-container -p 5432:5432 my-sqlserver-image\n</code></pre> <p>This command creates a container named <code>my-sqlserver-container</code> and maps port 5432 from the container to the host.</p> <p>Docker desktop &gt; Container</p> <p></p>"},{"location":"microservices/6.sqlserver-db/#step-4-test-the-sql-server-database-connection-using-ssms","title":"Step-4: Test the SQL Server database connection using SSMS","text":"<p>Testing the SQL Server database connection using SQL Server Management Studio (SSMS) ensures that the database server is accessible and that users can connect to it successfully.</p> <p>Launch SQL Server Management Studio (SSMS) and provide the necessary credentials to connect to the SQL Server instance.</p> <p>SSMS &gt; Login Page</p> <p></p> <p>SSMS &gt; After Login</p> <p></p>"},{"location":"microservices/6.sqlserver-db/#step-5-test-the-sql-server-database-connection-using-azure-data-studio","title":"Step-5: Test the SQL server database connection using Azure Data Studio","text":"<p>Azure Data Studio is a cross-platform database tool that offers features similar to SQL Server Management Studio (SSMS) but with additional support for Azure services and extensions.</p> <p>Launch Azure Data Studio and provide the necessary credentials to connect to the SQL Server instance.</p> <p>SSMS &gt; Login Page</p> <p></p> <p>SSMS &gt; After Login</p> <p></p>"},{"location":"microservices/6.sqlserver-db/#step-6-push-docker-container-to-acr","title":"Step-6: Push Docker Container to ACR","text":"<p>Push your SQL Server container image to Azure Container Registry (ACR) for use in AKS. Follow these steps:</p> <p>Log in to your Azure account using the Azure CLI:</p> <pre><code>az login\n</code></pre> <p>Authenticate to your ACR:</p> <pre><code>az acr login --name myacr\n</code></pre> <p>Replace <code>myacr</code> with your ACR name.</p> <p>Tag your local Docker image with the ACR login server:</p> <pre><code>docker tag my-sqlserver-image myacr.azurecr.io/my-sqlserver-image:v1\n</code></pre> <p>Push the Docker image to ACR:</p> <pre><code>docker push myacr.azurecr.io/my-sqlserver-image:v1\n</code></pre> <p>Replace <code>myacr</code> and <code>v1</code> with your ACR name and desired image version.</p> <p>Now, your SQL Server container image is stored in Azure Container Registry and can be easily pulled and deployed from AKS to Azure Database for SQL Server - Flexible Server.</p>"},{"location":"microservices/6.sqlserver-db/#conclusion","title":"Conclusion","text":"<p>You have successfully created a Docker container for SQL Server database, container created as part of this task will be used in the future labs in AKS.</p>"},{"location":"microservices/6.sqlserver-db/#references","title":"References","text":"<ul> <li>Quickstart: Run SQL Server Linux container images with Docker</li> </ul>"},{"location":"microservices/7.postgresql-db-execute-script/","title":"Create Your Second Database with PostgreSQL","text":""},{"location":"microservices/7.postgresql-db-execute-script/#introduction","title":"Introduction","text":"<p>Welcome to the new lab in our Microservices chapter! In this session, I will guide you through the creation of a simple PostgreSQL database. Specifically, we will explore how to create a Docker container designed for executing PostgreSQL database scripts within the PostgreSQL Flexible Server\u2014a PaaS managed by Microsoft Azure. This container will be triggered as a job in Azure Kubernetes Service (AKS) and automatically deleted after execution.</p>"},{"location":"microservices/7.postgresql-db-execute-script/#technical-scenario","title":"Technical Scenario","text":"<p>As a <code>Database Administrator</code> (DBA), you have been tasked with creating a Docker container using a Dockerfile and pushing the container to Azure Container Registry (ACR). Subsequently, this container will be utilized as an AKS job to execute SQL scripts stored in a designated folder. The job will be automatically deleted post-execution, seamlessly integrated into the deployment workflow using Helm charts.</p> <p>The containerized database, once prepared, will be pushed to the Azure Container Registry (ACR).</p> <p>The key objective of this lab is to prepare a database for deployment on PostgreSQL Flexible Database Server through AKS. By completing this lab, you will gain a foundational understanding of how a database can be created, automated through scripts, containerized, seamlessly integrated into a Kubernetes environment, and ultimately executed within the PostgreSQL server.</p>"},{"location":"microservices/7.postgresql-db-execute-script/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup repository for databases in Azure DevOps.</li> <li>Step-2: Create folder structure for database scripts</li> <li>Step-3: Add Dockerfiles to the database project.</li> <li>Step-4: Docker build locally.</li> <li>Step-5: Docker run locally.</li> <li>Step-6: Publish the Docker container to ACR.</li> </ul>"},{"location":"microservices/7.postgresql-db-execute-script/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Docker Installed:  - Docker Downloads.</li> <li>Git Client tool:  - Git Downloads.</li> <li>Azure DevOps and Git Repository: Initialize a Git repository for your PostgreSQL database</li> <li>Azure Container Registry (ACR)</li> </ul>"},{"location":"microservices/7.postgresql-db-execute-script/#prerequisites_1","title":"Prerequisites","text":"<p>You'll need to ensure that your system has the necessary prerequisites. Here's a checklist of prerequisites:</p> <ul> <li><code>Docker</code>:  - Docker Downloads.</li> <li><code>Git Client</code>:  - Git Downloads.</li> <li><code>Git Repository</code>: Initialize a Git repository for your Node.js application.</li> </ul>"},{"location":"microservices/7.postgresql-db-execute-script/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create container for executing PostgreSQL database scripts.</p> <p></p>"},{"location":"microservices/7.postgresql-db-execute-script/#step-1-setup-repository-for-databases-in-azure-devops","title":"Step-1: Setup repository for databases in Azure DevOps.","text":"<p>For this project, you can either leverage an existing Git repository created in our first chapter or initiate a new one.</p> <p>To clone an existing repository, execute the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/7.postgresql-db-execute-script/#step-2-create-folder-structure-for-database-scripts","title":"Step-2: Create folder structure for database scripts","text":"<p>In this step, we'll create and organize the folder structure for managing PostgreSQL database scripts. first we will start with creating a parent folder named <code>postgresql-db</code>, serving as the project directory for all PostgreSQL database-related files and folders.</p> <p>Folder Structure:</p> <ol> <li> <p>migrations: folder</p> <ul> <li>This is the first folder which houses all PostgreSQL script files related to database migrations. Adhering to a strict naming convention is crucial to avoid duplicate runs.</li> <li>Some files with naming convention:<ul> <li><code>1.0.0.0_migration.psql</code> (Initial version)</li> <li><code>1.0.0.1_migration.psql</code> (Minor changes)</li> <li><code>1.0.0.2_migration.psql</code> (Subsequent changes)</li> </ul> </li> </ul> <p>Let's looks into some sample scripts placed in each file:</p> </li> </ol> 1.0.0.0_migration.psql<pre><code>-- Table: public.table1\n\nCREATE TABLE IF NOT EXISTS public.table1 (\n    \"table1ID\" uuid NOT NULL,\n    \"table1Name\" character varying(50) COLLATE pg_catalog.\"default\" NOT NULL,\n    \"Features\" json,\n    \"CreatedDate\" timestamp with time zone NOT NULL DEFAULT now(),\n    \"UpdatedDate\" timestamp with time zone NOT NULL DEFAULT now(),\n    \"ProvisioningType\" smallint NOT NULL,\n    \"SubscriptionType\" smallint NOT NULL,\n    \"Provisioned\" boolean NOT NULL DEFAULT false,\n    CONSTRAINT table1_pkey PRIMARY KEY (\"table1ID\")\n);\n\n-- Additional Tables and Views...\n\nCREATE OR REPLACE VIEW schema_version\nAS\nSELECT '1.0.0.1'::text AS version;\n</code></pre> <p>The <code>schema_version</code> view update at the end of every script is mandatory to prevent duplicate runs.</p> 1.0.0.1_migration.psql<pre><code>-- Extending the schema and inserting data into the table created in the previous script\n\nINSERT INTO public.feature(\"FeatureName\", \"IsMandatory\", \"CreatedDate\", \"UpdatedDate\")\nVALUES\n  ('Feature1', false, now(), now()),\n  ('Feature2', false, now(), now()),\n  -- Additional Data...\n\nCREATE OR REPLACE VIEW schema_version\nAS\nSELECT '1.0.0.2'::text AS version;\n</code></pre> <p>Continue adding new script files to extend the database scripts.</p> <ol> <li> <p>sample_data: folder</p> <ul> <li> <p>This is the second folder which contains scripts for inserting sample data into the database, specifically for testing purposes in development and test environments.</p> </li> <li> <p>files in this folder:</p> <ul> <li><code>1.0.0_sample_data_table1.psql</code></li> <li><code>1.0.1_sample_data_table2.psql</code></li> <li><code>1.0.2_sample_data_table3.psql</code></li> </ul> </li> </ul> </li> <li> <p>create_schema.psql: file</p> <ul> <li>This file encapsulates the database shcema (DDL) related script.</li> </ul> </li> <li> <p>Dockerfile: file</p> <ul> <li>The Dockerfile contains instructions related to the database container. Further details will be discussed in the subsequent task.</li> </ul> </li> <li> <p>run_migrations.sh: file</p> <ul> <li> <p>This crucial shell script plays a significant role in executing database scripts. It connects to the database, passing necessary parameters during AKS job execution.</p> </li> <li> <p>this shell script streamlines the execution of PostgreSQL migration scripts within a Docker container. It handles authentication, schema version checking, initialization, and the iterative execution of migration scripts, ensuring a seamless and automated database schema evolution process during an AKS job execution.</p> </li> <li> <p>The purpose of this shell script is to automate the execution of PostgreSQL database migration scripts within a Docker container. Let's break down its functionalities:</p> </li> </ul> <pre><code>#!/bin/sh\n\n# Set up PostgreSQL connection parameters in .pgpass file\necho \"${POSTGRES_HOST}\":*:*:\"${POSTGRES_USER}\":\"${POSTGRES_PASSWORD}\" &gt; ~/.pgpass\nchmod 0600 ~/.pgpass\n\n# Change working directory to /migrations where migration scripts are located\ncd /migrations\n\n# Retrieve the current schema version from the database\nversion=$(psql -h ${POSTGRES_HOST} -U ${POSTGRES_USER} -w ${POSTGRES_DB} -t -c \"select version from schema_version\" | xargs) 2&gt; /dev/null\n\n# Check if schema version exists or if the query fails\nif [ $? -ne 0 ] || [ \"$version\" == \"\" ]\nthen\n    # If no version found, run the initial schema creation script\n    echo \"Running create_schema.psql\"\n    psql -h ${POSTGRES_HOST} -U ${POSTGRES_USER} -w ${POSTGRES_DB} -f \"create_schema.psql\"\nelse\n    # If version exists, echo the existing schema version\n    echo \"Schema version found: $version\"\nfi\n\n# Loop to test and execute upgrade scripts\nwhile : \ndo\n    # Retrieve the current schema version in the loop\n    version=$(psql -h ${POSTGRES_HOST} -U ${POSTGRES_USER} -w ${POSTGRES_DB} -t -c \"select version from schema_version\" | xargs)\n\n    # Echo the current schema version and test for an upgrade script\n    echo \"Schema version found: $version; testing for upgrade script ${version}_migration.psql\"\n\n    # Execute the upgrade script if found\n    psql -h ${POSTGRES_HOST} -U ${POSTGRES_USER} -w ${POSTGRES_DB} -f \"${version}_migration.psql\"\n\n    # Check if the execution of the upgrade script fails\n    if [ $? -ne 0 ]\n    then\n        break\n    else\n        # If upgrade script executed successfully, echo a success message\n        echo \"${version}_migration.psql run\"\n    fi\ndone\n\n# Echo the final schema version\necho \"Schema version is: ${version}\"\n\n# Exit with a status of 0 (success)\nexit 0\n</code></pre> </li> </ol> <p>Detailed explanations for each segment of the script will be explained here:</p> <ol> <li> <p><code>.pgpass</code> File Creation:</p> <ul> <li>The script starts by creating a <code>.pgpass</code> file in the user's home directory, which contains the PostgreSQL connection parameters such as host, user, and password. This ensures secure and automated authentication during database operations.</li> </ul> </li> <li> <p>Permissions Configuration:</p> <ul> <li>The script then sets the appropriate permissions (<code>chmod 0600</code>) for the <code>.pgpass</code> file to restrict access and enhance security.</li> </ul> </li> <li> <p>Navigation to Migrations Folder:</p> <ul> <li>The script changes the working directory to <code>/migrations</code>, where the PostgreSQL migration scripts are located.</li> </ul> </li> <li> <p>Database Version Check:</p> <ul> <li>It retrieves the current schema version from the database by executing a SQL query (<code>select version from schema_version</code>). If the query fails or no version is found, it indicates that the database schema needs to be initialized.</li> </ul> </li> <li> <p>Schema Initialization or Version Retrieval:</p> <ul> <li>If no schema version is found, the script echoes a message indicating the execution of <code>create_schema.psql</code> for initializing the database schema.</li> <li>If a schema version is found, the script echoes the existing version.</li> </ul> </li> <li> <p>Migration Script Execution Loop:</p> <ul> <li>The script enters a loop where it repeatedly checks for the latest schema version and attempts to execute the corresponding migration script (<code>${version}_migration.psql</code>).</li> <li>If the execution of a migration script fails, the loop breaks, indicating that all available migration scripts have been executed.</li> </ul> </li> <li> <p>Final Output:</p> <ul> <li>The script concludes by echoing the final schema version, providing visibility into the applied database schema version.</li> </ul> </li> <li> <p>Exit Status:</p> <ul> <li>The script exits with a status of <code>0</code>, indicating successful execution.</li> </ul> </li> </ol>"},{"location":"microservices/7.postgresql-db-execute-script/#step-3-add-dockerfiles-to-the-database-project","title":"Step-3: Add Dockerfiles to the database project.","text":"<p>To containerize our database project, let's create a Dockerfile in the root directory of your project and incorporate the following code. The Dockerfile provides instructions for building a container image of our database.</p> <pre><code># Define an argument for environment (default: prod)\nARG ENV=prod\n\n# Use Alpine Linux 3.7 as the base image\nFROM alpine:3.7 AS prod\n\n# Install PostgreSQL client in the Alpine image\nRUN apk --update add postgresql-client &amp;&amp; rm -rf /var/cache/apk/*\n\n# Copy migration scripts into the /migrations/ directory in the image\nCOPY migrations/* /migrations/\n\n# Copy the run_migrations.sh script into the root directory in the image\nCOPY run_migrations.sh /\n\n# Set executable permissions for the run_migrations.sh script\nRUN chmod +x /run_migrations.sh\n\n# Define the default command to execute when the container starts\nCMD [\"sh\", \"-c\", \"/run_migrations.sh\"]\n</code></pre>"},{"location":"microservices/7.postgresql-db-execute-script/#step-4-docker-build-locally","title":"Step-4: Docker build locally","text":"<p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application functions as expected.</p> <p>The <code>docker build</code> command is used to build Docker images from a Dockerfile.  </p> <pre><code>docker build -t sample/postgresql-db:20240101.1 .\n</code></pre> <p>output</p> <pre><code>Docker build output goes here\n</code></pre> <p>When you run the <code>docker build</code> command, Docker looks for a Dockerfile in the specified directory (PATH) and reads the instructions in the file to build a new image. </p> <p>The Dockerfile contains a series of instructions that define how to build the image, such as copying files, running commands, and setting environment variables. </p>"},{"location":"microservices/7.postgresql-db-execute-script/#step-5-docker-run-locally","title":"Step-5: Docker run locally","text":"<p>Run the Docker container locally to verify that the application functions correctly within a containerized environment. This step ensures that the containerized application operates as expected on your local machine.</p> <p>Run the <code>docker run</code> command to start a container based on the image:</p> <p><pre><code>docker run --rm -p 3000:3000 sample/postgresql-db:20240101.1 .\n</code></pre> output</p> <p><pre><code>Docker run output goes here\n</code></pre> if you open the docker desktop you will notice the new image &amp; container started running.</p>"},{"location":"microservices/7.postgresql-db-execute-script/#step-6-push-docker-container-to-acr","title":"Step-6: Push docker container to ACR","text":"<p>Now that we have Docker containers ready locally, it's time to push them to the Container Registry for future deployment on Azure Kubernetes Services (AKS). This step is crucial for preparing the container for deployment in a cloud environment.</p> <p>To publish a Docker container to Azure Container Registry (ACR), you will need to have the following:</p> <ol> <li>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI.</li> <li>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n# Login Succeeded\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre> list all the Docker images that are available on the local system <pre><code>docker images\n</code></pre> output <pre><code>REPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\nsample/postgresql-db                                         20230312.1                                                                   587f347206bc   8 minutes ago   216MB\n.\n.\n.\n</code></pre></li> <li><code>Tag</code> your Docker container image with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command: <pre><code>docker tag sample/postgresql-db:20240101.1 acr1dev.azurecr.io/sample/postgresql-db:20240101.1\n</code></pre></li> <li>Push your Docker container image to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command: <pre><code>docker push acr1dev.azurecr.io/sample/postgresql-db:20240101.1\n</code></pre> Output <pre><code>The push refers to repository [acr1dev.azurecr.io/sample/postgresql-db]\n649a035a1734: Pushed\n4061bd2dd536: Pushed\nc0257b3030b0: Pushed\n912a3b0fc587: Pushed\na36186d93e25: Pushed\na3d997b065bc: Pushed\n65d358b7de11: Pushed\nf97384e8ccbc: Pushed\nd56e5e720148: Pushed\nbeee9f30bc1f: Pushed\n20240101.1: digest: sha256:73f0669d18c6cae79beb81edc8c523191710f9ec4781d590884b46326f9ad6f9 size: 2419\n</code></pre></li> <li>Wait for the push to complete. Depending on the size of your Docker container image and the speed of your internet connection, this may take a few minutes.</li> <li>Verify the newly pushed image to ACR. <pre><code>az acr repository list --name acr1dev --output table\n</code></pre> Output <pre><code>Result\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\nsample/postgresql-db\n</code></pre></li> <li>Show the new tags of a image in the acr <pre><code>az acr repository show-tags --name acr1dev --repository sample/postgresql-db --output table\n</code></pre> output <pre><code>Result\n----------\n20240101.1\n</code></pre></li> </ol> <p>You've successfully pushed your Docker container image to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container images and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/7.postgresql-db-execute-script/#conclusion","title":"Conclusion","text":"<p>You have successfully created a Docker container for executing PostgreSQL database scripts, allowing for efficient management and deployment of your database schema. container created as part of this task will be used in the future labs in AKS to exeucte these scripts in PostgreSQL flexible server.</p>"},{"location":"microservices/7.postgresql-db/","title":"Setting up PostgreSQL database in a Docker Container","text":""},{"location":"microservices/7.postgresql-db/#introduction","title":"Introduction","text":"<p>PostgreSQL, also known as <code>Postgres</code> is a powerful, open-source relational database management system (RDBMS). PostgreSQL has gained popularity due to its advanced features and capabilities. PostgreSQL is a reliable choice for a wide range of applications, from small projects to large-scale enterprise systems. Its open-source nature, strong adherence to standards, and extensive feature set making it a popular database solution in the application development.</p> <p>Running <code>PostgresSQL</code> in a Docker container is best suited for development, testing, learning, or quick experimentation scenarios where you need a lightweight, isolated environment that can be spun up or torn down easily,  it's not ideal for production use. For production environments you can use <code>azure database for postgresql flexible server</code> which is a fully managed platform-as-a-service (PaaS).</p> <p>In this lab, I will guide you through the process of creating Docker container for PostgreSQL database and run PostgreSQL database in the docker, and finally accessing the PostgreSQL database from <code>pgadmin</code> and <code>psql</code> command line tools.</p>"},{"location":"microservices/7.postgresql-db/#objective","title":"Objective","text":"<p>The objective is to establish a local development environment for the PostgreSQL database. To accomplish this, you will create a docker compose file, run them locally. All of these tasks we are doing here will be useful in later chapters when deploying to the Azure Kubernetes Service (AKS).</p> <p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup Git Repository for PostgreSQL database.</li> <li>Step-2: Create Folder Structure for PostgreSQL database.</li> <li>Step-3: Add Dockerfiles to the Database Project</li> <li>Step-3.1: Docker Build Locally</li> <li>Step-3.2: Docker Run Locally</li> <li>Step-4: Create Docker Compose file</li> <li>Step-4.1: Build PostgreSQL database locally.</li> <li>Step-4.2: Run PostgreSQL database Container locally.</li> <li>Step-5: Test the PostgreSQL database connection from <code>psql</code> tool</li> <li>Step-6: Test the PostgreSQL database from <code>pgadmin4</code> tool</li> <li>Step-7: Push Docker Container to ACR</li> </ul> <p>By the end of this lab, you will have a PostgreSQL database running in a Docker container, managed through Azure DevOps, and ready for use in your development and production environments.</p>"},{"location":"microservices/7.postgresql-db/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Docker Desktop: - Docker Downloads.</li> <li>Docker compose installed</li> <li>Git Client tool: - Git Downloads.</li> <li>PostgreSQL installed - this will allow you to run <code>psql</code> command line tool</li> <li>Basic understanding of Docker and PostgreSQL.</li> <li>Access to an Azure Container Registry (ACR).</li> </ul> <p>Verify the docker installation by running following commands:</p> <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre> <p>Verify the docker compose by running following commands:</p> <pre><code>docker-compose version\n</code></pre>"},{"location":"microservices/7.postgresql-db/#step-1-setup-git-repository-for-postgresql-database","title":"Step-1: Setup Git Repository for PostgreSQL database","text":"<p>Setting up a Git repository for your PostgreSQL database project allows you to manage your code effectively, work in teams, and track the changes of your database codebase.</p> <ul> <li>Create a new project in Azure DevOps for your database-related work.</li> <li>Create a repository within the project to store your database scripts and Dockerfiles.</li> </ul> <p>For example to clone an existing repository, run the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/7.postgresql-db/#step-2-create-folder-structure-for-postgresql-database","title":"Step-2: Create Folder Structure for PostgreSQL database","text":"<p>In this step, we'll create a dedicated project or folder for our PostgreSQL database</p> <p>Create a new project:</p> <p>Inside our Git repository, create a new directory or folder specifically for your PostgreSQL database. This folder will contain all the necessary files for PostgreSQL database, including database scripts, docker compose &amp; Dockerfile and other psql files.</p> <p>Here's a suggested folder structure for a PostgreSQL database project:</p> <pre><code>your-project-name/\n\u2502\n\u251c\u2500\u2500 psql/\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u251c\u2500\u2500 schema/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 tables/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 table1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 table2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 views/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 view1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 view2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 functions/\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 function1.sql\n\u2502   \u2502   \u2502   \u2502   \u251c\u2500\u2500 function2.sql\n\u2502   \u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 procedures/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 procedure1.sql\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 procedure2.sql\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 data/\n\u2502   \u2502       \u251c\u2500\u2500 seed_data.sql\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 migrations/\n\u2502       \u251c\u2500\u2500 version1/\n\u2502       \u2502   \u251c\u2500\u2500 up.sql\n\u2502       \u2502   \u2514\u2500\u2500 down.sql\n\u2502       \u251c\u2500\u2500 version2/\n\u2502       \u2502   \u251c\u2500\u2500 up.sql\n\u2502       \u2502   \u2514\u2500\u2500 down.sql\n\u2502       \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 docker-compose.yml\n\u2514\u2500\u2500 README.md\n</code></pre> <p>Explanation:</p> <ul> <li><code>sql/</code>: This folder contains all SQL-related files for your project.</li> <li><code>scripts/</code>: Contains scripts for creating database objects like tables, views, functions, and stored procedures.<ul> <li><code>schema/</code>: Contains subfolders for different types of database objects.</li> <li><code>tables/</code>, <code>views/</code>, <code>functions/</code>, <code>procedures/</code>: Each of these folders contains SQL scripts for the respective database objects.</li> </ul> </li> <li><code>data/</code>: Contains data scripts such as seed data.</li> <li><code>migrations/</code>: Contains SQL migration scripts for managing database schema changes over time. Each migration version should have an <code>up.sql</code> script for applying the migration and a <code>down.sql</code> script for reverting it.</li> <li><code>Dockerfile</code>: The Dockerfile for building a Docker image for your PostgreSQL database.</li> <li><code>docker-compose.yml</code>: A Docker Compose file that defines the services, networks, and volumes for your PostgreSQL container.</li> <li><code>README.md</code>: Include documentation or instructions for using the repository, such as setup steps, environment variables, or specific details about the database scripts.</li> </ul> <p>Feel free to customize this structure according to your project's needs. You can add additional folders or files as required.</p> <p>Dockerfile vs Docker Compose</p> <p>A <code>Dockerfile</code> contains a set of instructions and commands used by Docker to automatically build a new container image.</p> <p>The <code>docker-compose.yaml</code> file is a configuration file used by Docker Compose, a tool for defining and running multi-container Docker applications. With a single command, Docker Compose uses the docker-compose.yaml file to create and start all the services defined in the file. </p> <p>In this lab I'll show you both approaches for creating PostgreSQL database.</p>"},{"location":"microservices/7.postgresql-db/#step-3-add-dockerfiles-to-the-database-project","title":"Step-3: Add Dockerfiles to the Database Project","text":"<p>To build a Docker image for PostgreSQL, create a Dockerfile in your project's root directory:</p> <pre><code># Use the official PostgreSQL image as the base image\nFROM postgres:latest\n\n# Set environment variables for PostgreSQL\nENV POSTGRES_USER=myuser\nENV POSTGRES_PASSWORD=mypassword\nENV POSTGRES_DB=mydatabase\n</code></pre> <p>In this Dockerfile:</p> <ul> <li>We use the official PostgreSQL image as our base image.</li> <li>Set environment variables to configure the PostgreSQL instance.</li> </ul> <p>Step-3.1: Docker Build Locally</p> <p>Navigate to your project's root directory and build the Docker image locally using the following command:</p> <pre><code>docker build -t my-postgresql-image .\n</code></pre> <p>Replace <code>my-postgresql-image</code> with a meaningful name for your image.</p> <p>Docker desktop &gt; Image</p> <p></p> <p>Step-3.2: Docker Run Locally</p> <p>To run your PostgreSQL container locally for testing and development, use the following command:</p> <pre><code>docker run -d --name my-postgresql-container -p 5432:5432 my-postgresql-image\n</code></pre> <p>This command creates a container named <code>my-postgresql-container</code> and maps port 5432 from the container to the host.</p> <p>Docker desktop &gt; Container</p> <p></p>"},{"location":"microservices/7.postgresql-db/#step-4-create-docker-compose-file","title":"Step-4:  Create Docker Compose file","text":"<p>To setup the PostgreSQL database with docker compose you need to first create a docker compose file that defines the PostgreSQL database service and any necessary dependencies.</p> <p>Create a file named <code>docker-compose.yml</code> in your project directory. This file will define the services and configurations for your PostgreSQL database setup.</p> <p>In the docker-compose.yml file, define the PostgreSQL database service. Use the official PostgreSQL database Docker image and specify any necessary configurations. Here's an example of a PostgreSQL database service definition:</p> docker-compose.yml<pre><code># Database type: PostgreSQL\n# Database name: postgres\n# Database username: postgres\n# Database password: example\n# ADVANCED OPTIONS; Database host: postgres\n\nversion: '3'\n\nservices:\n  postgres:\n    image: postgres:16\n    environment:\n      POSTGRES_USER : postgres\n      POSTGRES_PASSWORD: example\n    # ports:\n    #     - \"5432:5432\"\n    restart: always\n</code></pre> <ul> <li>Uses the <code>postgres:16</code> Docker image.</li> <li>Maps port 5432 on your host to port 5432 in the PostgreSQL database container.</li> <li>Sets up an initial user and password for PostgreSQL database.</li> </ul> <p>Step-4.1: Build PostgreSQL database locally</p> <p>The <code>docker-compose up</code> command is used to start and initialize the services defined in a Docker Compose file. We will build the Docker container locally using the docker compose and ensure that the containerized application working as expected.</p> <pre><code>docker-compose up\n\n# or - -d flag, it tells Docker Compose to run the containers in detached mode\ndocker-compose up -d\n\n#output\n[+] Running 33/2\n \u2714 postgres 14 layers [\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff]      0B/0B      Pulled\n[+] Running 3/3\n \u2714 Network Postgresql_default       Created\n \u2714 Container Postgresql-postgres-1  Started\n</code></pre> <p>List running Docker containers on your system.</p> <pre><code>docker ps\n\n# output\nCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMES\na433ad35d0a1   postgres:16   \"docker-entrypoint.s\u2026\"   20 seconds ago   Up 19 seconds   0.0.0.0:5432-&gt;5432/tcp   postgresql-postgres-1\n</code></pre> <p>List Docker images that are currently available on your local system.</p> <pre><code>docker image ls\n\n# output\nREPOSITORY   TAG       IMAGE ID       CREATED       SIZE\npostgres     16        488c2842403b   4 weeks ago   448MB\n</code></pre> <p>Step-4.2: Run PostgreSQL database Container locally</p> <p>Run the Docker container locally to verify that the PostgreSQL database  working correctly within a containerized environment. This step ensures that the containerized PostgreSQL database  works as expected on your local machine.</p> <p>List the running Docker containers on your system</p> <pre><code>docker container ls\n\n# output\nCONTAINER ID   IMAGE         COMMAND                  CREATED          STATUS          PORTS                    NAMES\na433ad35d0a1   postgres:16   \"docker-entrypoint.s\u2026\"   42 seconds ago   Up 41 seconds   0.0.0.0:5432-&gt;5432/tcp   postgresql-postgres-1\n</code></pre> <p>List the Docker networks that are available on your local system</p> <pre><code>docker network ls\n\n# output\nNETWORK ID     NAME      DRIVER    SCOPE\na63fce88f432   bridge    bridge    local\n3a43b39f60b0   host      host      local\n21fbef3d5c78   none      null      local\n</code></pre> <p>if you open the docker desktop you will notice the new image &amp; container started running.</p> <p>Note</p> <p>Ensure that you test the PostgreSQL connection using either the pgAdmin tool or the <code>psql</code> command-line tool.</p>"},{"location":"microservices/7.postgresql-db/#step-5-test-the-postgresql-database-connection-from-psql-tool","title":"Step-5: Test the PostgreSQL database connection from psql tool","text":"<p>The psql tool is a command-line interface (CLI) provided by PostgreSQL that allows users to interact with PostgreSQL databases directly from the terminal or command prompt.</p> <p>To test a PostgreSQL database connection using the <code>psql</code> command-line tool, follow these steps:</p> <p>Opening a terminal or command prompt on your computer.</p> <p>Use the <code>psql</code> utility to connect to your PostgreSQL database. </p> <pre><code>psql -h &lt;hostname&gt; -U &lt;username&gt; -d &lt;database_name&gt;\n</code></pre> <p>Replace the placeholders with your specific information:</p> <ul> <li><code>&lt;hostname&gt;</code>: The hostname  of the PostgreSQL server.</li> <li><code>&lt;username&gt;</code>: Your PostgreSQL username.</li> <li><code>&lt;database_name&gt;</code>: The name of the PostgreSQL database you want to connect to.</li> </ul> <p>For example, to connect to a PostgreSQL database running on the local machine with the username \"myuser\" and the database \"mydatabase,\" you would use the following command:</p> <pre><code>psql -h localhost -U myuser -d mydatabase\n</code></pre> <p>If the PostgreSQL server is running on the default port (5432), you don't need to specify the port. However, if it's running on a different port, you can use the <code>-p</code> option to specify the port number.</p> <p>Provide Password(if required):</p> <p>If your PostgreSQL database is configured to require a password for the specified username, you will be prompted to enter the password. Type the password and press Enter.</p> <p>Successful Connection:</p> <p>If the connection is successful, you should see the <code>psql</code> command prompt, which indicates that you are connected to the PostgreSQL database. It will look something like this:</p> <pre><code>psql (12.7)\nType \"help\" for help.\n\nmydatabase=#\n</code></pre> <p>You are now in an interactive session with the PostgreSQL database, and you can run SQL commands and queries.</p> <p>Test SQL Commands:</p> <p>To further test the database connection, you can execute SQL commands to retrieve or manipulate data. For example, you can run a simple SQL query like:</p> <pre><code>SELECT version();\n</code></pre> <p>This query will return the PostgreSQL version, confirming that you can interact with the database.</p> <p>Exit <code>psql</code>:</p> <p>To exit the <code>psql</code> session and return to your command prompt, you can type:</p> <pre><code>\\q\n</code></pre> <p>or press <code>Ctrl + D</code> (on Unix-based systems) or <code>Ctrl + Z</code> (on Windows).</p> <p></p> <p>By following these steps, you can connect to a PostgreSQL database using the psql tool and start interacting with the database using SQL commands directly from the command line.</p> <p>Create a Database</p> <p>Creating a database is really simple. Execute the command below, and the database should be created.</p> <pre><code># command\npostgres=# CREATE DATABASE [databasename];\n\n# example\nCREATE DATABASE mydatabase2;\n</code></pre>"},{"location":"microservices/7.postgresql-db/#step-6-test-the-postgresql-database-from-pgadmin4-tool","title":"Step-6: Test the PostgreSQL database from pgadmin4 tool","text":"<p>pgAdmin is a popular open-source graphical user interface (GUI) administration tool for PostgreSQL. It provides a comprehensive set of features for managing PostgreSQL databases, including creating and managing databases, schemas, tables, indexes, users, and permissions. pgAdmin allows users to interact with PostgreSQL databases visually, making database administration tasks more intuitive and efficient.</p> <p>To connect to a PostgreSQL database using the pgAdmin tool, follow these steps:</p> <p>Install and Launch pgAdmin 4:</p> <p>Download and install pgAdmin 4 on your computer. Once installed, launch the pgAdmin 4 application.</p> <p>Log in to pgAdmin</p> <p>When you launch pgAdmin for the first time, you'll be prompted to set the master password. Enter the password you want to use to log in to pgAdmin. You can also choose to save the login information for future use.</p> <p>Add a Server:</p> <p>After logging in, you'll see the pgAdmin interface. To connect to a PostgreSQL server, you need to add a server to pgAdmin. Follow these steps:</p> <ul> <li>In the left sidebar, expand \"Servers\" to reveal the \"PostgreSQL\" group.</li> <li>Right-click on \"PostgreSQL\" and select \"Register &gt; Server...\"</li> <li> <p>Configure Server Connection: you'll need to provide the following information:</p> </li> <li> <p>Name: Give your server a descriptive name to identify it within pgAdmin.</p> </li> <li>Host name/address: Specify the hostname or IP address of the PostgreSQL server you want to connect to.</li> <li>Port: Enter the port number where PostgreSQL is running (default is 5432).</li> <li>Maintenance database: Specify the name of a database to connect to initially (e.g., \"postgres\" or \"mydatabase\").</li> <li>Username: Enter the PostgreSQL username you want to use for this connection.</li> <li>Password: Provide the password for the specified username.</li> </ul> <p>for example:</p> <pre><code>hostname - `localhost`\ndefault database - `postgres`\nport - `5432` - default postgresql port #\ndefault user - `user` - this is the owner of default database postgres\npassword - `example` - this is what we've setup in our container\n</code></pre> <p></p> <p>Once you've entered the connection details, click the \"Save\" button to add the server.</p>"},{"location":"microservices/7.postgresql-db/#step-7-push-docker-container-to-acr","title":"Step-7: Push Docker Container to ACR","text":"<p>Push your PostgreSQL container image to Azure Container Registry (ACR) for use in AKS. Follow these steps:</p> <p>Log in to your Azure account using the Azure CLI:</p> <pre><code>az login\n</code></pre> <p>Authenticate to your ACR:</p> <pre><code>az acr login --name myacr\n</code></pre> <p>Replace <code>myacr</code> with your ACR name.</p> <p>Tag your local Docker image with the ACR login server:</p> <pre><code>docker tag my-postgresql-image myacr.azurecr.io/my-postgresql-image:v1\n</code></pre> <p>Push the Docker image to ACR:</p> <pre><code>docker push myacr.azurecr.io/my-postgresql-image:v1\n</code></pre> <p>Replace <code>myacr</code> and <code>v1</code> with your ACR name and desired image version.</p> <p>Now, your PostgreSQL container image is stored in Azure Container Registry and can be easily pulled and deployed from AKS to Azure Database for PostgreSQL - Flexible Server.</p>"},{"location":"microservices/7.postgresql-db/#conclusion","title":"Conclusion","text":"<p>You have successfully created a Docker container for PostgreSQL database, container created as part of this task will be used in the future labs in AKS.</p>"},{"location":"microservices/7.postgresql-db/#references","title":"References","text":"<ul> <li>Postgres Docker Official Image</li> </ul>"},{"location":"microservices/8.keycloak/","title":"Setting up Keycloak in a Docker Container","text":""},{"location":"microservices/8.keycloak/#introduction","title":"Introduction","text":"<p>As part of this chapter, I will introduce Keycloak as one of the applications in the microservices landscape for Security, which is a common requirement across many organizations. Keycloak is a free, open-source, powerful, and flexible popular open-source identity and access management solution that enables you to secure your applications and services.</p> <p>If you are new to Keycloak and would like to learn more, you can refer to my article on Getting Started with Keycloak</p> <p>In this lab, I will guide you through the process of creating Docker container for Keycloak, and finally accessing the Keycloak application in the web browser.</p> <p>Running Keycloak in a Docker container provides greater control and flexibility over its configuration and deployment, allowing us to tailor authentication, authorization, and identity management features to meet specific organizational requirements. This approach enables seamless customization of realms, themes, user federation, and integration with external identity providers, all while maintaining consistency across environments through containerized infrastructure.</p> <p>The objective is to establish a local development environment for the Keycloak application, securing Microservices with Keycloak in a Microservices Architecture,  our goal is to implement authentication and authorization mechanisms across the microservices, ensuring that only authorized users and services can access specific resources. To accomplish this, you will create a Dockerfile or Docker Compose files, run them locally, and subsequently push the image to an Azure Container Registry (ACR). All of these tasks we are doing here will be useful in later chapters when deploying to the Azure Kubernetes Service (AKS).</p> <p>Here are some Keycloak requirements within microservices architecture:</p> <ul> <li> <p>Keycloak integration:    Integrate Keycloak into the microservices architecture to provide authentication and authorization capabilities.    Keycloak should act as the central identity provider for all microservices.</p> </li> <li> <p>User management:    Implement user management within Keycloak, allowing users to sign up, log in, and manage their profiles.    Define user roles and groups for fine-grained access control.</p> </li> <li> <p>Secure API endpoints:    Protect API endpoints to ensure that only authenticated users or services with the appropriate permissions can access them.    Implement OAuth 2.0 or OpenID Connect for securing APIs.</p> </li> <li> <p>Single Sign-On (SSO):    Keycloak can serve as a versatile solution for enabling SSO between companies by establishing federated trust relationships between IdPs and SPs. This approach simplifies user access across organizations, enhances security, and provides a seamless user experience when accessing services and applications from different companies.</p> </li> <li> <p>JWT Tokens:    Utilize JSON Web Tokens (JWT) for secure communication between microservices and Keycloak.    Configure token expiration, signing, and validation.</p> </li> </ul>"},{"location":"microservices/8.keycloak/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup repository for Keycloak in Azure devops.</li> <li>Step-2: Create Keycloak Folder.</li> <li>Step-3: Add Dockerfiles to the Keycloak project.</li> <li>Step-4: Docker build locally.</li> <li>Step-5: Docker run locally.</li> <li>Step-6: Publish the Keycloak docker container to container registry.</li> </ul>"},{"location":"microservices/8.keycloak/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Docker and the VS Code Docker extension :  - Docker Downloads.</li> <li>Git Client tool:  - Git Downloads.</li> <li>Azure devops and Git Repository: Initialize a Git repository for your Keycloak application.</li> <li>Azure Container Registry (ACR)</li> <li>Docker compose installed</li> </ul> <p>Verify the docker installation by running following commands: <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre></p> <p>Verify the docker compose by running following commands:</p> <pre><code>docker-compose version\n</code></pre>"},{"location":"microservices/8.keycloak/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create docker container for Keycloak application.</p> <p></p>"},{"location":"microservices/8.keycloak/#step-1-setup-repository-for-keycloak-in-azure-devops","title":"Step-1: Setup repository for Keycloak in Azure DevOps","text":"<p>Before you begin with the Keycloak setup, it's necessary to have a version control repository to manage your project. </p> <ul> <li>Create azure devops project</li> <li>Initialize repository</li> </ul> <p>For this Keycloak application, we can either use an existing git repository created in our first chapter or initiate a new one.</p> <p>To clone an existing repository, run the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/8.keycloak/#step-2-create-keycloak-project","title":"Step-2: Create Keycloak project","text":"<p>In this step, we'll create a dedicated Folder for our Keycloak application</p> <p>Create a new Folder: Inside our Git repository, create a new directory or folder specifically for your Keycloak application. This folder will contain all the necessary files for Keycloak, including Dockerfiles and configurations.</p> <p></p>"},{"location":"microservices/8.keycloak/#step-3-keycloak-setup-with-docker-compose","title":"Step-3: Keycloak setup with docker compose","text":"<p>Setup Keycloak Service:</p> <p>To setup the Keycloak with docker compose you need to first create a docker compose file that defines the Keycloak service and any necessary dependencies, such as a PostgreSQL database. </p> <p>Here's a step-by-step explanation of how to set up Keycloak with docker compose:</p> <p>Create a file named <code>docker-compose.yml</code> in your project directory. This file will define the services and configurations for your Keycloak setup.</p> <p>In the docker-compose.yml file, define the Keycloak service. Use the official Keycloak Docker image and specify any necessary configurations. Here's an example of a Keycloak service definition:</p> docker-compose.yml<pre><code>services:\n  auth:\n    image: quay.io/keycloak/keycloak\n    ports:\n      - \"8080:8080\"\n    environment:\n      KEYCLOAK_ADMIN: admin \n      KEYCLOAK_ADMIN_PASSWORD: admin\n    command: \n      - start-dev \n      - --import-realm\n    volumes:\n      - /home/keycloak/realm.json:/opt/keycloak/data/import/realm.json\n</code></pre> <p>This definition:</p> <ul> <li>Uses the <code>quay.io/keycloak/keycloak</code> Docker image.</li> <li>Maps port 8080 on your host to port 8080 in the Keycloak container.</li> <li>Sets up an initial admin user and password for Keycloak.</li> </ul> <pre><code>docker-compose up\n# or\ndocker-compose up -d\n\n#output\n[+] Running 1/0\n \u2714 Container keycloak-auth-1  Created                                                                                                                                                                                   0.0s \nAttaching to keycloak-auth-1\n.\n.\n.\nRunning the server in development mode. DO NOT use this configuration in production.\n</code></pre> <p><pre><code>docker ps\n\n# output\n\nCONTAINER ID   IMAGE                                COMMAND                  CREATED              STATUS              PORTS                              NAMES\nd3ee7cef046e   quay.io/keycloak/keycloak            \"/opt/keycloak/bin/k\u2026\"   About a minute ago   Up About a minute   0.0.0.0:8080-&gt;8080/tcp, 8443/tcp   keycloak-auth-1\n</code></pre> <pre><code>docker image ls\n\n# output\nREPOSITORY                                TAG                                        IMAGE ID       CREATED         SIZE  \nquay.io/keycloak/keycloak                 latest                                     273d68e6fb8c   6 days ago      459MB \n</code></pre></p> <p><pre><code>docker container ls\n\n# output\nCONTAINER ID   IMAGE                                COMMAND                  CREATED          STATUS          PORTS                              NAMES\nd3ee7cef046e   quay.io/keycloak/keycloak            \"/opt/keycloak/bin/k\u2026\"   22 minutes ago   Up 22 minutes   0.0.0.0:8080-&gt;8080/tcp, 8443/tcp   keycloak-auth-1\n</code></pre> <pre><code>docker network ls\n\n# output\nNETWORK ID     NAME                           DRIVER    SCOPE\ne71f9c6bd718   bridge                         bridge    local\nd08c17ea4f0e   docker-nodejs-sample_default   bridge    local\ncfb02a162739   host                           host      local\nc8fb8d726406   keycloak_default               bridge    local\n8bba86e6ad07   none                           null      local\n</code></pre></p> <p>Access Keycloak: </p> <p>Once the Keycloak service is up and running, you can access the Keycloak admin console by opening a web browser and navigating to http://localhost:8080. You can log in using the admin user and password you defined in the Keycloak service configuration.</p> <p>Keycloal admin console</p> <p></p> <p>Keycloak Login page</p> <p></p> <p>Keycloak master relm</p> <p></p> <p>Setup Keycloak Service with PostgreSQL database:</p> <p>If you want to use a PostgreSQL database as Keycloak's backend, define a PostgreSQL service in the same <code>docker-compose.yml</code> file. </p> <p>Here's a complete <code>docker-compose.yml</code> file that sets up Keycloak with a PostgreSQL database:</p> docker-compose.yml<pre><code>version: '3'\nservices:\n  keycloak:\n    image: quay.io/keycloak/keycloak\n    container_name: keycloak\n    ports:\n      - \"8080:8080\"\n    environment:\n      - KEYCLOAK_ADMIN=admin\n      - KEYCLOAK_ADMIN_PASSWORD=admin\n    command: \n      - start-dev \n      - --import-realm\n    volumes:\n      - /home/keycloak/realm.json:/opt/keycloak/data/import/realm.json\n    depends_on:\n      - postgres\n    networks:\n      - keycloak_network\n\n  postgres:\n    image: postgres:latest\n    container_name: postgres\n    ports:\n      - \"5432:5432\"\n    environment:\n      - POSTGRES_DB=keycloak\n      - POSTGRES_USER=keycloak\n      - POSTGRES_PASSWORD=keycloak\n    networks:\n      - keycloak_network\n\nnetworks:\n  keycloak_network:\n    driver: bridge\n</code></pre> <p>In this <code>docker-compose.yml</code> file:</p> <ul> <li>The <code>keycloak</code> service uses the official Keycloak Docker image, maps port 8080 on your host to port 8080 in the Keycloak container, and sets up an initial admin user and password.</li> <li>The <code>postgres</code> service uses the official PostgreSQL Docker image, specifies the database name, username, and password for PostgreSQL, and maps port 5432 on your host to port 5432 in the PostgreSQL container.</li> <li><code>depends_on</code> ensures that the <code>keycloak</code> service starts only after the <code>postgres</code> service is up and running, as Keycloak relies on the PostgreSQL database.</li> <li>Both services are connected to a custom network called <code>keycloak_network</code> for communication between containers.</li> </ul> <p></p> <p>Once both services are up and running, you can access the Keycloak admin console by opening a web browser and navigating to <code>http://localhost:8080/auth</code>. Log in using the admin user and password you specified in the Keycloak service configuration.</p>"},{"location":"microservices/8.keycloak/#step-4-keycloak-setup-with-dockerfile","title":"Step-4: Keycloak setup with Dockerfile","text":"<p>Step-4.1: Create Dockerfile</p> <p>Let's create a Dockerfile in the root directory of our project and include the following code. We are going to use this Dockerfile to containerize our Keycloak application as per our need.</p> Dockerfile<pre><code>FROM quay.io/keycloak/keycloak:latest as builder\n\n# Enable health and metrics support\nENV KC_HEALTH_ENABLED=true\nENV KC_METRICS_ENABLED=true\n\n# Configure a database vendor\nENV KC_DB=postgres\n\nWORKDIR /opt/keycloak\n# for demonstration purposes only, please make sure to use proper certificates in production instead\nRUN keytool -genkeypair -storepass password -storetype PKCS12 -keyalg RSA -keysize 2048 -dname \"CN=server\" -alias server -ext \"SAN:c=DNS:localhost,IP:127.0.0.1\" -keystore conf/server.keystore\nRUN /opt/keycloak/bin/kc.sh build\n\nFROM quay.io/keycloak/keycloak:latest\nCOPY --from=builder /opt/keycloak/ /opt/keycloak/\n\n# change these values to point to a running postgres instance\nENV KC_DB=postgres\nENV KC_DB_URL=&lt;DBURL&gt;\nENV KC_DB_USERNAME=&lt;DBUSERNAME&gt;\nENV KC_DB_PASSWORD=&lt;DBPASSWORD&gt;\nENV KC_HOSTNAME=localhost\nENTRYPOINT [\"/opt/keycloak/bin/kc.sh\"]\n</code></pre> <p>Step-4.2: Docker build locally</p> <p>We will build the Docker container locally using the Dockerfiles and ensure that the containerized application working as expected.</p> <p>The <code>docker build</code> command is used to build Docker images from a Dockerfile.  </p> <pre><code>docker build -t sample/keycloak-app:20240101.1 .\n</code></pre> <p>output</p> <pre><code>Docker build output goes here\n</code></pre> <p>When you run the <code>docker build</code> command, Docker looks for a Dockerfile in the specified directory and reads the instructions in the file to build a new image. </p> <p>The Dockerfile contains a series of instructions that define how to build the image, such as copying files, running commands, and setting environment variables. </p> <p>Step-4.3: Docker run locally</p> <p>Run the Docker container locally to verify that the keycloak application working correctly within a containerized environment. This step ensures that the containerized keycloak application works as expected on your local machine.</p> <p>Run the <code>docker run</code> command to start a container based on the image:</p> <p><pre><code>docker run --rm -p 8080:8080 sample/keycloak-app:20240101.1 .\n</code></pre> output</p> <p><pre><code>Docker run output goes here\n</code></pre> if you open the docker desktop you will notice the new image &amp; container started running.</p>"},{"location":"microservices/8.keycloak/#step-5-publish-the-keycloak-docker-container-to-container-registry","title":"Step-5: Publish the Keycloak docker container to container registry","text":"<p>Now that we have Keycloak Docker container ready locally, it's time to push them to the Container Registry for future deployment on Azure Kubernetes Services (AKS). This step is important for preparing the container for deployment in a cloud environment.</p> <p>To publish a Keycloak Docker container to Azure Container Registry (ACR), you will need to have the following:</p> <p>Create an Azure Container Registry. If you don't have one, you can create one by following the instructions in the Azure Portal or using Azure CLI.</p> <p>Log in to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code># azure Login\naz login\n\n# set the azure subscription\naz account set -s \"anji.keesari\"\n\n# Log in to the container registry\naz acr login --name acr1dev\n# Login Succeeded\n# To get the login server address for verification\naz acr list --resource-group rg-acr-dev --query \"[].{acrLoginServer:loginServer}\" --output table\n\n# output should look similar to this.\n\n# AcrLoginServer    \n# ------------------\n# acr1dev.azurecr.io\n</code></pre> <p>list all the Docker images that are available on the local system</p> <pre><code>docker images\n\n# output\n\nREPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\nsample/keycloak-app                                         20230312.1                                                                   587f347206bc   8 minutes ago   216MB\n.\n.\n.\n</code></pre> <p><code>Tag</code> your Docker container image with the full name of your Azure Container Registry, including the repository name and the version tag. You can do this by running the following command:</p> <pre><code>docker tag sample/keycloak-app:20240101.1 acr1dev.azurecr.io/sample/keycloak-app:20240101.1\n</code></pre> <p>Push your Docker container image to your Azure Container Registry using the Docker command-line interface. You can do this by running the following command:</p> <pre><code>docker push acr1dev.azurecr.io/sample/keycloak-app:20240101.1\n\n#Output\nThe push refers to repository [acr1dev.azurecr.io/sample/keycloak-app]\n649a035a1734: Pushed\n4061bd2dd536: Pushed\nc0257b3030b0: Pushed\n912a3b0fc587: Pushed\na36186d93e25: Pushed\na3d997b065bc: Pushed\n65d358b7de11: Pushed\nf97384e8ccbc: Pushed\nd56e5e720148: Pushed\nbeee9f30bc1f: Pushed\n20240101.1: digest: sha256:73f0669d18c6cae79beb81edc8c523191710f9ec4781d590884b46326f9ad6f9 size: 2419\n</code></pre> <p>Wait for the push to complete. Depending on the size of your Docker container image and the speed of your internet connection, this may take a few minutes.</p> <p>Verify the newly pushed image to ACR.</p> <pre><code>az acr repository list --name acr1dev --output table\n\n# Output\n\nResult\n-------------------------------\nmcr.microsoft.com/dotnet/aspnet\nmcr.microsoft.com/dotnet/sdk\nsample/aspnet-api\nsample/aspnet-app\nsample/node-api\nsample/postgresql-db\nsample/keycloak-app\n</code></pre> <p>Show the new tags of a image in the acr</p> <pre><code>az acr repository show-tags --name acr1dev --repository sample/keycloak-app --output table\n</code></pre> <p>You've successfully pushed your Docker container image to Azure Container Registry. You can now use the Azure Portal or Azure CLI to manage your container images and deploy them to Azure services like Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/8.keycloak/#conclusion","title":"Conclusion","text":"<p>You have successfully created a Docker container for keycloak application, container created as part of this task will be used in the future labs in AKS.</p>"},{"location":"microservices/8.keycloak/#references","title":"References","text":"<ul> <li>keycloak Docker image</li> <li>Keycloak Official Documentation </li> <li>GitHub repository</li> <li>Stack Overflow </li> </ul> <p>-</p>"},{"location":"microservices/9.drupal/","title":"Setting up Drupal in a Docker Container","text":""},{"location":"microservices/9.drupal/#introduction","title":"Introduction","text":"<p>As part of this chapter, I will introduce Drupal as one of the applications in the microservices landscape for content management, which is a common requirement across many organizations. Drupal is a free, open-source, powerful, and flexible CMS written in PHP that enables you to create and manage websites with ease.</p> <p>If you are new to Drupal and would like to learn more, you can refer to my article on Getting Started with Drupal: A Beginner's Guide</p> <p>In this lab, I will guide you through the process of creating Docker container for Drupal and run PostgreSQL database in the backend, and finally accessing the drupal website in the web browser.</p> <p>The objective is to establish a local development environment for the drupal website. To accomplish this, you will create a docker Compose file, run them locally. All of these tasks we are doing here will be useful in later chapters when deploying to the Azure Kubernetes Service (AKS).</p>"},{"location":"microservices/9.drupal/#technical-scenario","title":"Technical Scenario","text":"<p>As an <code>Application Architect</code>, your responsibility is to design a content management system (CMS) that provides you with enhanced control and flexibility. By creating a custom Docker container with Drupal, you gain the ability to make modifications. You can adapt the Dockerfile to include additional packages, configurations, or custom modules/themes as per your project's specific requirements. This approach guarantees that your Drupal environment aligns with your project's needs while simultaneously utilizing Docker's advantages, including isolation, portability, and scalability.</p>"},{"location":"microservices/9.drupal/#objective","title":"Objective","text":"<p>In this exercise, our objective is to accomplish and learn the following tasks:</p> <ul> <li>Step-1: Setup Git Repository for Drupal.</li> <li>Step-2: Create Drupal Folder locally.</li> <li>Step-3: Create Docker Compose file</li> <li>Step-4: Build Drupal locally.</li> <li>Step-5: Run Drupal Container locally.</li> </ul>"},{"location":"microservices/9.drupal/#prerequisites","title":"Prerequisites","text":"<p>Before starting this lab, ensure you have the following prerequisites in place:</p> <ul> <li>Docker Desktop: - Docker Downloads.</li> <li>Git Client tool: - Git Downloads.</li> <li>Git Repository: Initialize a Git repository for your Drupal website.</li> <li>Docker installed</li> <li>Docker compose installed</li> <li>PostgreSQL installed - this will allow you to run <code>psql</code> command line tool</li> </ul> <p>Verify the docker installation by running following commands: <pre><code>docker version\n# or\ndocker --version\n# or\ndocker -v\n</code></pre></p> <p>Verify the docker compose by running following commands:</p> <pre><code>docker-compose version\n</code></pre>"},{"location":"microservices/9.drupal/#architecture-diagram","title":"Architecture Diagram","text":"<p>The following diagram shows the high level steps to create docker container for Drupal website.</p> <p></p>"},{"location":"microservices/9.drupal/#step-1-setup-git-repository-for-drupal","title":"Step-1: Setup Git Repository for Drupal","text":"<p>Setting up a Git repository for your Drupal project allows you to manage your code effectively, work in teams, and track the changes of your website's codebase.</p> <ul> <li>Create azure devops project</li> <li>Initialize repository</li> </ul> <p>For this Drupal website, we can either use an existing git repository created in our first chapter or initiate a new one.</p> <p>For example to clone an existing repository, run the following command:</p> <pre><code>git clone https://keesari.visualstudio.com/Microservices/_git/microservices\n</code></pre>"},{"location":"microservices/9.drupal/#step-2-create-drupal-project","title":"Step-2: Create Drupal Project","text":"<p>In this step, we'll create a dedicated project or folder for our Drupal Website</p> <p>Create a new project:</p> <p>Inside our Git repository, create a new directory or folder specifically for your Drupal website. This folder will contain all the necessary files for Drupal website, including docker compose &amp; Dockerfile and configurations.</p> <p></p>"},{"location":"microservices/9.drupal/#step-3-create-docker-compose-file","title":"Step-3:  Create Docker Compose file","text":"<p>To setup the Drupal with docker compose you need to first create a docker compose file that defines the drupal service and any necessary dependencies, such as a PostgreSQL database. </p> <p>Create a file named <code>docker-compose.yml</code> in your project directory. This file will define the services and configurations for your Drupal setup.</p> <p>In the docker-compose.yml file, define the Drupal service. Use the official Drupal Docker image and specify any necessary configurations. Here's an example of a Drupal service definition:</p> docker-compose.yml<pre><code># Drupal with PostgreSQL\n#\n# Access via \"http://localhost:8080\"\n#   (or \"http://$(docker-machine ip):8080\" if using docker-machine)\n#\n# During initial Drupal setup,\n# Database type: PostgreSQL\n# Database name: postgres\n# Database username: postgres\n# Database password: example\n# ADVANCED OPTIONS; Database host: postgres\n\nversion: '3.1'\n\nservices:\n\n  drupal:\n    image: drupal:10-apache\n    ports:\n      - 8080:80\n    volumes:\n      - /var/www/html/modules\n      - /var/www/html/profiles\n      - /var/www/html/themes\n      # this takes advantage of the feature in Docker that a new anonymous\n      # volume (which is what we're creating here) will be initialized with the\n      # existing content of the image at the same location\n      - /var/www/html/sites\n    restart: always\n\n  postgres:\n    image: postgres:16\n    environment:\n      POSTGRES_USER : postgres\n      POSTGRES_PASSWORD: example   \n    # ports:\n    #     - \"5432:5432\"\n    restart: always\n</code></pre> <ul> <li>Uses the <code>drupal:10-apache</code> Docker image.</li> <li>Maps port 8080 on your host to port 8080 in the drupal container.</li> <li>Sets up an initial admin user and password for drupal.</li> </ul>"},{"location":"microservices/9.drupal/#step-4-build-drupal-locally","title":"Step-4: Build Drupal locally","text":"<p>The <code>docker-compose up</code> command is used to start and initialize the services defined in a Docker Compose file. We will build the Docker container locally using the docker compose and ensure that the containerized application working as expected.</p> <pre><code>docker-compose up\n\n# or - -d flag, it tells Docker Compose to run the containers in detached mode\ndocker-compose up -d\n\n#output\n[+] Running 33/2\n \u2714 postgres 14 layers [\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff]      0B/0B      Pulled\n \u2714 drupal 17 layers [\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff\u28ff]      0B/0B      Pulled\n[+] Running 3/3\n \u2714 Network drupal_default       Created\n \u2714 Container drupal-drupal-1    Started\n \u2714 Container drupal-postgres-1  Started\n</code></pre> <p>List running Docker containers on your system. </p> <pre><code>docker ps\n\n# output\nCONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS                    NAMES\nb2701f4c0b44   postgres:16                 \"docker-entrypoint.s\u2026\"   24 minutes ago   Up 24 minutes   5432/tcp                 drupal-postgres-1\n5141598054d3   drupal:10-apache            \"docker-php-entrypoi\u2026\"   24 minutes ago   Up 24 minutes   0.0.0.0:8080-&gt;80/tcp     drupal-drupal-1\n</code></pre> <p>List Docker images that are currently available on your local system.</p> <pre><code>docker image ls\n\n# output\nREPOSITORY                                                TAG                                                                          IMAGE ID       CREATED         SIZE\ndrupal                                                    10-apache                                                                    48fb247e75d6   2 weeks ago     594MB\npostgres                                                  16                                                                           b0b90c1d9579   4 weeks ago     425MB\n</code></pre>"},{"location":"microservices/9.drupal/#step-43-run-drupal-container-locally","title":"Step-4.3: Run Drupal Container locally.","text":"<p>Run the Docker container locally to verify that the drupal website working correctly within a containerized environment. This step ensures that the containerized drupal website works as expected on your local machine.</p> <p>List the running Docker containers on your system</p> <pre><code>docker container ls\n\n# output\nCONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS                    NAMES\nb2701f4c0b44   postgres:16                 \"docker-entrypoint.s\u2026\"   25 minutes ago   Up 25 minutes   5432/tcp                 drupal-postgres-1\n5141598054d3   drupal:10-apache            \"docker-php-entrypoi\u2026\"   25 minutes ago   Up 25 minutes   0.0.0.0:8080-&gt;80/tcp     drupal-drupal-1\n</code></pre> <p>List the Docker networks that are available on your local system</p> <pre><code>docker network ls\n\n# output\n026de34a62dc   bridge                         bridge    local\n0dcb9a6803a2   drupal_default                 bridge    local\n</code></pre> <p>if you open the docker desktop you will notice the new image &amp; container started running.</p> <p>Note</p> <p>Ensure that you test the PostgreSQL connection using either the pgAdmin tool or the <code>psql</code> command-line tool.</p> <p></p> <p>Access Drupal Webstie</p> <p>Once the Drupal service is up and running, you can access the Drupal website by opening a web browser and navigating to http://localhost:8080. You can log in using the admin user and password you defined in the Drupal service configuration.</p> <p>Drupal website &gt; language</p> <p></p> <p>Drupal &gt; Installation Profile</p> <p></p> <p>Drupal &gt; Database configuration</p> <p></p> <p>Drupal &gt; configure site</p> <p></p> <p>Drupal &gt; welcome page</p> <p></p> <p>Drupal &gt; Users page</p> <p></p>"},{"location":"microservices/9.drupal/#conclusion","title":"Conclusion","text":"<p>You have successfully created a Docker container for Drupal Website, container created as part of this task will be used in the future labs in AKS.</p>"},{"location":"microservices/9.drupal/#references","title":"References","text":"<p>For further information and resources related to setting up Drupal in a Docker container, refer to the following:</p> <ul> <li>Drupal Docker Official Repository</li> <li>Docker Documentation</li> <li>Docker Compose Documentation</li> <li>Drupal.org - Official Drupal Website</li> </ul>"},{"location":"microservices/about-author/","title":"About the Author","text":"<p>Anji Keesari is a software engineer and cloud architect with over 20 years of experience in the technology industry. He has been involved in numerous projects related to cloud computing, microservices architecture, and technologies such as Kubernetes, Terraform, and containers.</p> <p>Anji has hands-on experience deploying and managing Kubernetes clusters in production environments. He is also proficient in tools like ArgoCD and Helm, utilizing them to deploy microservices applications on Kubernetes. Anji also has extensive knowledge of containers and containerization technologies, including Docker and container orchestration tools such as Kubernetes.</p> <p>Apart from his expertise in Kubernetes and related tools, Anji has a strong background in Terraform, utilizing it to deploy infrastructure on various cloud platforms, including Azure and AWS.</p> <p>Anji has a passion for teaching and sharing his knowledge with others. He has written numerous articles and tutorials on Kubernetes, ArgoCD, and Helm, and published in Medium website.</p> <p>With his extensive knowledge and experience in Kubernetes, ArgoCD, and Helm, Anji is the perfect author for this book on <code>Build and Deploy Microservices Applications on a Kubernetes using ArgoCD and Helm</code>. His passion for teaching and his ability to explain complex concepts in simple terms will make this book a valuable resource for readers of all levels of expertise.</p> <p>Throughout his career, Anji have worked with various companies in diverse domains such as Banking, Healthcare, and Finance, across countries such as India, UK, and US. He is dedicated to making a significant impact in his workplace and helping others along the way.</p> <p>During his free time, Anji finds joy in various activities such as playing soccer, going hiking, exploring new places, and most importantly, spending quality time with his loved ones.</p> <p>For any questions, suggestions, or topic requests, feel free to drop him a message, and he'll get in touch when his schedule permits.</p> <p>Contact Information:</p> <ul> <li>Email: anjkeesari@gmail.com</li> <li>Website: https://anjikeesari.com</li> </ul>"},{"location":"microservices/acknowledgments/","title":"Acknowledgments","text":"<p>Writing a book is a collaborative effort, and I could not have done it without the help and support of many people.</p> <p>First and foremost, I would like to thank my family for their patience, understanding, and encouragement throughout this project, they scarified lot of (long) weekends. Their love and support kept me going during the long hours of writing and editing.</p> <p>During this book writing I had to refer lot of online materials, I would also like to thank the many individuals and organizations who have contributed to the development of Kubernetes, ArgoCD, Helm, Terraform, and containerization technologies. Without their hard work and dedication, this book would not be possible.</p> <p>Finally, I would like to thank the readers of this book for their interest and support. I hope that this book will be a valuable resource for anyone who wants to learn how to build and deploy microservices applications on a Kubernetes using ArgoCD and Helm.</p> <p>Thank you all for your contributions and support.</p> <p>Warm regards,</p> <p>Anji Keesari</p>"},{"location":"microservices/book-cover/","title":"Download e-Book","text":""},{"location":"microservices/chapter-1/","title":"Chapter-1: Building Containerized Microservices","text":""},{"location":"microservices/chapter-1/#introduction","title":"Introduction","text":"<p>Welcome to the first chapter of our book. In this chapter, we will begin our journey by understanding microservices &amp; docker concepts, explaining the importance of containerization, and its significance in modern microservices application development.</p> <p>Next, we will start building and containerizing microservices using various technologies such as .NET Core, React Js and Node.js. These hands-on labs are designed to guide you through the process of creating containerized websites, APIs, setting up databases in containers, and running external services in containers, showcasing diverse technologies used in modern web applications.</p> <p>By the end of this chapter, you will have a solid foundation in docker, microservices development, and a range of programming tools and technologies. Whether you are an expert programmer or just starting out, this journey will help you with essential skills for building and deploying modern, containerized microservice applications.</p>"},{"location":"microservices/chapter-1/#hands-on-labs","title":"Hands-On Labs","text":"<p>Here is the high-level list of labs we will cover in this chapter:</p> <p>Lab-1: Getting Started with Microservices - In this lab, we'll introduce you to the concept of microservices and explain their importance in modern application development. You'll gain a high-level understanding of microservices architecture and its benefits.</p> <p>Lab-2: Getting Started with Docker - Here, we'll look into Docker, the modern containerization technology. You'll learn how to install docker, run your first container, and explore basic docker commands.</p> <p>Lab-3: Create your First Containerized Microservice with .NET Core - This lab guides you through creating a microservice using .NET Core and containerizing it with Docker. You'll learn how to write Dockerfiles and build container images for .NET Core microservices.</p> <p>Lab-5: Create your Second Containerized Microservice with Node.js - In this lab, we switch gears to Node.js and create another microservice. You'll containerize a Node.js-based microservice and understand the differences compared to .NET Core.</p> <p>Lab-6: Create your First Containerized Website using ASP.NET Core MVC - Now, it's time to create a containerized website using ASP.NET Core MVC. You'll build a web application, package it as a Docker image, and run it as a container.</p> <p>Lab-7: Create your Second Containerized Website using React JS - In this lab, we'll focus on front-end development by creating a React.js-based website. You'll containerize a React application and understand how to work with front-end containers.</p> <p>Lab-8: Create your First Database with SQL Server - Databases are an essential part of microservices. In this lab, we'll set up a SQL Server database within a container. You'll learn how to create and connect to containerized databases.</p> <p>Lab-9: Create your Second Database with PostgreSQL - PostgreSQL is another popular database choice. This lab guides you through running PostgreSQL in a docker container and executing scripts. You'll understand how to work with different database engines within containers.</p> <p>Lab-10: Running Keycloak application in a Docker Container - External services play a important role in microservices. In this lab, we'll run Keycloak application, an identity and access management system, in a Docker container. You'll configure and interact with Keycloak within the containerized environment.</p> <p>Lab-11: Running Drupal website in a Docker Container - Continuing with external services, we'll set up Drupal website, a content management system, in a Docker container. You'll explore how to work with content management systems within containers.</p> <p>These hands-on labs provide a practical foundation for building and containerizing microservices. By the end of these labs, you'll have hands-on experience with various technologies and a clear understanding of how to create and run microservices and external services in containers. This knowledge will be invaluable as we progress through the chapters and explore more advanced microservices concepts and deployment strategies.</p>"},{"location":"microservices/chapter-1/#categories-of-labs","title":"Categories of Labs:","text":"<p>Labs in this Chapter are categorized into four areas, these categories provide a structured approach to learning containerization across different aspects of web development, from APIs and websites to databases and external services. By completing labs in each category, participants will gain comprehensive knowledge and skills essential for modern application development  practices. </p> <ul> <li>Creating Containerized APIs (API Development)</li> <li>Creating Containerized Websites (Website Development)</li> <li>Setting Up Databases in Containers (Database Containers)</li> <li>Running External Services in Docker Containers (External Services)</li> </ul>"},{"location":"microservices/chapter-1/#creating-containerized-apis","title":"Creating Containerized APIs","text":"<p>Labs created within this category, you'll learn how to create containerized APIs using technologies like .NET Core Web API, Node.js.</p> <p>.NET Core Web API:</p> <ol> <li> <p>Introduction to .NET Core Web API: We'll start by introducing you to .NET Core Web API, a cross-platform framework for building Restful services.</p> </li> <li> <p>Setting Up an .NET Core Web API Project: We'll guide you through setting up a new .NET Core Web API project.</p> </li> <li> <p>Containerization with Docker: You'll learn how to package your .NET Core Web API as a Docker container. We'll provide guidance on creating a Dockerfile for your web application.</p> </li> <li> <p>Running the Containerized .NET Core Web API: You'll see how to run your containerized .NET Core Web API locally and understand how containers simplify deployment.</p> </li> </ol> <p>Node.js APIs:</p> <ol> <li> <p>Introduction to Node.js: Node.js is a popular JavaScript library for building Restful services. We'll introduce you to Node.js and explain its role in modern Rest APIs development.</p> </li> <li> <p>Creating a Node.js Rest API: You'll learn how to create a Node.js API from scratch. </p> </li> <li> <p>Containerization with Docker: Similar to .NET Core Web API, we'll guide you through containerizing your Node.js API. You'll create a Dockerfile for your Restful service.</p> </li> <li> <p>Running the Containerized Node.js API: You'll see how to run your containerized Node.js API locally. </p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with .NET Core Web API and Node.js along with the knowledge of how to containerize web applications. These skills are essential as we move forward to deploy these containerized websites alongside microservices in later chapters. </p>"},{"location":"microservices/chapter-1/#creating-containerized-websites","title":"Creating Containerized Websites","text":"<p>Labs created within this category, you'll learn how to create containerized websites using technologies like ASP.NET Core, MVC and React.js.</p> <p>ASP.NET Core MVC:</p> <ol> <li> <p>Introduction to ASP.NET Core MVC: We'll start by introducing you to ASP.NET Core MVC, a cross-platform framework for building web applications. You'll understand its role in creating dynamic web content.</p> </li> <li> <p>Setting Up an ASP.NET Core MVC Project: We'll guide you through setting up a new ASP.NET Core MVC project.</p> </li> <li> <p>Containerization with Docker: You'll learn how to package your ASP.NET Core MVC application as a Docker container. We'll provide guidance on creating a Dockerfile for your web application.</p> </li> <li> <p>Running the Containerized ASP.NET Core MVC Application: You'll see how to run your containerized ASP.NET Core MVC application locally and understand how containers simplify deployment.</p> </li> </ol> <p>React.js:</p> <ol> <li> <p>Introduction to React.js: React.js is a popular JavaScript library for building user interfaces. We'll introduce you to React.js and explain its role in modern web development.</p> </li> <li> <p>Creating a React.js Application: You'll learn how to create a React.js application from scratch. </p> </li> <li> <p>Containerization with Docker: Similar to ASP.NET Core MVC, we'll guide you through containerizing your React.js application. You'll create a Dockerfile for your web app.</p> </li> <li> <p>Running the Containerized React.js Application: You'll see how to run your containerized React.js application locally. </p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with ASP.NET Core MVC and React.js, along with the knowledge of how to containerize web applications. These skills are essential as we move forward to deploy these containerized websites alongside microservices in later chapters. </p>"},{"location":"microservices/chapter-1/#setting-up-databases-in-containers","title":"Setting Up Databases in Containers","text":"<p>Labs created within this category, we'll learn setting up databases within containers for microservices data storage. You'll learn how to create containerized database instances using SQL Server and PostgreSQL.</p> <p>Microservices often rely on databases to store and manage data. Containerizing databases offers numerous advantages, such as isolation, portability, and versioning. In this section, we'll focus on two popular database systems: SQL Server and PostgreSQL.</p> <p>SQL Server:</p> <ol> <li> <p>Introduction to SQL Server: We'll introduce you to SQL Server, a robust relational database management system (RDBMS) developed by Microsoft.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize SQL Server by pulling an official SQL Server Docker image from the Azure Container registry or Docker Hub.</p> </li> <li> <p>Running SQL Server in a Docker Container: We'll guide you through running a SQL Server container, configuring database settings, and connecting to the containerized SQL Server instance.</p> </li> <li> <p>Data Management: You'll explore data management tasks within a containerized SQL Server, such as creating databases, tables, and performing CRUD (Create, Read, Update, Delete) operations.</p> </li> <li> <p>Connecting to database locally: Finally you'll explore different tools like SQL Server Management Studio (SSMS) and Azure data studio for connecting to containerized SQL Server database.</p> </li> </ol> <p>PostgreSQL:</p> <ol> <li> <p>Introduction to PostgreSQL: We'll introduce you to PostgreSQL, a powerful open-source relational database system known for its scalability and extensibility.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize PostgreSQL by pulling an official PostgreSQL Docker image from the Docker Hub.</p> </li> <li> <p>Running PostgreSQL in a Docker Container: We'll guide you through running a PostgreSQL container, configuring database settings, and connecting to the containerized PostgreSQL instance.</p> </li> <li> <p>Data Management: You'll explore data management tasks within a containerized PostgreSQL database, including creating databases, tables, and executing SQL queries.</p> </li> <li> <p>Connecting to database locally: Finally you'll explore different tools like <code>PSQL</code> and Pgadmin4 for connecting to containerized PostgreSQL database.</p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with containerized SQL Server and PostgreSQL databases, understanding their role in microservices data storage. These skills are important as you proceed through the chapters, where microservices will interact with these containerized databases to retrieve and store data. </p>"},{"location":"microservices/chapter-1/#running-external-services-in-containers","title":"Running External Services in Containers","text":"<p>Labs created within this category, we'll learn integration of external services into your microservices architecture. You'll learn how to run external services like Keycloak and Drupal in Docker containers, enhancing the capabilities of your microservices.</p> <p>External services play a importent role in microservices architecture, providing essential functionalities such as authentication and content management. Containerizing these external services offers several advantages, including consistency and simplified deployment. In this section, we'll focus on two prominent external services: Keycloak and Drupal.</p> <p>Keycloak:</p> <ol> <li> <p>Introduction to Keycloak: Keycloak is an open-source identity and access management system. We'll introduce you to Keycloak and explain its significance in microservices authentication.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize Keycloak by pulling an official Keycloak Docker image from the Docker Hub.</p> </li> <li> <p>Running Keycloak in a Docker Container: We'll guide you through running a Keycloak container, configuring realms, users, and roles within the containerized Keycloak instance.</p> </li> <li> <p>Testing the Keycloak Application Locally: Finally you'll see how to browse your containerized Keycloak application locally and login into admin portal and intacting with Keycloak application. </p> </li> </ol> <p>Drupal:</p> <ol> <li> <p>Introduction to Drupal: Drupal is a popular open-source content management system (CMS). We'll introduce you to Drupal and its role in managing content for microservices.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize Drupal by pulling an official Drupal Docker image from the Docker Hub.</p> </li> <li> <p>Running Drupal in a Docker Container: We'll guide you through running a Drupal container, setting up a website, and managing content within the containerized Drupal instance.</p> </li> <li> <p>Testing the Drupal Website Locally: Finally you'll see how to browse your containerized Drupal Website locally and login into drupal portal and intacting with drupal website.</p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with containerized Keycloak and Drupal instances, understanding how to integrate them seamlessly into your microservices ecosystem. These skills are essential as you proceed through the chapters, where microservices will rely on these external services for authentication, authorization, and content management.</p>"},{"location":"microservices/docker-fundamentals/","title":"Exploring Docker Fundamentals","text":""},{"location":"microservices/docker-fundamentals/#overview","title":"Overview","text":"<p>In this article, we'll explore the basics of Docker, which are like building blocks for understanding how containers work. Whether you're an experienced coder or just starting out, grasping these basics is essential for easily deploying applications in containers. These core concepts will come in handy as you continue your learning journey with docker.</p>"},{"location":"microservices/docker-fundamentals/#what-is-docker","title":"What is Docker?","text":"<p>Docker is a powerful platform that simplifies the process of developing, shipping, and running applications. Docker uses a technology known as containerization to encapsulate an application and its dependencies into a self-contained unit called a <code>container</code>. These containers are lightweight, portable, and consistent across different environments.</p>"},{"location":"microservices/docker-fundamentals/#why-use-docker","title":"Why use Docker?","text":"<p>Docker simplifies the development, deployment, and management of applications, offering an adaptable solution for modern software development practices. Its popularity comes from from its ability to address challenges related to consistency, scalability, and efficiency in the software development lifecycle.</p> <p>Docker has become increasingly popular in the software development and IT industry due to its numerous advantages. Here are some key benefits of using Docker:</p> <ol> <li> <p>Portability:    Docker containers encapsulate applications and their dependencies, ensuring consistency across different environments. This portability eliminates the common problem of \"it works on my machine\" and facilitates seamless deployment across various systems.</p> </li> <li> <p>Isolation:    Containers provide a lightweight and isolated environment for applications. Each container runs independently, preventing conflicts between dependencies and ensuring that changes made in one container do not affect others.</p> </li> <li> <p>Efficiency:    Docker's containerization technology enables efficient resource utilization. Containers share the host OS kernel, making them lightweight compared to traditional virtual machines. This results in faster startup times and improved performance.</p> </li> <li> <p>Scalability:    Docker makes it easy to scale applications horizontally by running multiple instances of containers. This scalability allows developers to change the workloads and ensures optimal resource utilization.</p> </li> <li> <p>Microservices architecture:    Docker is integral to the microservices architecture, where applications are composed of small, independently deployable services. Containers facilitate the development, deployment, and scaling of microservices, enabling agility and ease of management.</p> </li> <li> <p>DevOps integration:    Docker aligns well with DevOps practices by promoting collaboration between development and operations teams. Containers can be easily integrated into continuous integration and continuous deployment (CI/CD) pipelines, streamlining the software delivery process.</p> </li> <li> <p>Community support:    Docker's community offers lot of pre-made tools and solutions, helping developers work faster and learn from others.</p> </li> <li> <p>Security:     Docker provides built-in security features, such as isolation and resource constraints, to enhance application security. </p> </li> <li> <p>Cross-platform compatibility:     Docker containers can run on various operating systems, including Linux, Windows, and macOS. This cross-platform compatibility is beneficial for teams working in heterogeneous environments.</p> </li> </ol>"},{"location":"microservices/docker-fundamentals/#docker-concepts","title":"Docker concepts","text":"<p>Understanding these basic concepts is essential for effectively working with Docker and leveraging its advantages in terms of portability, scalability, and consistency across different environments.  Here are basic concepts of Docker:</p> <ul> <li> <p>Containerization Containerization is a technology that allows you to package an application and its dependencies, including libraries and configuration files, into a single container image.</p> </li> <li> <p>Images An image is a lightweight, standalone, and executable package that includes everything needed to run a piece of software, including the code, runtime, libraries, and system tools. Docker images are used to create containers. They are built from a set of instructions called a Dockerfile.</p> </li> <li> <p>Dockerfile A Dockerfile is a text file that contains a set of instructions for building a Docker image. It specifies the base image, adds dependencies, copies files, and defines other settings necessary for the application to run.</p> </li> <li> <p>Containers Containers are instances of Docker images. They run in isolated environments, ensuring that the application behaves consistently across different environments. Containers share the host OS kernel but have their own file system, process space, and network interfaces.</p> </li> <li> <p>Registries Docker images can be stored and shared through registries. The default registry is Docker Hub, but private registries can also be used. Registries allow versioning, distribution, and collaboration on Docker images.</p> </li> <li> <p>Docker compose Docker Compose is a tool for defining and running multi-container Docker applications. It allows you to define a multi-container application in a single file, specifying services, networks, and volumes.</p> </li> <li> <p>Docker engine Docker Engine is the core component that manages Docker containers. It includes a server, REST API, and a command-line interface (CLI). The Docker daemon runs on the host machine, and the Docker CLI communicates with it to build, run, and manage containers.</p> </li> <li> <p>Volumes Volumes provide a way for containers to persist data outside their lifecycle. They can be used to share data between containers or to persist data even if a container is stopped or removed.</p> </li> <li> <p>Networking Docker provides networking capabilities that allow containers to communicate with each other or with the external world. Containers can be connected to different networks, and ports can be mapped between the host and the containers.</p> </li> </ul>"},{"location":"microservices/docker-fundamentals/#container-orchestration","title":"Container orchestration","text":"<p>Whether managing a small cluster or a large-scale production environment, adopting container orchestration is crucial for containerized applications. Here are some container orchestrations:</p> <ul> <li> <p>Kubernetes: Kubernetes is the most widely adopted container orchestration platform. It automates the deployment, scaling, and management of containerized applications, providing a robust and extensible framework.</p> </li> <li> <p>Docker Swarm: Docker Swarm is a native clustering and orchestration solution provided by Docker. While it may not be as feature-rich as Kubernetes, it offers simplicity and seamless integration with Docker.</p> </li> <li> <p>Amazon ECS: Amazon Elastic Container Service (ECS) is a fully managed container orchestration service provided by AWS. It integrates with other AWS services and is suitable for users already utilizing the AWS ecosystem.</p> </li> <li> <p>Azure Kubernetes Service (AKS): AKS is a managed Kubernetes service offered by Microsoft Azure. It simplifies the deployment and management of Kubernetes clusters in the Azure cloud.</p> </li> </ul>"},{"location":"microservices/docker-fundamentals/#docker-desktop","title":"Docker Desktop","text":"<p>Docker Desktop is a powerful tool that provides a user-friendly interface and environment for developing, building, and testing applications using Docker containers on local machine. </p> <p>Docker Desktop provides a convenient environment for developers to work with containers on their personal machines.</p>"},{"location":"microservices/docker-fundamentals/#install-docker","title":"Install Docker","text":"<p>Here are the steps to install Docker on a different operating systems:</p> <p>Windows:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Windows page.</li> <li>Click on the \"Download for Windows\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Follow the installation wizard, accepting the default options.</li> <li>The installer may require you to restart your computer.</li> </ul> <p>Enable Hyper-V (Windows 10 Pro/Enterprise):</p> <ul> <li>If you're running Windows 10 Pro or Enterprise, Docker Desktop will use Hyper-V for virtualization. Ensure that Hyper-V is enabled in the Windows Features.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, start Docker Desktop from the Start Menu.</li> <li>The Docker icon will appear in the system tray when Docker Desktop is running.</li> </ul> <p>macOS:</p> <p>Download Docker Desktop:</p> <ul> <li>Visit the Docker Desktop for Mac page.</li> <li>Click on the \"Download for Mac\" button.</li> <li>Follow the on-screen instructions to download the installer.</li> </ul> <p>Install Docker Desktop:</p> <ul> <li>Run the installer that you downloaded.</li> <li>Drag the Docker icon to the Applications folder.</li> <li>Launch Docker from Applications.</li> </ul> <p>Start Docker Desktop:</p> <ul> <li>Once installed, Docker Desktop should start automatically.</li> <li>The Docker icon will appear in the menu bar when Docker Desktop is running.</li> </ul> <p>Verify Docker install:</p> <p>To verify that Docker is installed correctly, open a terminal and run the following command:</p> <pre><code>docker --version\n\n# or\ndocker version\n</code></pre> <p>If you notice this, it indicates that your Docker is not in a running status.</p> <pre><code>error during connect: this error may indicate that the docker daemon is not running: Get \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/version\": open //./pipe/docker_engine: The system cannot find the file specified.\nClient:\n Cloud integration: v1.0.35\n Version:           24.0.2\n API version:       1.43\n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n</code></pre> <p>After Docker desktop is started and if everything is set up correctly, you should see following message indicating that your Docker installation is working.</p> <pre><code>Client:\n Cloud integration: v1.0.35 \n Version:           24.0.2  \n API version:       1.43    \n Go version:        go1.20.4\n Git commit:        cb74dfc\n Built:             Thu May 25 21:53:15 2023\n OS/Arch:           windows/amd64\n Context:           default\n\nServer: Docker Desktop 4.21.1 (114176)\n Engine:\n  Version:          24.0.2\n  API version:      1.43 (minimum version 1.12)\n  Go version:       go1.20.4\n  Git commit:       659604f\n  Built:            Thu May 25 21:52:17 2023\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.21\n  GitCommit:        3dce8eb055cbb6872793272b4f20ed16117344f8\n runc:\n  Version:          1.1.7\n  GitCommit:        v1.1.7-0-g860f061\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre> <p>Docker is now installed on your machine, and you can start using it to containerize your applications.</p>"},{"location":"microservices/docker-fundamentals/#docker-commands","title":"Docker Commands","text":"<p>For more comprehensive details on Docker commands, please refer to the Docker Commands Cheat Sheet on our website.</p>"},{"location":"microservices/docker-fundamentals/#conclusion","title":"Conclusion","text":"<p>Docker and containerization have changed the way we build and use application development. Now that you understand the basics of Docker, you're ready to dive deeper. Docker is straightforward and flexible, making it a great tool for developers. It ensures that your application works the same way in different situations, keeps things separate, and easily grows with your needs. So, go ahead and start your journey with containers.</p>"},{"location":"microservices/docker-fundamentals/#references","title":"References","text":"<ul> <li>Getting started guide</li> <li>Docker images</li> <li>Docker Documentation</li> <li>Docker Hub</li> </ul>"},{"location":"microservices/introduction/","title":"Introduction","text":"<p>Welcome to \"Building Microservices with Containers: A Practical Guide.\" In today's rapidly growing technological landscape, the demand for scalable, flexible, and resilient software solutions is necessary. In response to this demand, the architecture of choice for many modern applications is microservices. Microservices enables the development of complex systems as a set of small, independently deployable services.</p> <p>This book is your detailed guide to understanding and implementing microservices architecture using containerization technology, specifically Docker. Whether you're a regular application developer looking to adopt microservices or a new to the technology, this book will provide you with the knowledge and hands-on experience necessary to succeed in building scalable and maintainable applications.</p>"},{"location":"microservices/introduction/#why-microservices","title":"Why Microservices?","text":"<p>Before looking into the technical details, let's briefly explore why microservices have become the architecture of choice for many organizations. Microservices offer several advantages over traditional monolithic architectures, including:</p> <ul> <li>Scalability: Microservices allow individual components of an application to scale independently, enabling better resource utilization and improved performance.</li> <li>Flexibility: With microservices, teams can choose the most appropriate technology stack for each service, leading to greater flexibility and innovation.</li> <li>Resilience: Isolating services from each other reduces the impact of failures, making the overall system more resilient.</li> <li>Continuous Delivery: Microservices facilitate continuous delivery and deployment practices, enabling teams to release updates quickly and frequently.</li> </ul>"},{"location":"microservices/introduction/#why-containers","title":"Why Containers?","text":"<p>While microservices offer numerous benefits, managing a large number of services can be challenging. This is where containerization comes into play. Containers provide lightweight, portable, and isolated environments for running applications, making it easier to package, deploy, and manage microservices at scale. Docker, one of the most popular containerization platforms, has revolutionized the way developers build, ship, and run applications.</p>"},{"location":"microservices/introduction/#what-youll-learn","title":"What You'll Learn","text":"<p>In this book, we'll start by covering the fundamentals of microservices architecture and Docker containerization. We'll then guide you through the process of building and deploying microservices using a variety of technologies, including .NET Core, Node.js, React.js, and SQL databases. Along the way, you'll learn how to:</p> <ul> <li>Containerize microservices using Docker.</li> <li>Orchestrate containers with Docker Compose.</li> <li>Implement authentication and authorization using Keycloak.</li> <li>Build web applications with popular frameworks like .NET Core MVC and React.js.</li> <li>Set up and manage databases within containers using SQL Server and PostgreSQL.</li> <li>Deploy and scale microservices in a production environment.</li> </ul> <p>Each chapter includes practical, hands-on tutorials and real-world examples to help reinforce your understanding of the concepts covered. By the end of this book, you'll have the knowledge and skills to design, build, and deploy microservices-based applications with confidence.</p>"},{"location":"microservices/introduction/#who-this-book-is-for","title":"Who This Book Is For","text":"<p>This book is designed for developers, architects, and DevOps engineers who are interested in adopting microservices architecture using containerization technology. Whether you're new to microservices or looking to expand your knowledge, this book will provide you with the essential tools and techniques to succeed in today's growing software development landscape.</p> <p>Developers:</p> <ul> <li>If you're a developer looking to moving from traditional monolithic architectures to microservices, this book will provide you with the necessary knowledge and practical skills to design, develop, and deploy microservices-based applications using containerization technology.</li> <li>Whether you specialize in a specific programming language or framework, the hands-on tutorials and real-world examples in this book will help you gain a deeper understanding of how to implement microservices using a variety of technologies, including .NET Core, Node.js, React.js, SQL databases, and more.</li> </ul> <p>Architects:</p> <ul> <li>For architects responsible for designing and planning the architecture of modern applications, this book will serve as a comprehensive guide to understanding the principles, patterns, and best practices of microservices architecture.</li> <li>You'll learn how to design scalable, resilient, and maintainable systems using microservices and containerization technology, and how to address common challenges such as service discovery, communication, and data management in distributed environments.</li> </ul> <p>DevOps Engineers:</p> <ul> <li>If you're a DevOps engineer tasked with managing the deployment, scaling, and monitoring of microservices-based applications, this book will help you with the necessary skills to leverage containerization tools like Docker and orchestration platforms like Kubernetes.</li> <li>You'll learn how to automate the deployment process, implement continuous integration and continuous delivery (CI/CD) pipelines, and ensure the reliability and performance of microservices in production environments.</li> </ul> <p>Students and Researchers:</p> <ul> <li>This book can also be valuable for students and researchers studying software engineering,  and cloud computing. It provides a practical, hands-on approach to learning about microservices architecture and containerization technology, with real-world examples and case studies to illustrate key concepts.</li> </ul>"},{"location":"microservices/introduction/#key-benefits-of-reading-this-book","title":"Key Benefits of Reading This Book:","text":"<p>\"Building Microservices with Containers: A Practical Guide\" offers a lot of benefits to readers at various stages of their application development journey in understanding and implementing microservices architecture with containerization technology. Here are some key benefits you can expect from reading this book:</p> <p>Hands-On Tutorials:</p> <ul> <li>Benefit from step-by-step tutorials and real-world examples that guide you through the process of building and deploying microservices using Docker containers.</li> <li>Gain practical experience by working on hands-on exercises and projects designed to reinforce your learning and enhance your skills.</li> </ul> <p>Diverse Technology Stack:</p> <ul> <li>Explore a diverse range of technologies and frameworks, including .NET Core, Node.js, React.js, SQL databases, Docker, and Kubernetes.</li> <li>Learn how to choose the right tools and technologies for your specific use case, and how to integrate them effectively to build scalable and resilient applications.</li> </ul> <p>Transition from Monolithic to Microservices:</p> <ul> <li>Understand the benefits and challenges of transitioning from monolithic architectures to microservices, and how to plan and execute a successful migration strategy.</li> <li>Learn how to decompose monolithic applications into smaller, loosely-coupled services, and how to leverage containerization to improve scalability, flexibility, and resilience.</li> </ul> <p>Whether you're a developer, architect, DevOps engineer, student, or researcher, \"Building Microservices with Containers: A Practical Guide\" offers valuable insights, practical skills, and career advancement opportunities that will empower you to succeed in today's dynamic and fast-paced software development landscape.</p>"},{"location":"microservices/introduction/#hands-on-labs","title":"Hands-On Labs","text":"<p>Here is the high-level list of labs we will cover in this chapter:</p> <p>Lab-1: Getting Started with Microservices - In this lab, we'll introduce you to the concept of microservices and explain their importance in modern application development. You'll gain a high-level understanding of microservices architecture and its benefits.</p> <p>Lab-2: Getting Started with Docker - Here, we'll look into Docker, the modern containerization technology. You'll learn how to install docker, run your first container, and explore basic docker commands.</p> <p>Lab-3: Create your First Containerized Microservice with .NET Core - This lab guides you through creating a microservice using .NET Core and containerizing it with Docker. You'll learn how to write Dockerfiles and build container images for .NET Core microservices.</p> <p>Lab-5: Create your Second Containerized Microservice with Node.js - In this lab, we switch gears to Node.js and create another microservice. You'll containerize a Node.js-based microservice and understand the differences compared to .NET Core.</p> <p>Lab-6: Create your First Containerized Website using ASP.NET Core MVC - Now, it's time to create a containerized website using ASP.NET Core MVC. You'll build a web application, package it as a Docker image, and run it as a container.</p> <p>Lab-7: Create your Second Containerized Website using React JS - In this lab, we'll focus on front-end development by creating a React.js-based website. You'll containerize a React application and understand how to work with front-end containers.</p> <p>Lab-8: Create your First Database with SQL Server - Databases are an essential part of microservices. In this lab, we'll set up a SQL Server database within a container. You'll learn how to create and connect to containerized databases.</p> <p>Lab-9: Create your Second Database with PostgreSQL - PostgreSQL is another popular database choice. This lab guides you through running PostgreSQL in a docker container and executing scripts. You'll understand how to work with different database engines within containers.</p> <p>Lab-10: Running Keycloak application in a Docker Container - External services play a important role in microservices. In this lab, we'll run Keycloak application, an identity and access management system, in a Docker container. You'll configure and interact with Keycloak within the containerized environment.</p> <p>Lab-11: Running Drupal website in a Docker Container - Continuing with external services, we'll set up Drupal website, a content management system, in a Docker container. You'll explore how to work with content management systems within containers.</p> <p>These hands-on labs provide a practical foundation for building and containerizing microservices. By the end of these labs, you'll have hands-on experience with various technologies and a clear understanding of how to create and run microservices and external services in containers. This knowledge will be invaluable as we progress through the chapters and explore more advanced microservices concepts and deployment strategies.</p>"},{"location":"microservices/introduction/#categories-of-labs","title":"Categories of Labs:","text":"<p>Labs in this Chapter are categorized into four areas, these categories provide a structured approach to learning containerization across different aspects of web development, from APIs and websites to databases and external services. By completing labs in each category, participants will gain comprehensive knowledge and skills essential for modern application development  practices. </p> <ul> <li>Creating Containerized APIs (API Development)</li> <li>Creating Containerized Websites (Website Development)</li> <li>Setting Up Databases in Containers (Database Containers)</li> <li>Running External Services in Docker Containers (External Services)</li> </ul>"},{"location":"microservices/introduction/#creating-containerized-apis","title":"Creating Containerized APIs","text":"<p>Labs created within this category, you'll learn how to create containerized APIs using technologies like .NET Core Web API, Node.js.</p> <p>.NET Core Web API:</p> <ol> <li> <p>Introduction to .NET Core Web API: We'll start by introducing you to .NET Core Web API, a cross-platform framework for building Restful services.</p> </li> <li> <p>Setting Up an .NET Core Web API Project: We'll guide you through setting up a new .NET Core Web API project.</p> </li> <li> <p>Containerization with Docker: You'll learn how to package your .NET Core Web API as a Docker container. We'll provide guidance on creating a Dockerfile for your web application.</p> </li> <li> <p>Running the Containerized .NET Core Web API: You'll see how to run your containerized .NET Core Web API locally and understand how containers simplify deployment.</p> </li> </ol> <p>Node.js APIs:</p> <ol> <li> <p>Introduction to Node.js: Node.js is a popular JavaScript library for building Restful services. We'll introduce you to Node.js and explain its role in modern Rest APIs development.</p> </li> <li> <p>Creating a Node.js Rest API: You'll learn how to create a Node.js API from scratch. </p> </li> <li> <p>Containerization with Docker: Similar to .NET Core Web API, we'll guide you through containerizing your Node.js API. You'll create a Dockerfile for your Restful service.</p> </li> <li> <p>Running the Containerized Node.js API: You'll see how to run your containerized Node.js API locally. </p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with .NET Core Web API and Node.js along with the knowledge of how to containerize web applications. These skills are essential as we move forward to deploy these containerized websites alongside microservices in later chapters. </p>"},{"location":"microservices/introduction/#creating-containerized-websites","title":"Creating Containerized Websites","text":"<p>Labs created within this category, you'll learn how to create containerized websites using technologies like ASP.NET Core, MVC and React.js.</p> <p>ASP.NET Core MVC:</p> <ol> <li> <p>Introduction to ASP.NET Core MVC: We'll start by introducing you to ASP.NET Core MVC, a cross-platform framework for building web applications. You'll understand its role in creating dynamic web content.</p> </li> <li> <p>Setting Up an ASP.NET Core MVC Project: We'll guide you through setting up a new ASP.NET Core MVC project.</p> </li> <li> <p>Containerization with Docker: You'll learn how to package your ASP.NET Core MVC application as a Docker container. We'll provide guidance on creating a Dockerfile for your web application.</p> </li> <li> <p>Running the Containerized ASP.NET Core MVC Application: You'll see how to run your containerized ASP.NET Core MVC application locally and understand how containers simplify deployment.</p> </li> </ol> <p>React.js:</p> <ol> <li> <p>Introduction to React.js: React.js is a popular JavaScript library for building user interfaces. We'll introduce you to React.js and explain its role in modern web development.</p> </li> <li> <p>Creating a React.js Application: You'll learn how to create a React.js application from scratch. </p> </li> <li> <p>Containerization with Docker: Similar to ASP.NET Core MVC, we'll guide you through containerizing your React.js application. You'll create a Dockerfile for your web app.</p> </li> <li> <p>Running the Containerized React.js Application: You'll see how to run your containerized React.js application locally. </p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with ASP.NET Core MVC and React.js, along with the knowledge of how to containerize web applications. These skills are essential as we move forward to deploy these containerized websites alongside microservices in later chapters. </p>"},{"location":"microservices/introduction/#setting-up-databases-in-containers","title":"Setting Up Databases in Containers","text":"<p>Labs created within this category, we'll learn setting up databases within containers for microservices data storage. You'll learn how to create containerized database instances using SQL Server and PostgreSQL.</p> <p>Microservices often rely on databases to store and manage data. Containerizing databases offers numerous advantages, such as isolation, portability, and versioning. In this section, we'll focus on two popular database systems: SQL Server and PostgreSQL.</p> <p>SQL Server:</p> <ol> <li> <p>Introduction to SQL Server: We'll introduce you to SQL Server, a robust relational database management system (RDBMS) developed by Microsoft.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize SQL Server by pulling an official SQL Server Docker image from the Azure Container registry or Docker Hub.</p> </li> <li> <p>Running SQL Server in a Docker Container: We'll guide you through running a SQL Server container, configuring database settings, and connecting to the containerized SQL Server instance.</p> </li> <li> <p>Data Management: You'll explore data management tasks within a containerized SQL Server, such as creating databases, tables, and performing CRUD (Create, Read, Update, Delete) operations.</p> </li> <li> <p>Connecting to database locally: Finally you'll explore different tools like SQL Server Management Studio (SSMS) and Azure data studio for connecting to containerized SQL Server database.</p> </li> </ol> <p>PostgreSQL:</p> <ol> <li> <p>Introduction to PostgreSQL: We'll introduce you to PostgreSQL, a powerful open-source relational database system known for its scalability and extensibility.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize PostgreSQL by pulling an official PostgreSQL Docker image from the Docker Hub.</p> </li> <li> <p>Running PostgreSQL in a Docker Container: We'll guide you through running a PostgreSQL container, configuring database settings, and connecting to the containerized PostgreSQL instance.</p> </li> <li> <p>Data Management: You'll explore data management tasks within a containerized PostgreSQL database, including creating databases, tables, and executing SQL queries.</p> </li> <li> <p>Connecting to database locally: Finally you'll explore different tools like <code>PSQL</code> and Pgadmin4 for connecting to containerized PostgreSQL database.</p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with containerized SQL Server and PostgreSQL databases, understanding their role in microservices data storage. These skills are important as you proceed through the chapters, where microservices will interact with these containerized databases to retrieve and store data. </p>"},{"location":"microservices/introduction/#running-external-services-in-containers","title":"Running External Services in Containers","text":"<p>Labs created within this category, we'll learn integration of external services into your microservices architecture. You'll learn how to run external services like Keycloak and Drupal in Docker containers, enhancing the capabilities of your microservices.</p> <p>External services play a importent role in microservices architecture, providing essential functionalities such as authentication and content management. Containerizing these external services offers several advantages, including consistency and simplified deployment. In this section, we'll focus on two prominent external services: Keycloak and Drupal.</p> <p>Keycloak:</p> <ol> <li> <p>Introduction to Keycloak: Keycloak is an open-source identity and access management system. We'll introduce you to Keycloak and explain its significance in microservices authentication.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize Keycloak by pulling an official Keycloak Docker image from the Docker Hub.</p> </li> <li> <p>Running Keycloak in a Docker Container: We'll guide you through running a Keycloak container, configuring realms, users, and roles within the containerized Keycloak instance.</p> </li> <li> <p>Testing the Keycloak Application Locally: Finally you'll see how to browse your containerized Keycloak application locally and login into admin portal and intacting with Keycloak application. </p> </li> </ol> <p>Drupal:</p> <ol> <li> <p>Introduction to Drupal: Drupal is a popular open-source content management system (CMS). We'll introduce you to Drupal and its role in managing content for microservices.</p> </li> <li> <p>Containerization with Docker: You'll learn how to containerize Drupal by pulling an official Drupal Docker image from the Docker Hub.</p> </li> <li> <p>Running Drupal in a Docker Container: We'll guide you through running a Drupal container, setting up a website, and managing content within the containerized Drupal instance.</p> </li> <li> <p>Testing the Drupal Website Locally: Finally you'll see how to browse your containerized Drupal Website locally and login into drupal portal and intacting with drupal website.</p> </li> </ol> <p>By the end of these labs, you'll have hands-on experience with containerized Keycloak and Drupal instances, understanding how to integrate them seamlessly into your microservices ecosystem. These skills are essential as you proceed through the chapters, where microservices will rely on these external services for authentication, authorization, and content management.</p>"},{"location":"microservices/microfrontend/","title":"Microfrontend","text":""},{"location":"microservices/microfrontend/#introduction","title":"Introduction","text":"<p>In our previous labs we've created couple of Microservices to the show the Microservices architecture pattern, same pattern is applicable for MicroFrontend applications; In this lab we are going to focus on MicroFrontend Applications.</p> <p>Let's try to understand some basic concepts of MicroFrontend here before start creating frontend applications.</p>"},{"location":"microservices/microfrontend/#microservices-vs-microfrontend","title":"Microservices vs MicroFrontend?","text":"<p>Microservices and MicroFrontends are two related but distinct concepts in modern software architecture.</p> <p>Microservices refer to an architectural style where a complex application is broken down into smaller, independent services that can be developed, deployed, and maintained independently. Each service typically has a specific business function and communicates with other services through APIs.</p> <p>MicroFrontends, on the other hand, refer to an approach for building web user interfaces where a complex web application is broken down into smaller, independent frontend applications that can be developed, deployed, and maintained independently. Each frontend application typically corresponds to a specific part of the user interface and communicates with other frontend applications through APIs.</p> <p>While there are some similarities between the two concepts, there are also some important differences. One key difference is the scope of each approach. Microservices are concerned with breaking down a complex application into smaller, independent services at the backend level, while MicroFrontends are concerned with breaking down a complex web application into smaller, independent frontend applications.</p> <p>Another key difference is the level of independence of each approach. Microservices are fully independent backend services that can be developed, deployed, and maintained independently, while MicroFrontends are independent frontend applications that are loosely coupled and can be developed, deployed, and maintained independently, but still rely on a backend system to provide data and functionality.</p> <p>Overall, while Microservices and MicroFrontends share some similarities in terms of breaking down a complex application into smaller, independent parts, they address different concerns and are typically used in different parts of the software architecture.</p>"},{"location":"microservices/microfrontend/#what-is-microfrontend","title":"What is MicroFrontend?","text":"<p><code>MicroFrontend</code> is a web development approach that involves breaking down a frontend monolith into smaller, more manageable parts, called MicroFrontends. MicroFrontends are individual, self-contained parts of a user interface that can be developed, tested, and deployed independently from one another. Each MicroFrontend is responsible for a specific feature or functionality of the overall application. When combined, these MicroFrontends make up the complete user interface of the application.</p> <p>One of the key benefits of MicroFrontend is that it allows developers to work on different parts of the application independently, without disrupting the work of others. This approach also makes it easier to scale and maintain large web applications, as changes can be made to specific MicroFrontends without affecting the entire application. Additionally, MicroFrontends can be reused across different applications, further reducing development time and costs.</p>"},{"location":"microservices/microfrontend/#microfrontend-architecture","title":"MicroFrontend architecture","text":"<p>In MicroFrontend architecture, the user interface is composed of multiple MicroFrontend that are combined at runtime to create a cohesive, functional application. Each MicroFrontend is responsible for rendering a specific part of the UI and communicating with other MicroFrontends through well-defined interfaces.</p> <p>MicroFrontend architecture allows developers to work on individual parts of an application in isolation, without worrying about how changes in one part might affect other parts. This makes it easier to scale development teams, reuse code, and add new features to an application without disrupting the entire system.</p> <p>Overall, MicroFrontend is a relatively new approach to frontend development that is gaining popularity due to its flexibility, scalability, and ease of maintenance.</p>"},{"location":"microservices/microfrontend/#benefits-of-microfrontend-architecture","title":"Benefits of MicroFrontend architecture","text":"<p>There are several benefits of using a MicroFrontend architecture to build web applications:</p> <ul> <li> <p>Independent Development: One of the main advantages of MicroFrontend architecture is that it allows developers to work independently on different parts of the application without interfering with each other's work. This leads to faster development cycles, as teams can work in parallel, resulting in faster time-to-market.</p> </li> <li> <p>Scalability: MicroFrontend architecture makes it easier to scale web applications. Since each MicroFrontend is responsible for a specific functionality or feature, it can be easily replaced, updated or scaled independently without affecting the other parts of the application.</p> </li> <li> <p>Reusability: The modular nature of MicroFrontend architecture makes it possible to reuse components across different applications. This reduces development time and costs as the components can be developed once and reused multiple times.</p> </li> <li> <p>Improved User Experience: MicroFrontend architecture allows developers to create smaller, more focused user interfaces that are optimized for specific use cases. This results in a better user experience as users only see the features they need, and the application is faster and more responsive.</p> </li> <li> <p>Reduced Code Complexity: MicroFrontend architecture can help to reduce code complexity by breaking down complex monolithic applications into smaller, more manageable parts. This makes it easier to maintain and update the application over time.</p> </li> <li> <p>Technology Agnostic: MicroFrontend architecture allows developers to choose the best technology for each MicroFrontend. This means that different teams can use different technologies and frameworks without affecting the overall application.</p> </li> </ul> <p>Overall, MicroFrontend architecture provides a more modular, scalable, and efficient way to build web applications, allowing teams to work faster and more effectively while improving the user experience.</p> <p>Now you understand the MicroFrontend architecture, we will create few MicroFrontend applications in our next labs.</p>"},{"location":"microservices/toc/","title":"Table of Contents","text":"<ul> <li>About the Author</li> <li>Acknowledgments</li> <li>Introduction</li> </ul> <ul> <li>Chapter-1: Getting Started with Microservices<ul> <li>What are Microservices?</li> <li>Microservices vs Monolithic Architectures</li> <li>Advantages of Microservices</li> <li>Challenges and Considerations</li> <li>Key Technologies and Tools</li> <li>Microservices Communication</li> <li>Domain-Driven Design (DDD)</li> <li>Task-1: Identify Microservices for the book</li> <li>Task-2: Identify the List of Git Repositories Needed</li> <li>Task-3: Create new Azure DevOps Organization</li> <li>Task-4: Create new Azure DevOps Project</li> </ul> </li> <li>Chapter-2: Docker Fundamentals<ul> <li>Overview</li> <li>What is Docker?</li> <li>Why use Docker?</li> <li>Docker concepts</li> <li>Container orchestration</li> <li>Docker Desktop</li> <li>Install Docker</li> <li>Docker Commands</li> </ul> </li> <li>Chapter-3: Getting Started with Docker<ul> <li>Step 1: Get the Sample Application</li> <li>Step 2: Create Docker Image</li> <li>Step 3: Create Docker Container</li> <li>Step 4: Port Binding</li> <li>Step 5: Browse the Frontend Application</li> <li>Step 6: View Docker Logs</li> <li>Step 7: Docker Commands</li> </ul> </li> <li>Chapter-4: Create Your First Microservice with .NET Core Web API<ul> <li>Step-1: Create a new repo in azure DevOps</li> <li>Step-2: Clone the repo from azure DevOps</li> <li>Step-3: Create a new .NET Core Web API project</li> <li>Step-4: Test the new .NET core Web API project</li> <li>Step-5: Add Dockerfiles to the API project</li> <li>Step-6: Docker Build &amp; Run</li> <li>Step-7: Push docker container to ACR</li> <li>Step-8: Pull docker container from ACR</li> </ul> </li> <li>Chapter-5: Create Your Second Microservice with Node.js<ul> <li>Step-1: Setup repository in Azure DevOps.</li> <li>Step-2: Create a new Node JS API project</li> <li>Step-3: Test the Node JS API project</li> <li>Step-4: Add Dockerfiles to the MVC project</li> <li>Step-5: Docker build locally</li> <li>Step-6: Docker run locally</li> <li>Step-7: Push docker container to ACR</li> </ul> </li> <li>Chapter-6: Create Your First Website using .NET Core MVC Application<ul> <li>Step-1: Create a new ASP.NET Core Web App (MVC project)</li> <li>Step-2: Test the new ASP.NET core Web App project</li> <li>Step-3: Update home page contents[Optional]</li> <li>Step-4: Add Dockerfiles to the MVC project</li> <li>Step-5: Docker Build locally</li> <li>Step-6: Docker Run locally</li> <li>Step-7: Push docker container to ACR</li> </ul> </li> <li>Chapter-7: Create Your Second Website using React.js<ul> <li>Step-1: Install Node.js and NPM</li> <li>Step-2: Create a new React JS application</li> <li>Step-3: Add Dockerfiles to the MVC project</li> <li>Step-4: Docker Build locally</li> <li>Step-5: Docker Run locally</li> <li>Step-6: Push docker container to ACR</li> </ul> </li> <li>Chapter-8: Create your First Database with SQL Server<ul> <li>Benefits of SQL Server Container</li> <li>Step-1: Setup Git Repository for SQL Server database</li> <li>Step-2: Create Folder Structure for SQL Server database</li> <li>Step-3: Add Dockerfiles to the Database Project</li> <li>Step-4: Test the SQL Server database connection using SSMS</li> <li>Step-5: Test the SQL server database connection using Azure Data Studio</li> <li>Step-6: Push Docker Container to ACR</li> </ul> </li> <li>Chapter-9: Create your First Database with PostgreSQL<ul> <li>Step-1: Setup Git Repository for PostgreSQL database</li> <li>Step-2: Create Folder Structure for PostgreSQL database</li> <li>Step-3: Add Dockerfiles to the Database Project</li> <li>Step-4: Create Docker Compose file</li> <li>Step-5: Test the PostgreSQL database connection from psql tool</li> <li>Step-6: Test the PostgreSQL database from pgadmin4 tool</li> <li>Step-7: Push Docker Container to ACR</li> </ul> </li> <li>Chapter-10: Setting up Keycloak in a Docker Container<ul> <li>Step-1: Setup repository for Keycloak in Azure DevOps</li> <li>Step-2: Create Keycloak project</li> <li>Step-3: Keycloak setup with docker compose</li> <li>Step-3.1: Setup Keycloak Service</li> <li>Step-3.2: Setup Keycloak Service with PostgreSQL database</li> <li>Step-4: Keycloak setup with Dockerfile</li> <li>Step-4.1: Create Dockerfile</li> <li>Step-4.2: Docker build locally</li> <li>Step-4.3: Docker run locally</li> <li>Step-5: Publish the Keycloak docker container to container registry</li> </ul> </li> <li>Chapter-11: Setting up Drupal in a Docker Container<ul> <li>Step-1: Setup Git Repository for Drupal</li> <li>Step-2: Create Drupal Project</li> <li>Step-3: Create Docker Compose file</li> <li>Step-4: Build Drupal locally</li> <li>Step-4.3: Run Drupal Container locally</li> </ul> </li> </ul>"}]}